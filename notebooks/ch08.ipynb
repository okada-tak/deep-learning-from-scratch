{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HWLgcky8gStf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/okada-tak/deep-learning-from-scratch/blob/master/notebooks/ch08.ipynb)\n",
        "[![Open in SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/okada-tak/deep-learning-from-scratch/blob/master/notebooks/ch08.ipynb)  \n",
        "# ■ch08/xxxx.py以外は追記（岡田）\n",
        "# 8章 ディープラーニング のまとめ\n",
        "- 多くの問題では、ネットワークを深くすることで、性能の向上が期待できる。  \n",
        "- ILSVRCと呼ばれる画像認識のコンペティションの最近の動向は、ディープラーニングによる手法が上位を独占し、使われるネットワークもディープ化している。  \n",
        "- 有名なネットワークには、VGG、GoogLeNet、ResNetがある。  \n",
        "- GPUや分散学習、ビット精度の削減などによってディープラーニングの高速化を実現できる。  \n",
        "- ディープラーニング（ニューラルネットワーク）は、物体認識だけではなく、物体検出やセグメンテーションに利用できる。  \n",
        "- ディープラーニングを用いたアプリケーションとして、画像のキャプション生成、画像の生成、強化学習などがある。最近では、自動運転へのディープラーニングの利用も期待されている。\n",
        "\n",
        "※注：本書に書かれている内容は2016年当時のものであることに注意。現時点ですでに6年たっているので、2016年当時はこうだったと思って読むのがよいと思う。"
      ],
      "metadata": {
        "id": "qbSs0VoiWQ9n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.1 ネットワークをより深く\n",
        "## 8.1.1 よりディープなネットワークへ\n"
      ],
      "metadata": {
        "id": "J3ZBFmb2iSbT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ■追記（岡田）Colaboratory用\n",
        "Google Colaboratoryの場合、Google Driveに  \n",
        "dl-from-scratch/ch08  \n",
        "というフォルダを用意し、そこにこのjupyter notebookを配置。  \n",
        "(dl-from-scratchの部分は任意。)  \n",
        "また、datasetフォルダとcommonフォルダを\n",
        "dl-from-scratch/dataset  \n",
        "dl-from-scratch/common\n",
        "にコピーしておく。  \n",
        "\n",
        "以下のセルでGoogle Driveをマウント。許可を求められるので許可する。"
      ],
      "metadata": {
        "id": "FCybl7DeJdTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9srYgT-KjDYo",
        "outputId": "c5f406a1-9d92-4191-a929-c12ab23527c9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ■追記（岡田）Colaboratory用\n",
        "chdirする。"
      ],
      "metadata": {
        "id": "XLDjLidAjHWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys,os\n",
        "os.chdir('/content/drive/My Drive/dl-from-scratch/ch08')\n",
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KTTetokbjNPs",
        "outputId": "ebb7e1fd-a523-47ed-8610-a19189e109a9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/dl-from-scratch/ch08'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2IomrEyf_2z"
      },
      "source": [
        "# ch08/deep_convnet.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lm44-C6tf_22"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
        "import pickle\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "from common.layers import *\n",
        "\n",
        "\n",
        "class DeepConvNet:\n",
        "    \"\"\"認識率99%以上の高精度なConvNet\n",
        "\n",
        "    ネットワーク構成は下記の通り\n",
        "        conv - relu - conv- relu - pool -\n",
        "        conv - relu - conv- relu - pool -\n",
        "        conv - relu - conv- relu - pool -\n",
        "        affine - relu - dropout - affine - dropout - softmax\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=(1, 28, 28),\n",
        "                 conv_param_1 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_2 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_4 = {'filter_num':32, 'filter_size':3, 'pad':2, 'stride':1},\n",
        "                 conv_param_5 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_6 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 hidden_size=50, output_size=10):\n",
        "        # 重みの初期化===========\n",
        "        # 各層のニューロンひとつあたりが、前層のニューロンといくつのつながりがあるか（TODO:自動で計算する）\n",
        "        pre_node_nums = np.array([1*3*3, 16*3*3, 16*3*3, 32*3*3, 32*3*3, 64*3*3, 64*4*4, hidden_size])\n",
        "        weight_init_scales = np.sqrt(2.0 / pre_node_nums)  # ReLUを使う場合に推奨される初期値\n",
        "        \n",
        "        self.params = {}\n",
        "        pre_channel_num = input_dim[0]\n",
        "        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\n",
        "            self.params['W' + str(idx+1)] = weight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
        "            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\n",
        "            pre_channel_num = conv_param['filter_num']\n",
        "        self.params['W7'] = weight_init_scales[6] * np.random.randn(64*4*4, hidden_size)\n",
        "        self.params['b7'] = np.zeros(hidden_size)\n",
        "        self.params['W8'] = weight_init_scales[7] * np.random.randn(hidden_size, output_size)\n",
        "        self.params['b8'] = np.zeros(output_size)\n",
        "\n",
        "        # レイヤの生成===========\n",
        "        self.layers = []\n",
        "        self.layers.append(Convolution(self.params['W1'], self.params['b1'], \n",
        "                           conv_param_1['stride'], conv_param_1['pad']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Convolution(self.params['W2'], self.params['b2'], \n",
        "                           conv_param_2['stride'], conv_param_2['pad']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
        "        self.layers.append(Convolution(self.params['W3'], self.params['b3'], \n",
        "                           conv_param_3['stride'], conv_param_3['pad']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Convolution(self.params['W4'], self.params['b4'],\n",
        "                           conv_param_4['stride'], conv_param_4['pad']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
        "        self.layers.append(Convolution(self.params['W5'], self.params['b5'],\n",
        "                           conv_param_5['stride'], conv_param_5['pad']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Convolution(self.params['W6'], self.params['b6'],\n",
        "                           conv_param_6['stride'], conv_param_6['pad']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
        "        self.layers.append(Affine(self.params['W7'], self.params['b7']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Dropout(0.5))\n",
        "        self.layers.append(Affine(self.params['W8'], self.params['b8']))\n",
        "        self.layers.append(Dropout(0.5))\n",
        "        \n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x, train_flg=False):\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, Dropout):\n",
        "                x = layer.forward(x, train_flg)\n",
        "            else:\n",
        "                x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x, train_flg=True)\n",
        "        return self.last_layer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t, batch_size=100):\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "\n",
        "        acc = 0.0\n",
        "\n",
        "        for i in range(int(x.shape[0] / batch_size)):\n",
        "            tx = x[i*batch_size:(i+1)*batch_size]\n",
        "            tt = t[i*batch_size:(i+1)*batch_size]\n",
        "            y = self.predict(tx, train_flg=False)\n",
        "            y = np.argmax(y, axis=1)\n",
        "            acc += np.sum(y == tt)\n",
        "\n",
        "        return acc / x.shape[0]\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        tmp_layers = self.layers.copy()\n",
        "        tmp_layers.reverse()\n",
        "        for layer in tmp_layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 設定\n",
        "        grads = {}\n",
        "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
        "            grads['W' + str(i+1)] = self.layers[layer_idx].dW\n",
        "            grads['b' + str(i+1)] = self.layers[layer_idx].db\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def save_params(self, file_name=\"params.pkl\"):\n",
        "        params = {}\n",
        "        for key, val in self.params.items():\n",
        "            params[key] = val\n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "    def load_params(self, file_name=\"params.pkl\"):\n",
        "        with open(file_name, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "        for key, val in params.items():\n",
        "            self.params[key] = val\n",
        "\n",
        "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
        "            self.layers[layer_idx].W = self.params['W' + str(i+1)]\n",
        "            self.layers[layer_idx].b = self.params['b' + str(i+1)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdiCvxwTf_25"
      },
      "source": [
        "# ch08/awesome_net.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3rEQ_Vyf_25"
      },
      "outputs": [],
      "source": [
        "# Create your awesome net!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj5ffyyPf_25"
      },
      "source": [
        "# ch08/train_deepnet.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "↓はものすごい時間がかかる。（約7.5時間）"
      ],
      "metadata": {
        "id": "Fae1TxSgcPA8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cY08g-Dwf_26",
        "outputId": "6ee1044a-d2b7-4fa9-cfbe-7655b27518ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mストリーミング出力は最後の 5000 行に切り捨てられました。\u001b[0m\n",
            "train loss:0.7853441934526242\n",
            "train loss:0.8745073979900053\n",
            "train loss:0.8317410801948563\n",
            "train loss:0.7612261673751177\n",
            "train loss:0.7697415554228173\n",
            "train loss:0.9766088875199266\n",
            "train loss:0.8223825773487793\n",
            "train loss:0.8468997975519315\n",
            "train loss:0.9575722353637801\n",
            "train loss:0.9355733399537347\n",
            "train loss:0.8484592636917383\n",
            "train loss:1.2307749485061616\n",
            "train loss:1.03637934951269\n",
            "train loss:0.8660050697135173\n",
            "train loss:0.7615253959456814\n",
            "train loss:0.875109958747451\n",
            "train loss:0.8203418673253323\n",
            "train loss:0.8780857608433188\n",
            "train loss:1.022246333694255\n",
            "train loss:0.9874290476975703\n",
            "train loss:0.810389761142937\n",
            "train loss:0.9296156316446824\n",
            "train loss:0.8848709876439041\n",
            "train loss:0.9590522063236954\n",
            "train loss:0.9159089077619924\n",
            "train loss:0.8736073511914328\n",
            "train loss:0.8972268052640703\n",
            "train loss:0.8219233725772473\n",
            "train loss:0.8992317638902718\n",
            "train loss:1.003422914644461\n",
            "train loss:0.7369333884042052\n",
            "train loss:0.7867894506113865\n",
            "train loss:0.9950562319004129\n",
            "train loss:0.8437973637046422\n",
            "train loss:0.8605046261768079\n",
            "train loss:0.7637602125934531\n",
            "train loss:0.8254147884413047\n",
            "train loss:0.8117222458915053\n",
            "train loss:0.8258498624750368\n",
            "train loss:0.8334250100874406\n",
            "train loss:0.9688179925986766\n",
            "train loss:0.9186140913297314\n",
            "train loss:0.8278114339244044\n",
            "train loss:0.9853203200723237\n",
            "train loss:0.9065030858426727\n",
            "train loss:0.904806982059534\n",
            "train loss:0.8215574599819565\n",
            "train loss:0.9357175802908596\n",
            "train loss:0.9572158565913892\n",
            "train loss:0.9328667888630303\n",
            "train loss:0.8631468097162291\n",
            "train loss:0.8429181052300896\n",
            "train loss:0.9708459840621755\n",
            "train loss:0.8902461246621969\n",
            "train loss:0.9862677256361759\n",
            "train loss:0.8439131136771718\n",
            "train loss:0.8532373633601493\n",
            "train loss:0.8581601317447191\n",
            "train loss:0.844004800759314\n",
            "train loss:0.747570328546987\n",
            "train loss:1.0037428272983409\n",
            "train loss:0.7408635265532504\n",
            "train loss:0.880143054827791\n",
            "train loss:0.9103999808368173\n",
            "train loss:0.8127482994639269\n",
            "train loss:1.0388149848086798\n",
            "train loss:1.0153198878169598\n",
            "train loss:0.8646291871927467\n",
            "train loss:0.9336054575666388\n",
            "train loss:0.9226607480877356\n",
            "train loss:0.7959362510789455\n",
            "train loss:0.9793390898774201\n",
            "train loss:0.9102967490036116\n",
            "train loss:0.8253135656854417\n",
            "train loss:0.7276483961469172\n",
            "train loss:1.0222638230813723\n",
            "train loss:0.8696611381814288\n",
            "train loss:0.8781014796926732\n",
            "train loss:0.9453238244140466\n",
            "train loss:0.7853326693459607\n",
            "train loss:0.9541808956435864\n",
            "train loss:0.8116312041169468\n",
            "train loss:0.9752673231800445\n",
            "train loss:0.67016210647819\n",
            "train loss:0.9606193583511954\n",
            "train loss:0.9639538089659223\n",
            "train loss:0.8795547527701773\n",
            "train loss:0.8926353944023881\n",
            "train loss:1.002557735340821\n",
            "train loss:1.056253541160596\n",
            "train loss:0.9047128353455541\n",
            "train loss:0.858495083079528\n",
            "train loss:0.779341060332119\n",
            "train loss:0.8106308991231124\n",
            "train loss:0.8599937119128792\n",
            "train loss:0.7616138801021868\n",
            "train loss:0.9303508668388075\n",
            "train loss:0.8999446520166474\n",
            "train loss:0.720004746366689\n",
            "train loss:0.763522206428118\n",
            "train loss:0.8037022029586052\n",
            "train loss:0.8827738656991351\n",
            "train loss:1.079288695526316\n",
            "train loss:0.9109617875239396\n",
            "train loss:0.9424157090670513\n",
            "train loss:1.0763053361653656\n",
            "train loss:0.7673624148375174\n",
            "train loss:0.8110148658632496\n",
            "train loss:0.7899231420552703\n",
            "train loss:0.9274969527375735\n",
            "train loss:1.040995280229058\n",
            "train loss:0.9189904385030613\n",
            "train loss:0.941171505809132\n",
            "train loss:0.737996295312855\n",
            "train loss:0.8786770908435737\n",
            "train loss:0.8613401069291068\n",
            "train loss:0.839670353585755\n",
            "train loss:0.943100946859554\n",
            "train loss:0.9401986991086373\n",
            "train loss:0.8590901958720265\n",
            "train loss:0.9756883953903893\n",
            "train loss:0.8384969219043059\n",
            "train loss:0.9447420975605866\n",
            "train loss:0.7030422662209126\n",
            "train loss:0.8684190455880616\n",
            "train loss:0.8687820107676777\n",
            "train loss:1.0256227236544064\n",
            "train loss:0.8901215907192481\n",
            "train loss:0.9176833877373133\n",
            "train loss:0.7948318373479931\n",
            "train loss:0.9425752285894704\n",
            "train loss:0.8932388553343794\n",
            "train loss:1.1261916495112838\n",
            "train loss:0.849873126533045\n",
            "train loss:0.9636789452279126\n",
            "train loss:0.7893186601623942\n",
            "train loss:1.0179106494617645\n",
            "train loss:0.7331629977135649\n",
            "train loss:0.9302236999059451\n",
            "train loss:0.9275172682232372\n",
            "train loss:0.6520832156943648\n",
            "train loss:0.9116440746321776\n",
            "train loss:0.9819176659333168\n",
            "train loss:0.8573715924835121\n",
            "train loss:0.9243741084144177\n",
            "train loss:0.7963853340732975\n",
            "train loss:0.8271515140178761\n",
            "train loss:0.9312981329832862\n",
            "train loss:0.772590381949096\n",
            "train loss:0.905847481508745\n",
            "train loss:0.7644208501599388\n",
            "train loss:0.9499042512301438\n",
            "train loss:0.9141832525711051\n",
            "train loss:0.867377036630245\n",
            "train loss:0.9212935226746773\n",
            "train loss:0.9070903873265224\n",
            "train loss:0.8904715068608523\n",
            "train loss:0.8614316988553918\n",
            "train loss:0.9656603056108085\n",
            "train loss:0.8964341524506945\n",
            "train loss:0.902406940889758\n",
            "train loss:0.9853447439532944\n",
            "train loss:0.726459364702601\n",
            "train loss:0.8506297023947397\n",
            "train loss:0.7850971011527029\n",
            "train loss:0.8602457237302447\n",
            "train loss:0.7913872581747706\n",
            "train loss:0.9248579504719089\n",
            "train loss:0.8599959187403554\n",
            "train loss:0.8247224373352186\n",
            "train loss:0.8242099437755397\n",
            "train loss:0.6866607584868689\n",
            "train loss:0.9418957072938435\n",
            "train loss:0.8007515894677871\n",
            "train loss:0.8485933631159257\n",
            "train loss:0.8566572412523322\n",
            "train loss:0.7636089306761267\n",
            "train loss:0.7530781681256539\n",
            "train loss:0.8581721465937037\n",
            "train loss:0.8415036767653986\n",
            "train loss:0.9953724876779695\n",
            "train loss:0.8625200938012353\n",
            "train loss:1.0313681449168972\n",
            "train loss:0.7605575723755558\n",
            "train loss:1.0422797041285596\n",
            "train loss:0.8610890885252068\n",
            "train loss:0.8213887507481058\n",
            "train loss:0.7398656991008825\n",
            "train loss:1.0686870641414057\n",
            "train loss:0.928029324563253\n",
            "=== epoch:13, train acc:0.998, test acc:0.993 ===\n",
            "train loss:0.9339466389236578\n",
            "train loss:1.102508262341134\n",
            "train loss:1.0054761667596344\n",
            "train loss:0.8141928784459103\n",
            "train loss:0.8659085304269715\n",
            "train loss:0.852123178429182\n",
            "train loss:0.8993262194639863\n",
            "train loss:0.8364201636128835\n",
            "train loss:0.7758496477685708\n",
            "train loss:0.8503746313810988\n",
            "train loss:0.7881737917460845\n",
            "train loss:0.7719957003715453\n",
            "train loss:0.8402531958052077\n",
            "train loss:0.8741734539655256\n",
            "train loss:0.8402212563925484\n",
            "train loss:1.0307087886334296\n",
            "train loss:0.918079282832157\n",
            "train loss:0.6349466258536737\n",
            "train loss:0.8993989884388496\n",
            "train loss:0.7275254552434296\n",
            "train loss:0.8715388997837692\n",
            "train loss:0.9808753102584131\n",
            "train loss:0.6977908580411711\n",
            "train loss:0.7776607452697621\n",
            "train loss:0.9448307354225253\n",
            "train loss:0.8543324283209445\n",
            "train loss:0.8543935077522954\n",
            "train loss:0.8882795204237044\n",
            "train loss:0.9206891495774595\n",
            "train loss:0.9429623661407379\n",
            "train loss:0.8682648322071138\n",
            "train loss:1.0374317873676926\n",
            "train loss:0.7841912016362168\n",
            "train loss:0.8953488085328876\n",
            "train loss:0.921993255055346\n",
            "train loss:0.9127882800804115\n",
            "train loss:0.7995021657938401\n",
            "train loss:0.8833128625103192\n",
            "train loss:0.8955570967747234\n",
            "train loss:0.8190616805469177\n",
            "train loss:0.8679496235664707\n",
            "train loss:0.8882785887967469\n",
            "train loss:0.9095647722834923\n",
            "train loss:0.8530702935195719\n",
            "train loss:0.8343413175193614\n",
            "train loss:0.8656519831029331\n",
            "train loss:0.8179393226780388\n",
            "train loss:0.8781494154647084\n",
            "train loss:1.0004292226698412\n",
            "train loss:1.1450503472570188\n",
            "train loss:0.7994723151931702\n",
            "train loss:0.6803872640658358\n",
            "train loss:1.0549001024559366\n",
            "train loss:0.9473374610459789\n",
            "train loss:0.9115159747023422\n",
            "train loss:0.88304435762171\n",
            "train loss:0.8460953148466299\n",
            "train loss:0.9474537740863201\n",
            "train loss:0.8869526285303894\n",
            "train loss:1.06824874023022\n",
            "train loss:0.7878457099502642\n",
            "train loss:0.7713900094640447\n",
            "train loss:0.9266757686870707\n",
            "train loss:0.9781085539637471\n",
            "train loss:0.9578963611826273\n",
            "train loss:0.8361366796610451\n",
            "train loss:1.024382191059854\n",
            "train loss:0.7844809001101608\n",
            "train loss:0.7522531190342034\n",
            "train loss:0.8613231435162909\n",
            "train loss:0.9214946776189241\n",
            "train loss:0.7549511456991803\n",
            "train loss:0.8599968416211832\n",
            "train loss:1.1110892490068403\n",
            "train loss:0.9958802359780649\n",
            "train loss:0.8622323864303639\n",
            "train loss:0.8166604462642992\n",
            "train loss:0.8598193926211514\n",
            "train loss:0.7614305055054905\n",
            "train loss:0.7464321123613125\n",
            "train loss:0.9543286692157548\n",
            "train loss:0.7825233406603266\n",
            "train loss:1.0110674937293795\n",
            "train loss:0.7478510917135719\n",
            "train loss:0.8675091498395358\n",
            "train loss:0.8975598405162369\n",
            "train loss:0.9364849425498611\n",
            "train loss:0.8780957430394675\n",
            "train loss:1.0306643560956845\n",
            "train loss:0.829235946406652\n",
            "train loss:1.004328662453444\n",
            "train loss:0.8199142050729997\n",
            "train loss:0.9117661053251545\n",
            "train loss:0.9536822960453613\n",
            "train loss:0.9262626550971446\n",
            "train loss:0.9484487729512463\n",
            "train loss:0.714070282275182\n",
            "train loss:0.8148397640570072\n",
            "train loss:0.8921267204429775\n",
            "train loss:0.8574956634156744\n",
            "train loss:0.8047468646488184\n",
            "train loss:1.038481966296416\n",
            "train loss:0.9662087213034938\n",
            "train loss:0.8727046911875376\n",
            "train loss:0.9469119727463905\n",
            "train loss:0.8999449645568296\n",
            "train loss:0.8709233014165947\n",
            "train loss:0.8533587089574236\n",
            "train loss:0.8363969941182865\n",
            "train loss:0.7235783285088818\n",
            "train loss:0.9215538808631214\n",
            "train loss:0.8910738264217841\n",
            "train loss:0.8246841969345767\n",
            "train loss:0.8938133082799492\n",
            "train loss:0.7330119263962436\n",
            "train loss:0.8194912358608224\n",
            "train loss:0.9302645058581115\n",
            "train loss:0.798191754548534\n",
            "train loss:0.7723361705769436\n",
            "train loss:0.8217811489366561\n",
            "train loss:0.9018389938381688\n",
            "train loss:0.9001311497450365\n",
            "train loss:1.0020406565634394\n",
            "train loss:0.9071849977015789\n",
            "train loss:0.9740254579657304\n",
            "train loss:0.9164946025140928\n",
            "train loss:1.017683953755908\n",
            "train loss:0.86301901807601\n",
            "train loss:0.889485455841927\n",
            "train loss:0.8921554768644948\n",
            "train loss:0.918754665011586\n",
            "train loss:0.6967480498321307\n",
            "train loss:0.8991713656624267\n",
            "train loss:0.9258545709564676\n",
            "train loss:1.0194461266127675\n",
            "train loss:0.7989739517447718\n",
            "train loss:0.855109125389689\n",
            "train loss:1.0623945015503538\n",
            "train loss:0.7908625202828427\n",
            "train loss:0.9340304460305769\n",
            "train loss:0.6896267851176561\n",
            "train loss:0.9480463692270336\n",
            "train loss:0.8081674845086213\n",
            "train loss:0.901293743615856\n",
            "train loss:0.8527491326732829\n",
            "train loss:0.9707098200411468\n",
            "train loss:0.8426033040067233\n",
            "train loss:0.9375652328843852\n",
            "train loss:0.7756423689124017\n",
            "train loss:0.8914152363039206\n",
            "train loss:0.8327227134783578\n",
            "train loss:0.8063340887109943\n",
            "train loss:0.857945602830734\n",
            "train loss:0.8524670427365433\n",
            "train loss:0.9417547517236923\n",
            "train loss:0.9888699327273016\n",
            "train loss:0.848270426923968\n",
            "train loss:0.9966020836770275\n",
            "train loss:0.9594509485532179\n",
            "train loss:0.9712890425696321\n",
            "train loss:0.7841564290469042\n",
            "train loss:0.7199589640824823\n",
            "train loss:0.7925834694495927\n",
            "train loss:0.7421281927851713\n",
            "train loss:0.906523753420928\n",
            "train loss:0.9663823106089248\n",
            "train loss:0.8829412931978985\n",
            "train loss:0.9853403693356946\n",
            "train loss:0.8900865006204244\n",
            "train loss:0.9080178962482711\n",
            "train loss:0.9005926762534107\n",
            "train loss:0.6245798825291663\n",
            "train loss:0.8894157896660082\n",
            "train loss:0.9991311431969723\n",
            "train loss:0.8567915783870991\n",
            "train loss:0.9377768197034902\n",
            "train loss:1.0059615135943003\n",
            "train loss:0.8500843946198288\n",
            "train loss:0.9333378825989352\n",
            "train loss:1.0313060012876742\n",
            "train loss:0.8292064163386431\n",
            "train loss:0.9666462387770952\n",
            "train loss:0.9005359018879332\n",
            "train loss:0.860140885166004\n",
            "train loss:1.003700165638101\n",
            "train loss:0.8129885591565543\n",
            "train loss:0.7694775923398879\n",
            "train loss:0.79868939705774\n",
            "train loss:0.8599635263239458\n",
            "train loss:0.935265511378783\n",
            "train loss:0.9436660427706158\n",
            "train loss:0.7179584253979664\n",
            "train loss:0.8037189331691884\n",
            "train loss:0.9123341065766398\n",
            "train loss:0.8080090163961566\n",
            "train loss:1.0723104858444124\n",
            "train loss:0.8572928372628842\n",
            "train loss:0.8114666568307279\n",
            "train loss:0.7047615049193203\n",
            "train loss:0.7682653336579369\n",
            "train loss:0.9112633715553574\n",
            "train loss:0.8841340568185582\n",
            "train loss:0.8693557384742708\n",
            "train loss:0.9104377740455233\n",
            "train loss:0.7333229833662723\n",
            "train loss:0.8032113791875476\n",
            "train loss:0.8632903130251489\n",
            "train loss:0.7726911236892486\n",
            "train loss:0.8705920347305023\n",
            "train loss:0.8641762088692153\n",
            "train loss:1.0767769599373742\n",
            "train loss:0.9866030126665751\n",
            "train loss:0.908632743093094\n",
            "train loss:0.8833773803396516\n",
            "train loss:0.8196013690983003\n",
            "train loss:0.7984839489379533\n",
            "train loss:1.1000940809078634\n",
            "train loss:0.8414999478434381\n",
            "train loss:0.9675555569545555\n",
            "train loss:0.9524090945218413\n",
            "train loss:0.9264648921796372\n",
            "train loss:0.7374356843354127\n",
            "train loss:0.9718000603622602\n",
            "train loss:0.9493188782074783\n",
            "train loss:0.7330165184777286\n",
            "train loss:1.0537990191665838\n",
            "train loss:0.8517970778086493\n",
            "train loss:0.721540651295697\n",
            "train loss:1.03835824841789\n",
            "train loss:0.806363485214209\n",
            "train loss:0.9047530283583111\n",
            "train loss:0.9602875999166456\n",
            "train loss:0.9201138670477532\n",
            "train loss:0.8755623194055451\n",
            "train loss:0.7916209212342946\n",
            "train loss:0.7524552415754202\n",
            "train loss:0.9384760289279458\n",
            "train loss:0.8507215723225968\n",
            "train loss:1.1308097444275866\n",
            "train loss:0.977685401074559\n",
            "train loss:0.8586212029812748\n",
            "train loss:0.8891828260967323\n",
            "train loss:0.9567919168521203\n",
            "train loss:0.7165419815187549\n",
            "train loss:1.063601422006105\n",
            "train loss:0.9782473032137586\n",
            "train loss:0.9125882739231009\n",
            "train loss:0.9317318737976976\n",
            "train loss:0.8787555614395833\n",
            "train loss:0.9581676189880998\n",
            "train loss:1.1339111534888868\n",
            "train loss:0.8531456720812869\n",
            "train loss:0.821731008785318\n",
            "train loss:0.7738253519996747\n",
            "train loss:1.0087253858643206\n",
            "train loss:0.8311759642916714\n",
            "train loss:0.7679826117538084\n",
            "train loss:0.9289688597247922\n",
            "train loss:0.8198909520982696\n",
            "train loss:0.6767446911966626\n",
            "train loss:0.918639387464479\n",
            "train loss:0.9214640700191217\n",
            "train loss:0.8182361285378907\n",
            "train loss:0.9301329093181149\n",
            "train loss:0.9641582637399451\n",
            "train loss:0.9073697451805036\n",
            "train loss:0.8847829592702072\n",
            "train loss:0.9385319426026709\n",
            "train loss:0.8070887072616577\n",
            "train loss:0.9007645817992896\n",
            "train loss:0.8143657589183237\n",
            "train loss:0.8621712035712248\n",
            "train loss:0.8802695162202638\n",
            "train loss:0.7492352131349775\n",
            "train loss:0.9590587469663404\n",
            "train loss:0.7859740558028823\n",
            "train loss:0.9108122647712062\n",
            "train loss:0.9189297511914161\n",
            "train loss:1.0425002478092824\n",
            "train loss:0.9692127587519005\n",
            "train loss:0.8739329205421786\n",
            "train loss:0.8365537617107166\n",
            "train loss:0.9827782736855946\n",
            "train loss:0.9281327231731297\n",
            "train loss:0.9688532421209999\n",
            "train loss:0.838459946910695\n",
            "train loss:0.8178859400200682\n",
            "train loss:0.8723994702584186\n",
            "train loss:1.0086529083665352\n",
            "train loss:0.855695228404972\n",
            "train loss:0.9055639329795844\n",
            "train loss:0.898685038702059\n",
            "train loss:0.9082865662300823\n",
            "train loss:0.8859474640972208\n",
            "train loss:0.8303274811479043\n",
            "train loss:0.8909850089646221\n",
            "train loss:0.8630775367652294\n",
            "train loss:0.8675631936008215\n",
            "train loss:0.7972351778075822\n",
            "train loss:0.7424332384611358\n",
            "train loss:0.8666107904153993\n",
            "train loss:0.7943864741055653\n",
            "train loss:0.9017486590543698\n",
            "train loss:0.7933539463415626\n",
            "train loss:0.9315506330580869\n",
            "train loss:0.9584894704462634\n",
            "train loss:0.7044718454095841\n",
            "train loss:0.7162982469332239\n",
            "train loss:0.7617892724656984\n",
            "train loss:0.727454759048123\n",
            "train loss:0.899685379628094\n",
            "train loss:0.8563410509228997\n",
            "train loss:0.8493987405772147\n",
            "train loss:0.7743027622395711\n",
            "train loss:0.8946576496247027\n",
            "train loss:0.9683605376425283\n",
            "train loss:0.8274524284442712\n",
            "train loss:0.7980939301437638\n",
            "train loss:0.8114737511670829\n",
            "train loss:0.8863041848035118\n",
            "train loss:0.8756753543008103\n",
            "train loss:0.9564517195844218\n",
            "train loss:0.8512420792103705\n",
            "train loss:1.052929499428081\n",
            "train loss:0.8737314802632974\n",
            "train loss:0.9052806493327666\n",
            "train loss:0.8893847087551552\n",
            "train loss:0.8299392753177299\n",
            "train loss:0.8445968522579703\n",
            "train loss:0.9977029813314965\n",
            "train loss:0.8676493888092955\n",
            "train loss:0.9029728008738037\n",
            "train loss:0.9732573373947844\n",
            "train loss:0.8837660907604663\n",
            "train loss:0.880002453624187\n",
            "train loss:0.8314688654819181\n",
            "train loss:0.9537302967670181\n",
            "train loss:1.0765042735869776\n",
            "train loss:1.055410668203629\n",
            "train loss:0.9263951992874837\n",
            "train loss:0.8145514834630818\n",
            "train loss:0.7402419138460374\n",
            "train loss:0.8743769239053186\n",
            "train loss:0.814224059663836\n",
            "train loss:0.9220877840424653\n",
            "train loss:0.7851334874036167\n",
            "train loss:0.7903855003490275\n",
            "train loss:0.9724099637045068\n",
            "train loss:0.9403076432432566\n",
            "train loss:0.8037495768375353\n",
            "train loss:0.6918083936300546\n",
            "train loss:0.9625144798592844\n",
            "train loss:0.7705748451747333\n",
            "train loss:0.8863608299416856\n",
            "train loss:0.7763900255400615\n",
            "train loss:0.8159558477195833\n",
            "train loss:0.7513445134731771\n",
            "train loss:0.8519259385460043\n",
            "train loss:0.8539960902248065\n",
            "train loss:0.7838420205188925\n",
            "train loss:0.8257190542538574\n",
            "train loss:0.8591824528364082\n",
            "train loss:0.7760745357635802\n",
            "train loss:0.8801537109752472\n",
            "train loss:0.812628255100594\n",
            "train loss:0.8059130569831433\n",
            "train loss:1.0898901191038841\n",
            "train loss:0.7737759007850181\n",
            "train loss:0.8761424664871054\n",
            "train loss:0.8197628153180447\n",
            "train loss:0.8818714785458341\n",
            "train loss:0.8480968010622167\n",
            "train loss:0.8652999460509102\n",
            "train loss:0.9757895157889888\n",
            "train loss:1.0008124875774431\n",
            "train loss:0.952757045521923\n",
            "train loss:0.9130746708213622\n",
            "train loss:0.930236599338868\n",
            "train loss:0.9799727077329947\n",
            "train loss:0.9049844611490144\n",
            "train loss:0.8811839566906481\n",
            "train loss:0.7598120398363035\n",
            "train loss:0.9072343533208022\n",
            "train loss:0.8766740364334231\n",
            "train loss:0.8575588628898545\n",
            "train loss:0.9418409397956519\n",
            "train loss:0.865193652335831\n",
            "train loss:0.7592019544143939\n",
            "train loss:0.8065697726814327\n",
            "train loss:0.6822334959749441\n",
            "train loss:1.041084237467706\n",
            "train loss:0.9509518456748755\n",
            "train loss:0.8859586439771736\n",
            "train loss:0.9667357891730672\n",
            "train loss:0.861186105473081\n",
            "train loss:0.9058928184454069\n",
            "train loss:0.9517046097013717\n",
            "train loss:1.0578820860720632\n",
            "train loss:0.9273414696468331\n",
            "train loss:1.017184055972794\n",
            "train loss:0.8769690543613342\n",
            "train loss:0.8630207967364738\n",
            "train loss:0.8531035309487771\n",
            "train loss:0.943976113423289\n",
            "train loss:0.9641045037320954\n",
            "train loss:0.9014359151017476\n",
            "train loss:0.8682127747461653\n",
            "train loss:0.906411327996236\n",
            "train loss:0.7821483756996337\n",
            "train loss:0.9093025268881486\n",
            "train loss:0.8184636452956867\n",
            "train loss:0.9137416873912787\n",
            "train loss:0.8780347408646492\n",
            "train loss:1.0982743417482232\n",
            "train loss:0.863502108793092\n",
            "train loss:0.9028092482840941\n",
            "train loss:0.8876181773494481\n",
            "train loss:0.9307621709420479\n",
            "train loss:0.8167535398954564\n",
            "train loss:0.9353932695240041\n",
            "train loss:0.8059728025534977\n",
            "train loss:0.8359980301557489\n",
            "train loss:0.9200777240988272\n",
            "train loss:1.1193904371654824\n",
            "train loss:0.8560715821661726\n",
            "train loss:1.0073669650806392\n",
            "train loss:0.8717142173494722\n",
            "train loss:0.7900792705584097\n",
            "train loss:0.8154412006314352\n",
            "train loss:0.8414362111801625\n",
            "train loss:0.8359577849030435\n",
            "train loss:0.8516008060149973\n",
            "train loss:0.8789076078436119\n",
            "train loss:0.8179822429918379\n",
            "train loss:0.8420529838950266\n",
            "train loss:0.7595722385895215\n",
            "train loss:0.8716451765142648\n",
            "train loss:0.8656642832621927\n",
            "train loss:0.8900817742698512\n",
            "train loss:0.8281051893993591\n",
            "train loss:0.8340706345920001\n",
            "train loss:1.0199772034906958\n",
            "train loss:0.8338402997468193\n",
            "train loss:0.7479528721223468\n",
            "train loss:1.000797233394114\n",
            "train loss:0.769484894008999\n",
            "train loss:0.9410850543881315\n",
            "train loss:0.9691597442042493\n",
            "train loss:0.8240990131985487\n",
            "train loss:1.0688111667942024\n",
            "train loss:0.9911885085706257\n",
            "train loss:0.9497975590064095\n",
            "train loss:0.9924793380943059\n",
            "train loss:0.9416338273229118\n",
            "train loss:1.0498583157609997\n",
            "train loss:0.8777429441881744\n",
            "train loss:0.8303709390116835\n",
            "train loss:0.9118617031488953\n",
            "train loss:0.829615794618822\n",
            "train loss:0.8529532787342129\n",
            "train loss:0.8753295804698227\n",
            "train loss:0.7811684965823912\n",
            "train loss:0.8375302214523215\n",
            "train loss:0.8053420960450473\n",
            "train loss:0.9782155519998955\n",
            "train loss:0.9383754880560434\n",
            "train loss:0.7738066709590231\n",
            "train loss:0.8153799742255724\n",
            "train loss:1.0142683038923523\n",
            "train loss:0.8598545271623302\n",
            "train loss:0.8075290101335253\n",
            "train loss:0.7849769866675149\n",
            "train loss:0.7855224737351986\n",
            "train loss:0.9862790318373384\n",
            "train loss:0.8466816461002403\n",
            "train loss:0.8768070570001961\n",
            "train loss:0.8977286427363239\n",
            "train loss:0.9175126546697924\n",
            "train loss:0.7890158842613921\n",
            "train loss:0.662879868557667\n",
            "train loss:0.9904399104105542\n",
            "train loss:0.9012626088213217\n",
            "train loss:0.9180206126692021\n",
            "train loss:0.7809012349113479\n",
            "train loss:0.9200809580795664\n",
            "train loss:0.9187684035536631\n",
            "train loss:0.8302330006191118\n",
            "train loss:0.8544208677329722\n",
            "train loss:0.8120299115041867\n",
            "train loss:0.8116783237518326\n",
            "train loss:0.9758733885550969\n",
            "train loss:1.1284044721218554\n",
            "train loss:0.9464402095489619\n",
            "train loss:1.010271785477055\n",
            "train loss:0.843379668697467\n",
            "train loss:0.8187793188124036\n",
            "train loss:1.0127311262440217\n",
            "train loss:0.8444454214604393\n",
            "train loss:0.8494707662141284\n",
            "train loss:0.8564022547411873\n",
            "train loss:0.9308374472288182\n",
            "train loss:0.9804653486038998\n",
            "train loss:0.6461025666125927\n",
            "train loss:0.7772861055456903\n",
            "train loss:0.868613402134282\n",
            "train loss:0.7574062502740615\n",
            "train loss:0.8924717712451805\n",
            "train loss:0.7063600756327857\n",
            "train loss:0.8042021173648465\n",
            "train loss:0.8501000782027164\n",
            "train loss:0.7638465331575242\n",
            "train loss:0.6490469477676002\n",
            "train loss:0.8106768799751376\n",
            "train loss:0.7901136779010443\n",
            "train loss:0.9009760779568629\n",
            "train loss:0.9207900022560024\n",
            "train loss:0.6288637054925078\n",
            "train loss:1.0840538907414754\n",
            "train loss:0.9350356220808314\n",
            "train loss:0.9525545630192124\n",
            "train loss:0.7840281035448071\n",
            "train loss:1.0044067559636554\n",
            "train loss:0.8722105871202372\n",
            "train loss:0.8859191665487788\n",
            "train loss:0.8439935104038893\n",
            "train loss:0.9812405734295586\n",
            "train loss:0.9084754885157332\n",
            "train loss:0.8828965923889851\n",
            "train loss:0.8884611504045579\n",
            "train loss:0.8759501502081616\n",
            "train loss:0.8871877999533951\n",
            "train loss:0.7827535307425163\n",
            "train loss:0.7764461533633183\n",
            "train loss:0.8744384188906509\n",
            "train loss:0.8274058850937668\n",
            "train loss:0.8509288904667798\n",
            "train loss:0.8206554067581994\n",
            "train loss:1.056678882425309\n",
            "train loss:0.8647404157454507\n",
            "train loss:0.8542141972408552\n",
            "train loss:0.8715344821217692\n",
            "train loss:0.8326329325074845\n",
            "train loss:0.8911224662598108\n",
            "train loss:0.8339349674436941\n",
            "train loss:1.058772190828983\n",
            "train loss:0.9541453151667656\n",
            "train loss:0.9677261162612225\n",
            "train loss:0.8319631892315933\n",
            "train loss:0.9358575993425245\n",
            "train loss:0.7765201269494166\n",
            "train loss:0.8119100198253344\n",
            "train loss:0.8829747745739148\n",
            "train loss:0.767905273616797\n",
            "train loss:0.8351210993511897\n",
            "train loss:0.7001693567150185\n",
            "train loss:0.7647104034207102\n",
            "train loss:0.8286941838120129\n",
            "train loss:0.8014006232139198\n",
            "train loss:0.7582425427214986\n",
            "train loss:0.9413288954019613\n",
            "train loss:0.9660134659795416\n",
            "train loss:0.7736365525921279\n",
            "train loss:0.9856291015581716\n",
            "train loss:0.6958482348928746\n",
            "train loss:0.9695603594765331\n",
            "train loss:0.9024192397408064\n",
            "train loss:0.8355318688745847\n",
            "train loss:0.8911131843275544\n",
            "train loss:0.7929415069990068\n",
            "train loss:0.8670183245094505\n",
            "train loss:0.7752139657668689\n",
            "train loss:0.9393925947698896\n",
            "train loss:0.7791190050977511\n",
            "train loss:0.8759765627434861\n",
            "train loss:0.9015797026479753\n",
            "train loss:0.8122351175220396\n",
            "train loss:0.9027375459630123\n",
            "train loss:0.9528341243334237\n",
            "train loss:0.744935011958257\n",
            "train loss:0.8940514363580683\n",
            "train loss:0.9125851249812807\n",
            "train loss:1.0579335130049288\n",
            "train loss:0.7675042641326655\n",
            "train loss:0.7843239825574776\n",
            "train loss:0.8083647069823389\n",
            "train loss:0.9108747928047355\n",
            "train loss:1.0145474603584574\n",
            "train loss:0.8047800126369197\n",
            "train loss:0.7648305718388373\n",
            "train loss:0.7644120716834056\n",
            "train loss:0.9752898147662893\n",
            "train loss:0.6687616356407399\n",
            "train loss:0.9067051547438375\n",
            "train loss:0.8812428975927041\n",
            "train loss:0.941088374936035\n",
            "train loss:0.8253618431135284\n",
            "train loss:0.8012059772583785\n",
            "train loss:0.8073330146977757\n",
            "train loss:0.8382230341334888\n",
            "train loss:0.9346045092166922\n",
            "=== epoch:14, train acc:0.997, test acc:0.991 ===\n",
            "train loss:0.8818994225012792\n",
            "train loss:0.967879144722357\n",
            "train loss:0.92243348800555\n",
            "train loss:0.7073274419782473\n",
            "train loss:0.825315262079071\n",
            "train loss:0.8403754664831644\n",
            "train loss:0.883952081050571\n",
            "train loss:0.7472946153915369\n",
            "train loss:0.9397608527824377\n",
            "train loss:0.8433177732576251\n",
            "train loss:0.7678862068170578\n",
            "train loss:0.8319818096067391\n",
            "train loss:0.85434707758372\n",
            "train loss:0.7779945380471252\n",
            "train loss:0.8949734596672226\n",
            "train loss:0.8648792127403702\n",
            "train loss:0.9090982702757967\n",
            "train loss:0.7307613427647536\n",
            "train loss:0.848435877955433\n",
            "train loss:0.8495710437760667\n",
            "train loss:0.8813443081571763\n",
            "train loss:0.8857998790748032\n",
            "train loss:0.8554036965968557\n",
            "train loss:0.71002194996782\n",
            "train loss:0.8629547727666734\n",
            "train loss:0.7420508707014257\n",
            "train loss:0.8059552898457889\n",
            "train loss:0.894913808014212\n",
            "train loss:0.7655563096525988\n",
            "train loss:0.9215219021993947\n",
            "train loss:0.8626915286576249\n",
            "train loss:0.861575151401234\n",
            "train loss:1.03374876649302\n",
            "train loss:0.8614392906745887\n",
            "train loss:0.9186489350064106\n",
            "train loss:1.0328352167777377\n",
            "train loss:0.944427299443808\n",
            "train loss:0.7484088719032471\n",
            "train loss:0.76515772064565\n",
            "train loss:0.7980833477171315\n",
            "train loss:0.8735417268092935\n",
            "train loss:0.8797885710129902\n",
            "train loss:0.8807528352900451\n",
            "train loss:0.9575512566830752\n",
            "train loss:1.0592781118545929\n",
            "train loss:0.9850562462098579\n",
            "train loss:0.8442340942977375\n",
            "train loss:0.8764819928749651\n",
            "train loss:0.7733510059265738\n",
            "train loss:0.7263595371664472\n",
            "train loss:0.8353890552204974\n",
            "train loss:0.8023369343006149\n",
            "train loss:0.846446454568047\n",
            "train loss:0.910787816071668\n",
            "train loss:0.9244738533194514\n",
            "train loss:0.8860701199065325\n",
            "train loss:0.7347328549680965\n",
            "train loss:0.7810973992528694\n",
            "train loss:0.8388919954176727\n",
            "train loss:0.9018070018812355\n",
            "train loss:1.0938822071662009\n",
            "train loss:0.7854409184223883\n",
            "train loss:1.047550861521449\n",
            "train loss:0.7873627682610163\n",
            "train loss:0.7513955092102809\n",
            "train loss:0.9765674645575787\n",
            "train loss:0.8885664303490596\n",
            "train loss:1.0080797068136693\n",
            "train loss:0.925367013252917\n",
            "train loss:0.9388155016010031\n",
            "train loss:0.8191609334198852\n",
            "train loss:0.8500077484805274\n",
            "train loss:0.979555562010602\n",
            "train loss:1.0202916977095113\n",
            "train loss:0.7726772180729765\n",
            "train loss:0.7492995536707386\n",
            "train loss:0.7021837383272053\n",
            "train loss:0.8975717882329917\n",
            "train loss:0.8723812883662664\n",
            "train loss:0.8652212721737367\n",
            "train loss:0.8084782835453171\n",
            "train loss:0.84030844371849\n",
            "train loss:0.9355319719922955\n",
            "train loss:0.914162835106984\n",
            "train loss:0.9257738593566437\n",
            "train loss:0.9458783920650928\n",
            "train loss:0.9130327030777359\n",
            "train loss:0.7697189549944926\n",
            "train loss:0.8793175014362276\n",
            "train loss:0.888495517072133\n",
            "train loss:0.8251751511123683\n",
            "train loss:0.893800928507703\n",
            "train loss:0.8824534722783394\n",
            "train loss:0.8141047344951354\n",
            "train loss:0.9365330285478312\n",
            "train loss:0.7671101381639188\n",
            "train loss:0.8716428449110545\n",
            "train loss:1.027773455590549\n",
            "train loss:0.9032114118364344\n",
            "train loss:1.091049413200869\n",
            "train loss:0.9779780696336478\n",
            "train loss:0.9577603795527088\n",
            "train loss:0.9012751431615277\n",
            "train loss:1.017944987893779\n",
            "train loss:0.7975007737896536\n",
            "train loss:0.7410160764048258\n",
            "train loss:0.8717550376297402\n",
            "train loss:0.8607939748455641\n",
            "train loss:0.7705640422293841\n",
            "train loss:0.8385709700549627\n",
            "train loss:0.9098103798171715\n",
            "train loss:0.7191615038470119\n",
            "train loss:0.800146158323265\n",
            "train loss:0.8599513525016037\n",
            "train loss:0.896774401700156\n",
            "train loss:0.8737920679174007\n",
            "train loss:0.802583311249366\n",
            "train loss:0.8114184473435054\n",
            "train loss:0.8223933979527467\n",
            "train loss:0.8915140361764388\n",
            "train loss:0.8816194941957023\n",
            "train loss:0.8268355202664344\n",
            "train loss:1.084695571760316\n",
            "train loss:0.8047292755832784\n",
            "train loss:0.8845110905309277\n",
            "train loss:0.7923551137151196\n",
            "train loss:0.8835982554570248\n",
            "train loss:0.8157831954195817\n",
            "train loss:0.9291224756915213\n",
            "train loss:0.979818138255196\n",
            "train loss:0.6865574648234148\n",
            "train loss:0.8146957157022189\n",
            "train loss:0.8924260827318966\n",
            "train loss:0.9948219992228773\n",
            "train loss:0.8757045710730977\n",
            "train loss:0.8641957683672997\n",
            "train loss:1.0294264529630623\n",
            "train loss:0.8702734990104888\n",
            "train loss:1.0725815192467574\n",
            "train loss:0.9171800758975376\n",
            "train loss:0.8268843400904697\n",
            "train loss:0.9036015148710465\n",
            "train loss:0.8065757624424689\n",
            "train loss:1.074974252016914\n",
            "train loss:1.0551258265951113\n",
            "train loss:0.9335138431639827\n",
            "train loss:0.7558306159849508\n",
            "train loss:0.9604221495182018\n",
            "train loss:0.9661413328681457\n",
            "train loss:0.9053995645836578\n",
            "train loss:0.9987595906490201\n",
            "train loss:0.8121234283202332\n",
            "train loss:0.9518473763643925\n",
            "train loss:0.8336617097929414\n",
            "train loss:0.9815738579396311\n",
            "train loss:0.9564075547908284\n",
            "train loss:0.9371191764936939\n",
            "train loss:0.7897430973559649\n",
            "train loss:0.9252671416658529\n",
            "train loss:0.9010881252863198\n",
            "train loss:0.94378142363368\n",
            "train loss:0.7750455431365961\n",
            "train loss:0.7138479500214987\n",
            "train loss:0.8260644861160634\n",
            "train loss:0.8616494694909662\n",
            "train loss:0.9177179386455196\n",
            "train loss:0.8521976436026129\n",
            "train loss:0.8772517853704993\n",
            "train loss:0.826940709295322\n",
            "train loss:0.8864080765033858\n",
            "train loss:0.9410749461827034\n",
            "train loss:0.8269825862401241\n",
            "train loss:0.8155811311080241\n",
            "train loss:0.974498534597305\n",
            "train loss:1.0184011206702328\n",
            "train loss:0.9696733211496155\n",
            "train loss:0.857856910277241\n",
            "train loss:0.7117795415205975\n",
            "train loss:1.0588927225944493\n",
            "train loss:0.8712304172542352\n",
            "train loss:0.8550203960620958\n",
            "train loss:0.9669067486812599\n",
            "train loss:0.9748455664825574\n",
            "train loss:0.8058938291707947\n",
            "train loss:0.8038710014356338\n",
            "train loss:0.7839184716618207\n",
            "train loss:0.9684757221365534\n",
            "train loss:0.9902173697848572\n",
            "train loss:0.9285976328217541\n",
            "train loss:0.9698650443814314\n",
            "train loss:0.8193827164280524\n",
            "train loss:0.7055104353386459\n",
            "train loss:0.9826706075343961\n",
            "train loss:0.9400449703823965\n",
            "train loss:0.7433318729761893\n",
            "train loss:0.9511087747368613\n",
            "train loss:0.815885220928618\n",
            "train loss:0.9357307933804926\n",
            "train loss:0.8886728065207774\n",
            "train loss:0.6998828807068042\n",
            "train loss:0.8868086081439053\n",
            "train loss:0.8101726716372406\n",
            "train loss:0.7466822074104468\n",
            "train loss:0.8882323498818426\n",
            "train loss:0.9752680831393328\n",
            "train loss:0.913186006099329\n",
            "train loss:0.8295867971814581\n",
            "train loss:0.735655933264036\n",
            "train loss:0.8014848319822186\n",
            "train loss:0.8679679575374195\n",
            "train loss:0.7807654163327813\n",
            "train loss:0.9255129530179275\n",
            "train loss:0.9647351036005705\n",
            "train loss:0.7815012253627244\n",
            "train loss:0.8904053645316112\n",
            "train loss:0.81623691480687\n",
            "train loss:0.7644342926376437\n",
            "train loss:0.8840455528181266\n",
            "train loss:0.9575410568599731\n",
            "train loss:0.9430614140119863\n",
            "train loss:0.8362367633598357\n",
            "train loss:0.7158740955921862\n",
            "train loss:1.016449924254527\n",
            "train loss:0.8591486178039389\n",
            "train loss:0.9786095824090669\n",
            "train loss:0.7979777650457328\n",
            "train loss:0.9084817649720794\n",
            "train loss:0.9677555555453908\n",
            "train loss:0.9562834735261135\n",
            "train loss:0.9138906484873168\n",
            "train loss:0.7900607182504987\n",
            "train loss:0.8421459801586734\n",
            "train loss:0.8539248928114617\n",
            "train loss:0.8768828925326266\n",
            "train loss:0.7517056822540575\n",
            "train loss:0.8158230546213785\n",
            "train loss:0.7934117087368613\n",
            "train loss:0.9964856251726242\n",
            "train loss:0.9235818720986382\n",
            "train loss:0.9559286627530719\n",
            "train loss:0.9632780417468182\n",
            "train loss:0.9121777294582922\n",
            "train loss:0.8394225073496788\n",
            "train loss:0.7465077105440325\n",
            "train loss:0.744153925464243\n",
            "train loss:0.9483179741565888\n",
            "train loss:0.9425920060450572\n",
            "train loss:0.9565454943514021\n",
            "train loss:0.8228049677525022\n",
            "train loss:0.9025210886200867\n",
            "train loss:0.9175103408928124\n",
            "train loss:0.8757752453572235\n",
            "train loss:0.8730576051890013\n",
            "train loss:0.8506462885872486\n",
            "train loss:0.8375611813421334\n",
            "train loss:0.9312737416388609\n",
            "train loss:0.955091415876713\n",
            "train loss:0.9617181848453249\n",
            "train loss:0.9979687587262666\n",
            "train loss:0.7249329999673613\n",
            "train loss:0.7514736206612729\n",
            "train loss:0.8093682771913768\n",
            "train loss:0.9605503392465934\n",
            "train loss:0.8373136274008351\n",
            "train loss:0.8026520662521667\n",
            "train loss:0.9414412102375824\n",
            "train loss:0.7226894687335828\n",
            "train loss:0.830331914672858\n",
            "train loss:0.8765674935756542\n",
            "train loss:1.0171949049268276\n",
            "train loss:0.8573325504314966\n",
            "train loss:0.8558378078362319\n",
            "train loss:0.8444631300608806\n",
            "train loss:0.8297907700049244\n",
            "train loss:0.8021401082763342\n",
            "train loss:0.9298501813894585\n",
            "train loss:0.8715984238407332\n",
            "train loss:1.0492055791984434\n",
            "train loss:0.9059940302543029\n",
            "train loss:0.9137236699900224\n",
            "train loss:0.9199731157628435\n",
            "train loss:0.9452972800343078\n",
            "train loss:0.9081771446728094\n",
            "train loss:0.7488884572770677\n",
            "train loss:1.0324356479786432\n",
            "train loss:0.9531004101086515\n",
            "train loss:0.8778609570740433\n",
            "train loss:0.8789541676074498\n",
            "train loss:0.9228701134192926\n",
            "train loss:0.9167058741193138\n",
            "train loss:0.9134728206925677\n",
            "train loss:0.9654691969218251\n",
            "train loss:0.8668851916417586\n",
            "train loss:0.9594145106246392\n",
            "train loss:0.8736637565095529\n",
            "train loss:1.332291238239365\n",
            "train loss:0.9181302508983673\n",
            "train loss:0.9638281081699834\n",
            "train loss:0.9867812885128359\n",
            "train loss:0.9747119690886451\n",
            "train loss:0.9191199058695715\n",
            "train loss:1.056845456341528\n",
            "train loss:0.9440810577027363\n",
            "train loss:0.8447017405679427\n",
            "train loss:0.9451234760400257\n",
            "train loss:0.8382842320883835\n",
            "train loss:0.8202594683326181\n",
            "train loss:0.9087117076692914\n",
            "train loss:0.8249215276515669\n",
            "train loss:0.8971784029022511\n",
            "train loss:0.8711922603134608\n",
            "train loss:0.7788036305145735\n",
            "train loss:0.9282586639546768\n",
            "train loss:0.7893805243581729\n",
            "train loss:0.9038887565078184\n",
            "train loss:0.7317326223012648\n",
            "train loss:1.0952789694686726\n",
            "train loss:0.8161097446926945\n",
            "train loss:0.887669698281191\n",
            "train loss:0.9348815139255516\n",
            "train loss:0.811948978124926\n",
            "train loss:0.9967611998848995\n",
            "train loss:0.9479825835386997\n",
            "train loss:0.8919230241484536\n",
            "train loss:0.7714120793353146\n",
            "train loss:0.8279825505781241\n",
            "train loss:0.9572963373391437\n",
            "train loss:0.7518280095606366\n",
            "train loss:0.7371348629745919\n",
            "train loss:0.8447203711053289\n",
            "train loss:0.972076999866609\n",
            "train loss:1.077624574230478\n",
            "train loss:0.8083533054067976\n",
            "train loss:0.9192675856934034\n",
            "train loss:1.0025247731420643\n",
            "train loss:1.0067007615201309\n",
            "train loss:0.7310155623842605\n",
            "train loss:0.8370249446054079\n",
            "train loss:0.9263701904866813\n",
            "train loss:0.8084498419112742\n",
            "train loss:0.9066618082160235\n",
            "train loss:0.8549935826783414\n",
            "train loss:0.6951640835073516\n",
            "train loss:0.9129311979561432\n",
            "train loss:0.6878409819961661\n",
            "train loss:0.9567150139814228\n",
            "train loss:0.7980784285800923\n",
            "train loss:0.881283748417228\n",
            "train loss:0.8495235793443371\n",
            "train loss:0.9109856141052327\n",
            "train loss:0.8113367854679311\n",
            "train loss:0.9782886719569278\n",
            "train loss:0.8944595997609405\n",
            "train loss:0.8710719054175843\n",
            "train loss:1.040133492798791\n",
            "train loss:0.9123416684388127\n",
            "train loss:0.9038792525311894\n",
            "train loss:0.7561613826018614\n",
            "train loss:0.8732433301715785\n",
            "train loss:0.8379267643013785\n",
            "train loss:0.8459805003722148\n",
            "train loss:0.8879990570933264\n",
            "train loss:0.9019002137241517\n",
            "train loss:0.9319532522469516\n",
            "train loss:0.918122992895855\n",
            "train loss:0.8148448616126331\n",
            "train loss:0.8269044015448059\n",
            "train loss:1.0198125803291958\n",
            "train loss:0.8001047076365926\n",
            "train loss:0.9708156387180544\n",
            "train loss:0.7959218897645808\n",
            "train loss:0.9973217140114963\n",
            "train loss:1.0563502807921739\n",
            "train loss:0.8849377828628601\n",
            "train loss:0.8334293385458489\n",
            "train loss:0.8924739908063651\n",
            "train loss:0.989512030711012\n",
            "train loss:0.9078805245880632\n",
            "train loss:0.9714510263054024\n",
            "train loss:0.8300379997699033\n",
            "train loss:0.8446958377978045\n",
            "train loss:0.9478187140908697\n",
            "train loss:0.7389053153649795\n",
            "train loss:0.8083897321789856\n",
            "train loss:0.7002019226661383\n",
            "train loss:0.9489501833763804\n",
            "train loss:0.7796088932492179\n",
            "train loss:1.0357003494119927\n",
            "train loss:0.8413872445726899\n",
            "train loss:0.7735721063352851\n",
            "train loss:1.0405221946774694\n",
            "train loss:1.010890952950964\n",
            "train loss:0.9609766993693032\n",
            "train loss:0.9715299811078046\n",
            "train loss:0.977430912844674\n",
            "train loss:0.8045959733175218\n",
            "train loss:0.9079756209000361\n",
            "train loss:0.7669978522556726\n",
            "train loss:0.9725578772882225\n",
            "train loss:0.8089307432657322\n",
            "train loss:0.842784024843463\n",
            "train loss:0.9675430928373413\n",
            "train loss:0.8522166657667885\n",
            "train loss:0.8992328094173331\n",
            "train loss:0.8443330163858913\n",
            "train loss:0.9846899905054453\n",
            "train loss:0.8354679299689667\n",
            "train loss:0.7852699560903575\n",
            "train loss:0.9894557222850637\n",
            "train loss:0.8143827163069878\n",
            "train loss:0.7918409419240313\n",
            "train loss:0.7494674923875253\n",
            "train loss:0.9753415240621974\n",
            "train loss:0.6561030935722441\n",
            "train loss:0.8829032954808441\n",
            "train loss:0.9466561077816062\n",
            "train loss:0.9069750153442573\n",
            "train loss:0.7239599482082015\n",
            "train loss:0.8309830440723478\n",
            "train loss:0.9345448994339511\n",
            "train loss:0.8090801912126289\n",
            "train loss:0.9445552110901629\n",
            "train loss:0.7621556798019834\n",
            "train loss:0.8518205503043674\n",
            "train loss:0.8734178060547727\n",
            "train loss:0.7838309403243773\n",
            "train loss:0.95882431877486\n",
            "train loss:0.877407498243898\n",
            "train loss:0.9058375286970606\n",
            "train loss:0.828124291779286\n",
            "train loss:0.7704268514774576\n",
            "train loss:0.9863430866775438\n",
            "train loss:0.74136887818524\n",
            "train loss:0.7807827221753757\n",
            "train loss:0.8566240794393866\n",
            "train loss:0.8339234664493733\n",
            "train loss:0.9097455517666165\n",
            "train loss:0.8570794352279614\n",
            "train loss:1.00284800178346\n",
            "train loss:0.8223073366718122\n",
            "train loss:0.9357974164926651\n",
            "train loss:0.9789099925816522\n",
            "train loss:0.9428281861543522\n",
            "train loss:0.7931302046092406\n",
            "train loss:1.036081036046131\n",
            "train loss:0.8977143090038389\n",
            "train loss:0.9884099005961672\n",
            "train loss:0.9572843634416298\n",
            "train loss:0.9833906018929316\n",
            "train loss:0.9716077608317378\n",
            "train loss:0.8634531750950495\n",
            "train loss:0.8313407824567198\n",
            "train loss:0.8185473156900208\n",
            "train loss:0.8085857352451096\n",
            "train loss:0.9588316578952022\n",
            "train loss:0.8129309637445911\n",
            "train loss:0.8645661326401004\n",
            "train loss:0.7880751111059641\n",
            "train loss:0.8435710478762914\n",
            "train loss:0.8071847852341475\n",
            "train loss:0.8257286836323253\n",
            "train loss:0.9305321082870864\n",
            "train loss:0.952244204345725\n",
            "train loss:0.8223512367983024\n",
            "train loss:0.7740503231572408\n",
            "train loss:0.6202753428076531\n",
            "train loss:0.9366468903270585\n",
            "train loss:0.8986016028935867\n",
            "train loss:0.9291346104769802\n",
            "train loss:0.7793770441071224\n",
            "train loss:0.9891645241325644\n",
            "train loss:0.8921865030124766\n",
            "train loss:0.8097751588641942\n",
            "train loss:0.9309335924090472\n",
            "train loss:0.9128955701850242\n",
            "train loss:0.9028950046529053\n",
            "train loss:0.9165145174172723\n",
            "train loss:0.8700641602757827\n",
            "train loss:0.7247704847349113\n",
            "train loss:0.9444805546271888\n",
            "train loss:0.7726472314113044\n",
            "train loss:0.939025163493725\n",
            "train loss:0.7528004611877184\n",
            "train loss:0.900877772892628\n",
            "train loss:0.8817288911854136\n",
            "train loss:1.051873849145578\n",
            "train loss:0.7740118386539724\n",
            "train loss:0.8891983557818005\n",
            "train loss:0.9173033964892925\n",
            "train loss:0.7412006539436138\n",
            "train loss:0.7186820253927089\n",
            "train loss:0.937813625716307\n",
            "train loss:0.9289062035252116\n",
            "train loss:0.7472305536067932\n",
            "train loss:0.7468455678081461\n",
            "train loss:0.8237190862445795\n",
            "train loss:0.8698762636139776\n",
            "train loss:0.8813635952337515\n",
            "train loss:0.7608086540983001\n",
            "train loss:0.8478308647897937\n",
            "train loss:0.8603795514976434\n",
            "train loss:0.9728656535233471\n",
            "train loss:0.8732170669440739\n",
            "train loss:0.8566543463498371\n",
            "train loss:0.8971655234320433\n",
            "train loss:0.7311833201393227\n",
            "train loss:1.0087365216656248\n",
            "train loss:0.7603696412942811\n",
            "train loss:0.8227692771857187\n",
            "train loss:0.955082323819956\n",
            "train loss:1.0253079266302982\n",
            "train loss:0.8220624227934519\n",
            "train loss:0.7245555753993724\n",
            "train loss:0.844179491916896\n",
            "train loss:0.9296513632814493\n",
            "train loss:1.1432023953377863\n",
            "train loss:0.8875112085923017\n",
            "train loss:0.8826394808560635\n",
            "train loss:0.9142884674417285\n",
            "train loss:0.8247354777294589\n",
            "train loss:1.0045380374088104\n",
            "train loss:0.7894324821642918\n",
            "train loss:0.857631187694016\n",
            "train loss:0.7778552564138304\n",
            "train loss:0.8498837770513186\n",
            "train loss:0.8459216645185661\n",
            "train loss:1.0168388492752367\n",
            "train loss:0.9242375853562411\n",
            "train loss:0.8086264341881767\n",
            "train loss:1.0010053132890824\n",
            "train loss:0.8465780280121677\n",
            "train loss:0.765375755591824\n",
            "train loss:0.8474506511438362\n",
            "train loss:0.9725840011139555\n",
            "train loss:1.0062176316208196\n",
            "train loss:0.9726667144118193\n",
            "train loss:0.7365192183995086\n",
            "train loss:0.8619204588731786\n",
            "train loss:0.8258239235385924\n",
            "train loss:0.9997303386175784\n",
            "train loss:0.8324735062625284\n",
            "train loss:0.8411041964918986\n",
            "train loss:0.914670229299839\n",
            "train loss:0.7286205812819722\n",
            "train loss:0.9424788021822076\n",
            "train loss:0.8414400863590916\n",
            "train loss:0.8590828857434315\n",
            "train loss:0.9803570245985611\n",
            "train loss:1.0464856409766738\n",
            "train loss:0.8651956366341296\n",
            "train loss:0.8601410782307441\n",
            "train loss:0.822551425300655\n",
            "train loss:0.7738625191938232\n",
            "train loss:0.8900837490352513\n",
            "train loss:0.8404975620122289\n",
            "train loss:0.8220461999396862\n",
            "train loss:1.0349756652450244\n",
            "train loss:0.7603244236028719\n",
            "train loss:0.8528710838579543\n",
            "train loss:0.7755225582966929\n",
            "train loss:0.821269953943012\n",
            "train loss:0.8715849239506788\n",
            "train loss:0.9120061114988277\n",
            "train loss:0.97256883290586\n",
            "train loss:0.8869080429256766\n",
            "train loss:0.9052321319579403\n",
            "train loss:1.0666199116903714\n",
            "train loss:0.7536877050903612\n",
            "train loss:0.7594801386761816\n",
            "train loss:1.0426571659115362\n",
            "train loss:1.0485457181290025\n",
            "train loss:0.8564882462735598\n",
            "train loss:0.9323388627683238\n",
            "train loss:0.6726111134468692\n",
            "train loss:0.9257108567862228\n",
            "train loss:0.9868047020899476\n",
            "train loss:0.6749106224476182\n",
            "train loss:0.8570787816629393\n",
            "train loss:0.8772365425528865\n",
            "train loss:0.741865665858558\n",
            "train loss:0.8869802430229167\n",
            "train loss:0.7613412595608731\n",
            "train loss:0.7586311977035425\n",
            "train loss:0.9588382267444789\n",
            "train loss:0.8227519904392185\n",
            "train loss:0.7913733719555377\n",
            "train loss:0.9112263161066042\n",
            "train loss:0.7299261679981544\n",
            "train loss:0.7681700838426612\n",
            "train loss:0.8487515646629039\n",
            "train loss:0.819216098158585\n",
            "train loss:0.8847920372325261\n",
            "train loss:0.7545168963171105\n",
            "train loss:0.8796087612734295\n",
            "train loss:0.6856296861978212\n",
            "train loss:0.8983713896771296\n",
            "train loss:0.7964001453439213\n",
            "train loss:0.8386448958167911\n",
            "train loss:0.7849899263396718\n",
            "train loss:0.9677311027468458\n",
            "=== epoch:15, train acc:0.997, test acc:0.993 ===\n",
            "train loss:0.9321341236894659\n",
            "train loss:0.9940097252713805\n",
            "train loss:0.9244315559568068\n",
            "train loss:0.9226507803434768\n",
            "train loss:0.9625215886239854\n",
            "train loss:0.9123690409751439\n",
            "train loss:1.0088903718940794\n",
            "train loss:0.8021046938552217\n",
            "train loss:0.8891882423364631\n",
            "train loss:0.860172753910767\n",
            "train loss:0.9880762792822988\n",
            "train loss:0.7253499776843113\n",
            "train loss:0.9561098144013325\n",
            "train loss:0.9113385423976145\n",
            "train loss:1.0065290526768327\n",
            "train loss:0.9642783870938888\n",
            "train loss:0.8739303904066223\n",
            "train loss:1.1317685982035914\n",
            "train loss:0.8922351443083244\n",
            "train loss:0.836059567911984\n",
            "train loss:0.576508896371808\n",
            "train loss:0.7472124827215402\n",
            "train loss:0.8755182268126471\n",
            "train loss:0.9168151310721475\n",
            "train loss:0.839085590822435\n",
            "train loss:0.7031062801138073\n",
            "train loss:0.8943506768749364\n",
            "train loss:0.841365000025016\n",
            "train loss:0.757377271933387\n",
            "train loss:0.8339406112992408\n",
            "train loss:0.8377044462444719\n",
            "train loss:0.8099105452228709\n",
            "train loss:0.7608625254727531\n",
            "train loss:0.9246568305563385\n",
            "train loss:0.8051974192761147\n",
            "train loss:0.9435836435818655\n",
            "train loss:0.7650769928144528\n",
            "train loss:1.0402996673401799\n",
            "train loss:0.8707104294106962\n",
            "train loss:0.7542189534585196\n",
            "train loss:0.7371451443482606\n",
            "train loss:1.0088525539338296\n",
            "train loss:0.9315720877248319\n",
            "train loss:0.7998936030934223\n",
            "train loss:0.9323673087361195\n",
            "train loss:0.846534880814509\n",
            "train loss:0.869096559113669\n",
            "train loss:0.8661148618455085\n",
            "train loss:0.9144154257156978\n",
            "train loss:0.9359487935130809\n",
            "train loss:0.8722258564871409\n",
            "train loss:0.927740248421207\n",
            "train loss:1.0104571261631863\n",
            "train loss:0.9942004664670938\n",
            "train loss:0.830361820468552\n",
            "train loss:0.8686956637324861\n",
            "train loss:0.9654245221787677\n",
            "train loss:1.0548456003475186\n",
            "train loss:0.8854730423351903\n",
            "train loss:0.8324416044989514\n",
            "train loss:0.7599281626697671\n",
            "train loss:0.8443737000259439\n",
            "train loss:0.817232782101589\n",
            "train loss:0.9224319315109394\n",
            "train loss:0.8879933552516809\n",
            "train loss:0.8112276402457433\n",
            "train loss:0.827191466552903\n",
            "train loss:0.9379332266511763\n",
            "train loss:0.934045006659294\n",
            "train loss:0.8130194703706088\n",
            "train loss:0.78269532500476\n",
            "train loss:0.9437019824786297\n",
            "train loss:0.9176378195759269\n",
            "train loss:0.8072762312413297\n",
            "train loss:0.7460162916977118\n",
            "train loss:0.8209943972141694\n",
            "train loss:0.8582656619391699\n",
            "train loss:1.0441172179051803\n",
            "train loss:0.6955923815800706\n",
            "train loss:0.770332429899785\n",
            "train loss:0.869005456318484\n",
            "train loss:0.9624184565174809\n",
            "train loss:0.7490780725978743\n",
            "train loss:0.8791838147032558\n",
            "train loss:0.8569942515415989\n",
            "train loss:0.8268829134081004\n",
            "train loss:0.7996774479612835\n",
            "train loss:1.0987588585919281\n",
            "train loss:0.8492600484417133\n",
            "train loss:0.8623389601021003\n",
            "train loss:0.7438660232598509\n",
            "train loss:0.8563156421647321\n",
            "train loss:0.8160602468450944\n",
            "train loss:0.9539698574274287\n",
            "train loss:0.8652329504444404\n",
            "train loss:0.7783027932782396\n",
            "train loss:0.8091679949878102\n",
            "train loss:0.811651409838853\n",
            "train loss:0.879629204250154\n",
            "train loss:0.948149318095041\n",
            "train loss:1.1088207548735591\n",
            "train loss:0.831795903947548\n",
            "train loss:0.7825693164697313\n",
            "train loss:0.9327271602456403\n",
            "train loss:0.83183038013254\n",
            "train loss:0.8295546445178802\n",
            "train loss:0.8504796121608813\n",
            "train loss:0.8302054488855418\n",
            "train loss:0.9516030416903353\n",
            "train loss:0.9194781920110681\n",
            "train loss:0.7935886809513057\n",
            "train loss:0.9041802658341814\n",
            "train loss:0.7352201192162033\n",
            "train loss:0.9635656287491708\n",
            "train loss:0.9431041117559964\n",
            "train loss:0.7926911798860147\n",
            "train loss:0.865512904436202\n",
            "train loss:0.8961106783870584\n",
            "train loss:0.8124583156523569\n",
            "train loss:0.8440858517283556\n",
            "train loss:0.9458078816133354\n",
            "train loss:0.7921391341217197\n",
            "train loss:0.841404138419687\n",
            "train loss:0.9388717162265466\n",
            "train loss:0.7329477488094303\n",
            "train loss:0.9510942038397024\n",
            "train loss:1.0470001269537246\n",
            "train loss:0.8501167868357239\n",
            "train loss:0.9056398522263431\n",
            "train loss:0.7649083550845844\n",
            "train loss:0.8498659961370889\n",
            "train loss:0.7229211020473789\n",
            "train loss:1.038115829465137\n",
            "train loss:0.7835126453571294\n",
            "train loss:0.7237587049583329\n",
            "train loss:0.8366302851034797\n",
            "train loss:0.961345264479134\n",
            "train loss:0.7155089403952357\n",
            "train loss:0.8498313558360141\n",
            "train loss:0.9962147907025671\n",
            "train loss:0.8424928168067922\n",
            "train loss:0.8161830761898281\n",
            "train loss:1.013655860601244\n",
            "train loss:0.8726523448682422\n",
            "train loss:0.8232236970709457\n",
            "train loss:0.886688693759285\n",
            "train loss:0.8236362182508584\n",
            "train loss:0.9155360034934167\n",
            "train loss:0.8868250747361466\n",
            "train loss:0.805120576523925\n",
            "train loss:0.7949006938892079\n",
            "train loss:0.9493962591089453\n",
            "train loss:0.8119013618461237\n",
            "train loss:0.9052229420412133\n",
            "train loss:0.6766987649943681\n",
            "train loss:0.9223875877144845\n",
            "train loss:1.0964534101534715\n",
            "train loss:0.9317972317181465\n",
            "train loss:0.8351054570863026\n",
            "train loss:0.6781813794603251\n",
            "train loss:0.9736279008745196\n",
            "train loss:0.892351810139705\n",
            "train loss:0.9302789489348495\n",
            "train loss:0.9113627994998956\n",
            "train loss:0.8120325743058696\n",
            "train loss:0.7872497862822295\n",
            "train loss:0.890391508327382\n",
            "train loss:0.7695776178481509\n",
            "train loss:0.922358303740149\n",
            "train loss:0.7983326743839819\n",
            "train loss:0.8340072231896967\n",
            "train loss:0.9933580636977415\n",
            "train loss:0.7281997791240458\n",
            "train loss:0.8762587826379133\n",
            "train loss:0.7324805307621056\n",
            "train loss:0.9319850613679406\n",
            "train loss:0.9187864054473721\n",
            "train loss:0.8310743145500943\n",
            "train loss:1.0139476136503232\n",
            "train loss:0.7519887660673713\n",
            "train loss:0.9449877488599121\n",
            "train loss:0.8918171085400824\n",
            "train loss:0.8739088515295218\n",
            "train loss:0.807145907548077\n",
            "train loss:0.9259250523154813\n",
            "train loss:1.037886386625723\n",
            "train loss:1.14790054279154\n",
            "train loss:0.9883974184399409\n",
            "train loss:0.9491557910798083\n",
            "train loss:0.8330626504746843\n",
            "train loss:0.7869441094061222\n",
            "train loss:0.8636853984961241\n",
            "train loss:0.7806846498075434\n",
            "train loss:0.751948496398803\n",
            "train loss:0.8764459939405973\n",
            "train loss:0.8754722105687185\n",
            "train loss:0.7746047983150511\n",
            "train loss:0.8629714576861758\n",
            "train loss:0.908843956359189\n",
            "train loss:0.882746735207202\n",
            "train loss:1.038243497611039\n",
            "train loss:0.9346830990638266\n",
            "train loss:0.9064531197911893\n",
            "train loss:1.0114768627441426\n",
            "train loss:1.024429844098342\n",
            "train loss:0.7147756714608581\n",
            "train loss:0.917877119016448\n",
            "train loss:0.7449199883831387\n",
            "train loss:0.9838436943659608\n",
            "train loss:0.9151361691819397\n",
            "train loss:0.8885563165933461\n",
            "train loss:1.011504649036665\n",
            "train loss:1.0403142038123094\n",
            "train loss:0.8864900138065547\n",
            "train loss:0.8308655444913482\n",
            "train loss:0.8802105535290597\n",
            "train loss:1.007781349246268\n",
            "train loss:0.8675990940963118\n",
            "train loss:0.9755094465516041\n",
            "train loss:0.7840332630876358\n",
            "train loss:0.870275160412438\n",
            "train loss:0.9467110061686295\n",
            "train loss:0.8769002658360922\n",
            "train loss:0.8224331956186925\n",
            "train loss:0.8572572870875257\n",
            "train loss:0.7790162436757504\n",
            "train loss:0.8939202701936704\n",
            "train loss:0.749455982315813\n",
            "train loss:0.6856155561288081\n",
            "train loss:0.9072459079748306\n",
            "train loss:0.8127919758219418\n",
            "train loss:0.8294102833823804\n",
            "train loss:0.8965660527983242\n",
            "train loss:0.8598772768468718\n",
            "train loss:0.7350585208088185\n",
            "train loss:0.7387050912687791\n",
            "train loss:1.0143068123920491\n",
            "train loss:0.9197533929380707\n",
            "train loss:0.9525836480733189\n",
            "train loss:0.9130119771160874\n",
            "train loss:0.9410568309839953\n",
            "train loss:0.9497143870006017\n",
            "train loss:0.870469655271721\n",
            "train loss:1.0249070793275346\n",
            "train loss:0.8430944695346497\n",
            "train loss:0.6925107617370597\n",
            "train loss:0.9065857205621666\n",
            "train loss:1.0442523994210682\n",
            "train loss:0.9259043173263446\n",
            "train loss:0.8449136587471743\n",
            "train loss:0.8307734294281667\n",
            "train loss:0.7465370624483256\n",
            "train loss:0.9613690806744998\n",
            "train loss:1.0127479763277447\n",
            "train loss:0.8729658571667559\n",
            "train loss:0.685144396842908\n",
            "train loss:0.8400029702247245\n",
            "train loss:0.860290738972559\n",
            "train loss:0.8415012744118373\n",
            "train loss:0.901492530077922\n",
            "train loss:0.726378173117604\n",
            "train loss:0.883513740555826\n",
            "train loss:0.9391518010406494\n",
            "train loss:1.1033768544076816\n",
            "train loss:0.8197580102041254\n",
            "train loss:0.7519368542098712\n",
            "train loss:0.8137617321698047\n",
            "train loss:0.8758099456136504\n",
            "train loss:0.8823374641362158\n",
            "train loss:1.0188649361787059\n",
            "train loss:0.8507289626429576\n",
            "train loss:0.9730746749715985\n",
            "train loss:0.8441610702173259\n",
            "train loss:1.0161935805310234\n",
            "train loss:0.7919395648576772\n",
            "train loss:0.7903643057675859\n",
            "train loss:0.6129112882894918\n",
            "train loss:0.8661166265759551\n",
            "train loss:0.9307039571041141\n",
            "train loss:0.8838372108673187\n",
            "train loss:0.9567359160935608\n",
            "train loss:0.753735069762001\n",
            "train loss:0.901860678901026\n",
            "train loss:0.7542281602653838\n",
            "train loss:0.7231376522336486\n",
            "train loss:0.7909737030868575\n",
            "train loss:0.7852058344021358\n",
            "train loss:0.793576277466282\n",
            "train loss:0.8353130279021168\n",
            "train loss:0.8761576797020041\n",
            "train loss:0.8876826566633461\n",
            "train loss:0.8150898854386054\n",
            "train loss:0.8783556715286314\n",
            "train loss:0.876283593087094\n",
            "train loss:0.795419365704637\n",
            "train loss:0.8002686424133592\n",
            "train loss:0.8871478694297334\n",
            "train loss:0.8504768888750482\n",
            "train loss:0.7496754486716668\n",
            "train loss:1.0059510578300332\n",
            "train loss:0.7860816627052871\n",
            "train loss:0.9789131567129332\n",
            "train loss:0.7688711336961171\n",
            "train loss:0.7920663553977997\n",
            "train loss:0.8113348117575617\n",
            "train loss:0.8521192726259621\n",
            "train loss:0.9902344947010303\n",
            "train loss:0.915078132333806\n",
            "train loss:0.8290004032382658\n",
            "train loss:0.8572473917530294\n",
            "train loss:0.6585850143888837\n",
            "train loss:0.7988508194569426\n",
            "train loss:0.9816695473606498\n",
            "train loss:0.8916413451730568\n",
            "train loss:0.8597665448127898\n",
            "train loss:0.7940934031378272\n",
            "train loss:1.0812927236680834\n",
            "train loss:0.9521790095212811\n",
            "train loss:0.8000516872641554\n",
            "train loss:0.9011611309143305\n",
            "train loss:0.8619533368039842\n",
            "train loss:0.9956393060333839\n",
            "train loss:0.8196551538110326\n",
            "train loss:1.0344269301786277\n",
            "train loss:0.9177669803365134\n",
            "train loss:0.9278113407998728\n",
            "train loss:0.8471709863826201\n",
            "train loss:0.7985089002475847\n",
            "train loss:0.8542661654021727\n",
            "train loss:0.9556660659050205\n",
            "train loss:0.9485917854424428\n",
            "train loss:0.8449703265890944\n",
            "train loss:0.7263533357766019\n",
            "train loss:0.8393470850257758\n",
            "train loss:0.7322705325466886\n",
            "train loss:0.9387002657951187\n",
            "train loss:0.8001361528837169\n",
            "train loss:0.8023643273104207\n",
            "train loss:0.8855245558039688\n",
            "train loss:0.8061178699011461\n",
            "train loss:0.7679250353070856\n",
            "train loss:0.8232243539743374\n",
            "train loss:0.7447655489520496\n",
            "train loss:0.8602121047275983\n",
            "train loss:0.7869995742211324\n",
            "train loss:0.9596786872503037\n",
            "train loss:0.848278263554336\n",
            "train loss:0.7632039530066562\n",
            "train loss:0.7761753765039907\n",
            "train loss:0.7905506407369587\n",
            "train loss:0.9393869482507051\n",
            "train loss:0.7366687878076918\n",
            "train loss:0.8737566548565414\n",
            "train loss:0.8791772470952551\n",
            "train loss:0.683108754862799\n",
            "train loss:0.9631670731886218\n",
            "train loss:0.7337412881754779\n",
            "train loss:0.7537531474168133\n",
            "train loss:0.804925271251546\n",
            "train loss:0.8029033131802905\n",
            "train loss:0.8621696814629733\n",
            "train loss:0.7427491108003117\n",
            "train loss:0.9089807905788567\n",
            "train loss:0.8688206449662482\n",
            "train loss:0.8150812108361296\n",
            "train loss:0.8480596261826884\n",
            "train loss:0.9212301380832638\n",
            "train loss:1.0272670263918275\n",
            "train loss:0.8670850864146786\n",
            "train loss:1.0638225027989798\n",
            "train loss:0.8875445739332208\n",
            "train loss:0.8287863062206401\n",
            "train loss:0.783522083633835\n",
            "train loss:0.8264285588885018\n",
            "train loss:0.6473883556972777\n",
            "train loss:0.8314485376729135\n",
            "train loss:0.8951025458942664\n",
            "train loss:0.7085049462831502\n",
            "train loss:0.8313956151896557\n",
            "train loss:0.9871149125020224\n",
            "train loss:0.8966484792197672\n",
            "train loss:0.8510550192829036\n",
            "train loss:0.7880971190961152\n",
            "train loss:0.8716986464583018\n",
            "train loss:0.8349979363551674\n",
            "train loss:0.8753720897026662\n",
            "train loss:0.9013155763776239\n",
            "train loss:0.8463065526785926\n",
            "train loss:0.8911277078031468\n",
            "train loss:0.8523228143308014\n",
            "train loss:0.8502093553341145\n",
            "train loss:0.8529168707678844\n",
            "train loss:0.9181178416494944\n",
            "train loss:0.9868277909319813\n",
            "train loss:0.7065192197601142\n",
            "train loss:0.9608585152283105\n",
            "train loss:0.8725604876519519\n",
            "train loss:0.7819162245851209\n",
            "train loss:0.7099969959898929\n",
            "train loss:0.9694759465586228\n",
            "train loss:0.8516443710633714\n",
            "train loss:0.9193917644293923\n",
            "train loss:0.8768678826705087\n",
            "train loss:0.8655972593845204\n",
            "train loss:0.7943574967246952\n",
            "train loss:0.848528675546162\n",
            "train loss:0.6912513967340383\n",
            "train loss:0.8310080956292559\n",
            "train loss:0.9294412451106008\n",
            "train loss:1.0147982211590543\n",
            "train loss:0.7886615713547174\n",
            "train loss:0.6725753547779799\n",
            "train loss:0.8063250471467985\n",
            "train loss:0.9148394174072749\n",
            "train loss:1.0359478900413834\n",
            "train loss:0.7151294999351395\n",
            "train loss:0.7940024368636542\n",
            "train loss:0.9093323515833197\n",
            "train loss:0.8588345475114117\n",
            "train loss:1.0384763059515272\n",
            "train loss:0.7895981022055188\n",
            "train loss:0.7294192110351871\n",
            "train loss:0.8710046705130554\n",
            "train loss:0.766397876000218\n",
            "train loss:0.9990797397043163\n",
            "train loss:0.8947574584363842\n",
            "train loss:0.8541656510439336\n",
            "train loss:0.8211534334879264\n",
            "train loss:0.8287928156746964\n",
            "train loss:0.9228499035620402\n",
            "train loss:0.8441134106267344\n",
            "train loss:0.9621031820317754\n",
            "train loss:0.9701553971915764\n",
            "train loss:0.7506845223848936\n",
            "train loss:0.8451052273035853\n",
            "train loss:0.9539489436188245\n",
            "train loss:0.6737439854658999\n",
            "train loss:0.9331651324169541\n",
            "train loss:0.8073364026634351\n",
            "train loss:0.9298555668881064\n",
            "train loss:1.049541023193744\n",
            "train loss:0.8198164345043503\n",
            "train loss:0.9141774551183808\n",
            "train loss:0.8837277193576943\n",
            "train loss:0.8196145653546267\n",
            "train loss:0.738571984786877\n",
            "train loss:0.8083653687291448\n",
            "train loss:0.913944675158407\n",
            "train loss:0.9325345823950366\n",
            "train loss:0.7397036958549288\n",
            "train loss:0.9506656866515438\n",
            "train loss:0.7818625158392746\n",
            "train loss:0.9832232966490565\n",
            "train loss:0.9260774881035403\n",
            "train loss:0.9482901872596946\n",
            "train loss:0.8960829284263825\n",
            "train loss:0.8044149050424758\n",
            "train loss:0.8400706656436984\n",
            "train loss:0.7826999551356175\n",
            "train loss:0.9655148712310947\n",
            "train loss:0.8499525914669048\n",
            "train loss:0.8655673251418178\n",
            "train loss:0.7523954128474648\n",
            "train loss:0.7724932798388031\n",
            "train loss:0.673689214633794\n",
            "train loss:0.9466492294758173\n",
            "train loss:1.032989512206747\n",
            "train loss:0.9473114321011762\n",
            "train loss:0.8314194870752314\n",
            "train loss:0.8874452600113747\n",
            "train loss:0.7928023621899327\n",
            "train loss:0.7604102420197429\n",
            "train loss:0.8775105845899012\n",
            "train loss:0.9187589977805505\n",
            "train loss:0.9097353159126337\n",
            "train loss:0.9243579283270984\n",
            "train loss:1.063937109163376\n",
            "train loss:1.0846879227984672\n",
            "train loss:0.8001459934801555\n",
            "train loss:0.936587015813928\n",
            "train loss:1.0933609379262363\n",
            "train loss:0.8049021505878758\n",
            "train loss:0.8829452648389418\n",
            "train loss:0.8287002907732822\n",
            "train loss:0.8776658106524028\n",
            "train loss:1.0209215981750488\n",
            "train loss:0.7929253901894051\n",
            "train loss:0.7099224236562951\n",
            "train loss:0.8197442697157115\n",
            "train loss:0.9326062935845498\n",
            "train loss:0.8874046152995778\n",
            "train loss:0.8355272433577176\n",
            "train loss:0.9642896856142039\n",
            "train loss:0.8530006595949418\n",
            "train loss:0.9788527324904069\n",
            "train loss:0.764525385252276\n",
            "train loss:0.7869057432339924\n",
            "train loss:1.0306209551292518\n",
            "train loss:0.8877708844092348\n",
            "train loss:0.827634036044844\n",
            "train loss:0.8515517390128465\n",
            "train loss:0.5698496885588158\n",
            "train loss:0.8533641194683184\n",
            "train loss:0.9556296997696653\n",
            "train loss:0.8173289702619273\n",
            "train loss:0.9795508876144793\n",
            "train loss:0.9416637491926536\n",
            "train loss:0.8608676773633037\n",
            "train loss:1.0005817923854528\n",
            "train loss:0.8206194255882465\n",
            "train loss:0.9318331243587127\n",
            "train loss:0.9365538898329455\n",
            "train loss:0.9070279856052899\n",
            "train loss:0.9086796213962266\n",
            "train loss:0.7633056907443787\n",
            "train loss:0.7555594197426114\n",
            "train loss:0.9216604422312313\n",
            "train loss:1.005699104989372\n",
            "train loss:0.7619474176173107\n",
            "train loss:0.739772777527822\n",
            "train loss:0.9521020078724572\n",
            "train loss:0.7526659076037872\n",
            "train loss:1.0207349182152412\n",
            "train loss:0.9972628402028237\n",
            "train loss:0.914332493872313\n",
            "train loss:0.7694745301490741\n",
            "train loss:0.8568505818090111\n",
            "train loss:0.7229402945373467\n",
            "train loss:0.7717985800182747\n",
            "train loss:0.7640828503421986\n",
            "train loss:0.8261332543190485\n",
            "train loss:1.0401783678035548\n",
            "train loss:0.8073199082373962\n",
            "train loss:0.9552803874852396\n",
            "train loss:0.8288810967292919\n",
            "train loss:0.66037345982757\n",
            "train loss:0.9377573369745204\n",
            "train loss:0.783582890658409\n",
            "train loss:0.7414191752957795\n",
            "train loss:0.916534715887879\n",
            "train loss:0.892825343374434\n",
            "train loss:1.0556156833224721\n",
            "train loss:0.833233680598426\n",
            "train loss:0.9423026675875334\n",
            "train loss:1.0375466073765194\n",
            "train loss:0.9349940285534754\n",
            "train loss:0.9913493018503863\n",
            "train loss:0.7800323143603294\n",
            "train loss:1.0177764032693402\n",
            "train loss:0.9159133800712647\n",
            "train loss:0.7841108268233964\n",
            "train loss:0.8006023318998247\n",
            "train loss:0.8706502878719212\n",
            "train loss:0.8204455063446748\n",
            "train loss:0.8452582357539177\n",
            "train loss:0.7797399821104374\n",
            "train loss:0.9292249901770206\n",
            "train loss:0.8549793164576083\n",
            "train loss:1.021503261842755\n",
            "train loss:0.8652757809873366\n",
            "train loss:0.7902357804991592\n",
            "train loss:0.7843551481156628\n",
            "train loss:0.9292719141125348\n",
            "train loss:0.9012887706255822\n",
            "train loss:0.9855196781432238\n",
            "train loss:0.8346570381669883\n",
            "train loss:1.0222387794904746\n",
            "train loss:0.7853396624516216\n",
            "train loss:0.9006849423647231\n",
            "train loss:0.9161188652997627\n",
            "train loss:0.7944910475251251\n",
            "train loss:0.950584610553402\n",
            "train loss:0.8073494273149688\n",
            "train loss:0.889619828221424\n",
            "train loss:0.9704425888689056\n",
            "train loss:0.8359793645036975\n",
            "train loss:0.8990221163301831\n",
            "train loss:0.8442802453719243\n",
            "train loss:1.0569212860875314\n",
            "train loss:0.9623850578668551\n",
            "train loss:0.9440962062480655\n",
            "train loss:0.9603363641239301\n",
            "train loss:1.0101978041975537\n",
            "train loss:0.783272772753611\n",
            "train loss:0.7888952447501414\n",
            "train loss:0.9530650949241852\n",
            "train loss:0.8734370503987624\n",
            "train loss:0.8699335272977323\n",
            "train loss:0.8345300352998476\n",
            "train loss:0.7796961805538056\n",
            "train loss:0.9577179395935923\n",
            "train loss:0.8772373926066324\n",
            "train loss:0.745376375774401\n",
            "train loss:0.9309391056909881\n",
            "train loss:0.8352832871751141\n",
            "train loss:0.9500930635147188\n",
            "train loss:1.0012760152128772\n",
            "train loss:0.887329636240347\n",
            "train loss:0.9867976650698167\n",
            "train loss:0.973200402729152\n",
            "=== epoch:16, train acc:0.997, test acc:0.993 ===\n",
            "train loss:0.9133972465727374\n",
            "train loss:0.8407800716017042\n",
            "train loss:0.8659238200031304\n",
            "train loss:1.0682500827514445\n",
            "train loss:0.7722716707003398\n",
            "train loss:0.664936159090119\n",
            "train loss:0.991359986469572\n",
            "train loss:0.8874389825483527\n",
            "train loss:0.8111915843381516\n",
            "train loss:0.8125994472336815\n",
            "train loss:0.9712280953590148\n",
            "train loss:1.0009951882862764\n",
            "train loss:0.780080317574577\n",
            "train loss:0.7538770135530878\n",
            "train loss:0.8411727826495654\n",
            "train loss:0.6847719320143943\n",
            "train loss:0.8596515920390331\n",
            "train loss:0.9199365775613433\n",
            "train loss:0.780632405570429\n",
            "train loss:1.0162724711876918\n",
            "train loss:0.978056841161736\n",
            "train loss:0.8511704387293314\n",
            "train loss:0.7312465271390947\n",
            "train loss:0.8330663817390253\n",
            "train loss:0.926255731240125\n",
            "train loss:0.8568528173690924\n",
            "train loss:0.813196622482578\n",
            "train loss:1.0374560685041814\n",
            "train loss:0.8139465894486159\n",
            "train loss:0.7855033132399143\n",
            "train loss:0.7745152405539457\n",
            "train loss:0.9537723536101171\n",
            "train loss:0.8230002656011547\n",
            "train loss:1.0152121581688125\n",
            "train loss:0.6361638205506974\n",
            "train loss:1.0846686376768655\n",
            "train loss:0.8529491063280662\n",
            "train loss:0.8744081654766667\n",
            "train loss:0.8496433136142356\n",
            "train loss:0.8934930042008137\n",
            "train loss:0.8719435346211841\n",
            "train loss:0.898489029382995\n",
            "train loss:0.8408430984090284\n",
            "train loss:0.7995877668197501\n",
            "train loss:0.8924180043838007\n",
            "train loss:0.7590769857444926\n",
            "train loss:0.8955060664425443\n",
            "train loss:0.6904226487085225\n",
            "train loss:0.8555377988960864\n",
            "train loss:0.9166187570455689\n",
            "train loss:0.799149773853794\n",
            "train loss:0.9105901094977565\n",
            "train loss:0.7953672749753218\n",
            "train loss:0.8850533798620976\n",
            "train loss:0.8038615005904322\n",
            "train loss:0.9297828430735513\n",
            "train loss:0.8027567851814809\n",
            "train loss:0.808560168734653\n",
            "train loss:0.9687082072301483\n",
            "train loss:0.9494057595268665\n",
            "train loss:0.8822097098607189\n",
            "train loss:0.9677641684821815\n",
            "train loss:1.0377181782410974\n",
            "train loss:0.9060717235399917\n",
            "train loss:0.720400931891731\n",
            "train loss:0.8203406092032237\n",
            "train loss:0.7817865200802625\n",
            "train loss:0.8119047112979745\n",
            "train loss:0.9403145057876844\n",
            "train loss:0.7852116968562248\n",
            "train loss:0.794548675755924\n",
            "train loss:0.9698813129687006\n",
            "train loss:0.9900946214133337\n",
            "train loss:1.0266064224134235\n",
            "train loss:0.8515900748169416\n",
            "train loss:1.0716379593035983\n",
            "train loss:0.8172743423446818\n",
            "train loss:0.8377457955453507\n",
            "train loss:0.8314778186004536\n",
            "train loss:0.7907273738101139\n",
            "train loss:0.8153644709043332\n",
            "train loss:0.8625300659560969\n",
            "train loss:0.7768353224703958\n",
            "train loss:0.7858825627693529\n",
            "train loss:0.9861452811733962\n",
            "train loss:0.6864378698577893\n",
            "train loss:0.7844803872746757\n",
            "train loss:0.9253564242240416\n",
            "train loss:0.8751716684400573\n",
            "train loss:0.7828329372525533\n",
            "train loss:0.7783975135084177\n",
            "train loss:0.9662840908541098\n",
            "train loss:0.8479553666686671\n",
            "train loss:0.8026937539662572\n",
            "train loss:0.8180735539998502\n",
            "train loss:0.8218755130172317\n",
            "train loss:0.8291188010109363\n",
            "train loss:0.9107501138095836\n",
            "train loss:0.9231841541849893\n",
            "train loss:0.885589351626363\n",
            "train loss:0.8584036342422174\n",
            "train loss:1.0205103624015077\n",
            "train loss:0.7774134510518794\n",
            "train loss:0.7029846949334226\n",
            "train loss:0.9787499705769394\n",
            "train loss:0.8169617468812669\n",
            "train loss:0.7807773172591754\n",
            "train loss:0.8811024328280103\n",
            "train loss:0.5891680121228053\n",
            "train loss:0.9893096795122307\n",
            "train loss:0.8932193326726673\n",
            "train loss:0.9944809687032677\n",
            "train loss:1.0474928979680571\n",
            "train loss:0.8871890375098735\n",
            "train loss:0.7915782871302528\n",
            "train loss:0.9227633828249892\n",
            "train loss:0.9106965441725245\n",
            "train loss:0.9040649823205585\n",
            "train loss:0.7883834558982237\n",
            "train loss:0.6615531094510319\n",
            "train loss:0.8055834929991746\n",
            "train loss:0.9672209113034956\n",
            "train loss:0.8962660469666404\n",
            "train loss:0.9969821068937594\n",
            "train loss:1.083597416629536\n",
            "train loss:0.9468998212440525\n",
            "train loss:0.9726198570521918\n",
            "train loss:0.9500386179265314\n",
            "train loss:0.868205239409005\n",
            "train loss:1.0303077351560797\n",
            "train loss:0.8986483386724935\n",
            "train loss:0.8577652360385188\n",
            "train loss:0.8802059081731596\n",
            "train loss:0.9117600766000222\n",
            "train loss:0.9797922936218096\n",
            "train loss:0.80320855240739\n",
            "train loss:0.7089327811470818\n",
            "train loss:0.8460755145559183\n",
            "train loss:0.8877806690751803\n",
            "train loss:0.9185478517298437\n",
            "train loss:0.879786217389687\n",
            "train loss:0.8947867297091259\n",
            "train loss:0.8757190100881769\n",
            "train loss:0.7837945985315171\n",
            "train loss:1.0498721934604949\n",
            "train loss:0.867817563715135\n",
            "train loss:0.7776100535445352\n",
            "train loss:0.8682905683222983\n",
            "train loss:0.8535356937318948\n",
            "train loss:0.8090376701578792\n",
            "train loss:0.681502144044811\n",
            "train loss:0.8289001749381919\n",
            "train loss:0.9858467335487587\n",
            "train loss:0.9062404926161204\n",
            "train loss:1.0529035895970797\n",
            "train loss:0.8699694110759755\n",
            "train loss:0.9268298872604019\n",
            "train loss:0.8032239184133065\n",
            "train loss:0.9449047265094316\n",
            "train loss:0.9135864491231063\n",
            "train loss:0.7867938473424765\n",
            "train loss:0.8936618874555573\n",
            "train loss:0.8855339706545848\n",
            "train loss:0.945772585403781\n",
            "train loss:0.779327131077998\n",
            "train loss:0.7343165000287324\n",
            "train loss:0.8811724663303943\n",
            "train loss:0.8036961771606406\n",
            "train loss:0.8996275578435862\n",
            "train loss:0.7607155799860391\n",
            "train loss:0.9501515050969566\n",
            "train loss:0.9406438569989304\n",
            "train loss:0.8172605649248709\n",
            "train loss:0.9787070301077014\n",
            "train loss:0.9991072739152541\n",
            "train loss:0.9247335540927972\n",
            "train loss:0.7461786504222456\n",
            "train loss:0.9114336560672056\n",
            "train loss:0.9043811089681714\n",
            "train loss:0.9131004721434093\n",
            "train loss:0.9121625900042423\n",
            "train loss:0.9942668232392815\n",
            "train loss:0.8734864425756647\n",
            "train loss:0.8158378675848161\n",
            "train loss:0.8932251135139108\n",
            "train loss:0.6684324314498251\n",
            "train loss:0.7868967392534355\n",
            "train loss:1.0197208291239375\n",
            "train loss:0.8981186729747083\n",
            "train loss:0.9425070374748222\n",
            "train loss:0.8367847545652015\n",
            "train loss:0.8461299325393017\n",
            "train loss:0.7845505437978453\n",
            "train loss:0.8454774793675949\n",
            "train loss:0.824923972388476\n",
            "train loss:0.9840810799868257\n",
            "train loss:0.7375822375690029\n",
            "train loss:0.6108385456956199\n",
            "train loss:0.7414351916237742\n",
            "train loss:0.8165058888767888\n",
            "train loss:0.9071811515751826\n",
            "train loss:1.0175229182551093\n",
            "train loss:0.6609144770566728\n",
            "train loss:0.9412054094935884\n",
            "train loss:1.0051932273195845\n",
            "train loss:0.9476621283156568\n",
            "train loss:0.7546220928493905\n",
            "train loss:0.9326699905118865\n",
            "train loss:0.807514120925816\n",
            "train loss:0.9396948795787781\n",
            "train loss:0.9042042348467731\n",
            "train loss:0.8174963139626134\n",
            "train loss:0.8937668623658519\n",
            "train loss:0.6658951502993763\n",
            "train loss:0.8039293591614154\n",
            "train loss:0.8717260985997559\n",
            "train loss:0.7453126932358122\n",
            "train loss:0.8041069612315604\n",
            "train loss:0.8280009602214643\n",
            "train loss:0.7925045854363907\n",
            "train loss:0.8197670312820202\n",
            "train loss:0.9084865719745533\n",
            "train loss:0.7362578949578159\n",
            "train loss:0.7723233380025631\n",
            "train loss:0.7575388179413377\n",
            "train loss:1.0723814757433698\n",
            "train loss:0.7995537432569687\n",
            "train loss:0.9353074291645055\n",
            "train loss:0.8805644802681775\n",
            "train loss:0.7799304137495829\n",
            "train loss:0.8674686892347766\n",
            "train loss:0.8682192601179319\n",
            "train loss:0.8789562869962191\n",
            "train loss:0.9358360989788409\n",
            "train loss:0.8184034066973933\n",
            "train loss:0.7640161630643734\n",
            "train loss:0.9354860285554788\n",
            "train loss:0.8725176884207151\n",
            "train loss:0.8339725004565615\n",
            "train loss:0.9102156484850034\n",
            "train loss:0.8359136096794528\n",
            "train loss:0.8764990344328393\n",
            "train loss:0.7596118502670143\n",
            "train loss:0.6753050022228877\n",
            "train loss:0.9479449618096525\n",
            "train loss:0.9029277262692442\n",
            "train loss:0.8567683956946869\n",
            "train loss:0.8765799147820077\n",
            "train loss:0.8166844809203502\n",
            "train loss:0.8522478816422975\n",
            "train loss:0.7465645533319974\n",
            "train loss:0.9184568828711595\n",
            "train loss:0.8866882542704208\n",
            "train loss:0.9408783310573549\n",
            "train loss:0.8241537412871814\n",
            "train loss:0.7898635036322227\n",
            "train loss:0.7614921917104656\n",
            "train loss:0.8386779076155472\n",
            "train loss:0.7289452672045493\n",
            "train loss:0.8509145647573927\n",
            "train loss:0.9216167003791178\n",
            "train loss:1.0144924071298898\n",
            "train loss:0.7719982366880593\n",
            "train loss:0.8150764851833501\n",
            "train loss:0.793206553447851\n",
            "train loss:0.9053576928368409\n",
            "train loss:0.7285809840215878\n",
            "train loss:1.1131625594785388\n",
            "train loss:0.6288635538125306\n",
            "train loss:0.9629956654971781\n",
            "train loss:0.9363866791353302\n",
            "train loss:0.6865153965096202\n",
            "train loss:0.9830352178261207\n",
            "train loss:1.034517999673424\n",
            "train loss:1.018076128242506\n",
            "train loss:0.9389809415400455\n",
            "train loss:0.7676326225416682\n",
            "train loss:1.0272375083416294\n",
            "train loss:0.8214049088684621\n",
            "train loss:0.7384306700250185\n",
            "train loss:0.9657696199392386\n",
            "train loss:0.7979319610389016\n",
            "train loss:0.7530685307940738\n",
            "train loss:0.7534741831450948\n",
            "train loss:0.8489511870694005\n",
            "train loss:0.8488884536736863\n",
            "train loss:0.8949783127981975\n",
            "train loss:0.7846113344002691\n",
            "train loss:0.8319903466712755\n",
            "train loss:1.0360384298355982\n",
            "train loss:0.8839256053984589\n",
            "train loss:0.8605516887058456\n",
            "train loss:0.7934333615764305\n",
            "train loss:0.8128920515872379\n",
            "train loss:0.8493256466180047\n",
            "train loss:1.0027759135033714\n",
            "train loss:0.8509443769302014\n",
            "train loss:0.9292406998682108\n",
            "train loss:0.8934391282588217\n",
            "train loss:0.8182145738092721\n",
            "train loss:0.8966874864211204\n",
            "train loss:0.7894620456312816\n",
            "train loss:0.9437839588848408\n",
            "train loss:0.8649171272329593\n",
            "train loss:0.8063444678435916\n",
            "train loss:0.924223653866199\n",
            "train loss:0.9433388956838153\n",
            "train loss:0.7679185623130788\n",
            "train loss:0.8233853492445347\n",
            "train loss:0.9123071249767329\n",
            "train loss:0.9157531195274479\n",
            "train loss:0.7046608015479121\n",
            "train loss:0.8825783100032715\n",
            "train loss:0.7615841563650199\n",
            "train loss:0.8261771354360206\n",
            "train loss:0.7764222980954316\n",
            "train loss:0.8401821171178103\n",
            "train loss:0.8526059568749704\n",
            "train loss:0.728740111838206\n",
            "train loss:0.847633652845547\n",
            "train loss:0.8484766672693184\n",
            "train loss:0.7981455986735391\n",
            "train loss:0.9425947920168873\n",
            "train loss:0.8994780111998187\n",
            "train loss:0.8446388543204144\n",
            "train loss:0.717990028080128\n",
            "train loss:0.7160893736790673\n",
            "train loss:0.8332109487752062\n",
            "train loss:0.8274820635684255\n",
            "train loss:0.8203962649661447\n",
            "train loss:0.7799689242116697\n",
            "train loss:0.9159759581861142\n",
            "train loss:0.8140577060058513\n",
            "train loss:0.9362083529245411\n",
            "train loss:0.781973962753751\n",
            "train loss:0.8648559468201398\n",
            "train loss:0.8779675048607097\n",
            "train loss:0.7627060239549163\n",
            "train loss:0.9573576079692562\n",
            "train loss:0.8881660044941475\n",
            "train loss:0.8478516553805403\n",
            "train loss:0.9486026057215162\n",
            "train loss:0.9946526982106719\n",
            "train loss:0.8466231142418046\n",
            "train loss:0.7619428542274211\n",
            "train loss:0.8248443737621338\n",
            "train loss:0.8111451183242337\n",
            "train loss:0.9142299970620062\n",
            "train loss:0.847366296052431\n",
            "train loss:0.9972318968257459\n",
            "train loss:0.8736403418024378\n",
            "train loss:0.831425859933517\n",
            "train loss:0.8942086924333614\n",
            "train loss:0.9259320719396099\n",
            "train loss:0.8888925849386815\n",
            "train loss:1.0242628398521767\n",
            "train loss:0.9104605289306599\n",
            "train loss:0.8147670327829598\n",
            "train loss:0.8481015014813471\n",
            "train loss:0.6790276220478022\n",
            "train loss:0.7929183282927553\n",
            "train loss:0.7186142972700043\n",
            "train loss:0.9130114819918214\n",
            "train loss:0.9791190161387733\n",
            "train loss:0.8642851568229356\n",
            "train loss:0.7491951599574689\n",
            "train loss:0.9717922158890784\n",
            "train loss:1.0744425018144543\n",
            "train loss:0.8456822224500629\n",
            "train loss:0.7727165775786261\n",
            "train loss:0.8271639957764255\n",
            "train loss:0.7881799456373872\n",
            "train loss:1.0045493879123348\n",
            "train loss:0.8425914422429973\n",
            "train loss:0.9256242035563919\n",
            "train loss:0.9125449895360359\n",
            "train loss:0.857770991724819\n",
            "train loss:0.8930643560993926\n",
            "train loss:0.8154339862767962\n",
            "train loss:0.8283753907330086\n",
            "train loss:0.8894424440366506\n",
            "train loss:0.8622314722850473\n",
            "train loss:0.7827193876946241\n",
            "train loss:0.930278919203863\n",
            "train loss:0.8331479315883921\n",
            "train loss:0.9115637450648626\n",
            "train loss:0.8189537011180827\n",
            "train loss:0.7778286899319071\n",
            "train loss:0.7546154574876646\n",
            "train loss:0.8683848905671953\n",
            "train loss:0.9410300106538958\n",
            "train loss:0.980472268450976\n",
            "train loss:1.0198446174466458\n",
            "train loss:0.8036738510576984\n",
            "train loss:0.796435342191667\n",
            "train loss:0.9135730230647624\n",
            "train loss:0.8258114208300877\n",
            "train loss:0.9744946715636499\n",
            "train loss:0.8764257470103516\n",
            "train loss:0.9969628985174122\n",
            "train loss:0.7647241221852954\n",
            "train loss:0.7838457928230155\n",
            "train loss:0.686323707005509\n",
            "train loss:0.9364797288003958\n",
            "train loss:1.0474723352635638\n",
            "train loss:1.0151081320507132\n",
            "train loss:0.9205620624844448\n",
            "train loss:0.947960349081189\n",
            "train loss:0.9669827477580304\n",
            "train loss:0.9692369455618831\n",
            "train loss:0.8781122072771886\n",
            "train loss:0.8981159044027188\n",
            "train loss:0.8186249377609471\n",
            "train loss:0.8549834804718495\n",
            "train loss:0.9154009747999241\n",
            "train loss:0.982715334941255\n",
            "train loss:0.8650812966478314\n",
            "train loss:1.068160698567162\n",
            "train loss:0.8040182456325149\n",
            "train loss:0.999043793407369\n",
            "train loss:0.8715312920850038\n",
            "train loss:0.9417482561443515\n",
            "train loss:0.8564447655044255\n",
            "train loss:0.706944389733493\n",
            "train loss:0.7852015362368018\n",
            "train loss:0.8053258594352793\n",
            "train loss:0.8613515914487906\n",
            "train loss:0.897755788267099\n",
            "train loss:0.8738756735895308\n",
            "train loss:0.8263565200774238\n",
            "train loss:0.8925650977623585\n",
            "train loss:0.7830590894411317\n",
            "train loss:0.9578209393239958\n",
            "train loss:0.9226551607401708\n",
            "train loss:0.884391684268509\n",
            "train loss:1.0653557117414818\n",
            "train loss:0.9985337703131206\n",
            "train loss:0.8546550802518433\n",
            "train loss:0.7200721035036375\n",
            "train loss:0.9339709076991266\n",
            "train loss:0.7381153225659743\n",
            "train loss:0.8431225509802927\n",
            "train loss:0.9476854639237909\n",
            "train loss:0.9558764069009414\n",
            "train loss:0.9780741346089235\n",
            "train loss:0.723077481645368\n",
            "train loss:0.8653867100310143\n",
            "train loss:0.9913702242369804\n",
            "train loss:0.813003755181184\n",
            "train loss:0.8101387007177298\n",
            "train loss:0.83458370825778\n",
            "train loss:0.8067156240021096\n",
            "train loss:0.7531026679507299\n",
            "train loss:0.9847374395542421\n",
            "train loss:1.0249393076769175\n",
            "train loss:0.9024392324407335\n",
            "train loss:0.808347344499596\n",
            "train loss:0.7816205197795243\n",
            "train loss:0.8334863681444418\n",
            "train loss:1.0709957808597281\n",
            "train loss:0.8719751650955159\n",
            "train loss:1.0641359561141424\n",
            "train loss:0.6987800541966895\n",
            "train loss:0.7989543381637917\n",
            "train loss:0.9958634735726339\n",
            "train loss:0.8348273362509631\n",
            "train loss:0.8837016198790804\n",
            "train loss:0.7680106154668684\n",
            "train loss:0.7705953909054645\n",
            "train loss:0.8326918970677913\n",
            "train loss:0.7819796386357922\n",
            "train loss:0.9710566289308009\n",
            "train loss:0.8764607864132578\n",
            "train loss:0.8004858112599236\n",
            "train loss:0.8692710160611786\n",
            "train loss:1.0102174279892548\n",
            "train loss:0.8816882602668988\n",
            "train loss:1.045419530609064\n",
            "train loss:0.9345141142516963\n",
            "train loss:0.8441043415412228\n",
            "train loss:0.821196685666674\n",
            "train loss:0.7688277649118079\n",
            "train loss:0.7481689059382477\n",
            "train loss:0.9284597045670787\n",
            "train loss:0.8240953966541538\n",
            "train loss:0.9025649508817761\n",
            "train loss:0.854178065418974\n",
            "train loss:0.7001931277982972\n",
            "train loss:0.9153273152127632\n",
            "train loss:1.006012911844513\n",
            "train loss:0.9356063479518142\n",
            "train loss:0.6235561436442602\n",
            "train loss:0.7956790833224286\n",
            "train loss:0.9649099696324818\n",
            "train loss:0.9457066204235679\n",
            "train loss:0.8848325795133676\n",
            "train loss:0.8207640496254375\n",
            "train loss:0.7843388321140671\n",
            "train loss:0.8866227080620316\n",
            "train loss:0.910767559971361\n",
            "train loss:1.0273911452383118\n",
            "train loss:0.918623157187239\n",
            "train loss:0.8356273499828523\n",
            "train loss:0.9154676521909437\n",
            "train loss:0.8540904393520397\n",
            "train loss:0.7701916680858587\n",
            "train loss:0.9719720295770152\n",
            "train loss:0.8259591290313059\n",
            "train loss:0.9111403171535128\n",
            "train loss:0.8462337860344319\n",
            "train loss:0.9763673054005677\n",
            "train loss:0.951121361444898\n",
            "train loss:0.973685057909898\n",
            "train loss:0.827577821637881\n",
            "train loss:0.9088360417550833\n",
            "train loss:0.8681912224376401\n",
            "train loss:0.9412084057368638\n",
            "train loss:0.9606949423485941\n",
            "train loss:0.8846900200281931\n",
            "train loss:0.9137274432681842\n",
            "train loss:0.9179842648449521\n",
            "train loss:0.8845342397902042\n",
            "train loss:0.867619749573416\n",
            "train loss:0.8519728665387776\n",
            "train loss:0.8319316254097094\n",
            "train loss:0.8234754851497488\n",
            "train loss:0.6927630847134776\n",
            "train loss:0.8241320081181596\n",
            "train loss:0.8297026061375441\n",
            "train loss:1.0449959195933523\n",
            "train loss:0.9355741219341727\n",
            "train loss:0.9354430399691944\n",
            "train loss:0.816582389459153\n",
            "train loss:0.7514907390061176\n",
            "train loss:1.0618344268025222\n",
            "train loss:0.8425534792584984\n",
            "train loss:0.9298523840838334\n",
            "train loss:0.8190644567476432\n",
            "train loss:0.7567807978898198\n",
            "train loss:0.6696450060243335\n",
            "train loss:0.8383576900428714\n",
            "train loss:0.8314758009923985\n",
            "train loss:0.8921128296724752\n",
            "train loss:0.8347146866625775\n",
            "train loss:0.8932081993108012\n",
            "train loss:0.7946945861929542\n",
            "train loss:0.8170099316008377\n",
            "train loss:0.7117723341871741\n",
            "train loss:0.9169810387278278\n",
            "train loss:0.7510796102945618\n",
            "train loss:0.8411383961011141\n",
            "train loss:0.8659598330318014\n",
            "train loss:0.9960174717279512\n",
            "train loss:0.9535599338008716\n",
            "train loss:0.7833595499063832\n",
            "train loss:0.8378428796278259\n",
            "train loss:0.9527430884503251\n",
            "train loss:0.7623083728683394\n",
            "train loss:0.8807368291651493\n",
            "train loss:0.899201596376257\n",
            "train loss:0.9636129056304948\n",
            "train loss:0.9087615023575566\n",
            "train loss:0.8031215043477569\n",
            "train loss:0.7316923752568054\n",
            "train loss:0.7057258093514139\n",
            "train loss:1.0134754185643298\n",
            "train loss:0.7225028357461322\n",
            "train loss:0.7477365738884972\n",
            "train loss:0.8503168940705099\n",
            "train loss:0.8763602791449538\n",
            "train loss:0.8824148189316924\n",
            "train loss:0.9247352892480681\n",
            "train loss:0.8453721159055695\n",
            "train loss:0.9904454917513402\n",
            "train loss:0.8312215133779582\n",
            "train loss:0.850094552783443\n",
            "train loss:0.9064735884249278\n",
            "train loss:0.8275770758360281\n",
            "train loss:0.9681701553450192\n",
            "train loss:0.9377137929529789\n",
            "train loss:0.8615306321994644\n",
            "train loss:0.8310777243377045\n",
            "train loss:0.9515670409973274\n",
            "train loss:1.031394289349382\n",
            "train loss:0.8332653696954152\n",
            "train loss:0.8792398722560162\n",
            "train loss:0.8485368792182082\n",
            "train loss:0.9226797846972032\n",
            "train loss:0.8771258091418193\n",
            "train loss:0.8299262725539893\n",
            "train loss:0.9183679477837511\n",
            "train loss:0.7746878930754438\n",
            "train loss:0.7940638832437372\n",
            "train loss:0.9632625443375066\n",
            "train loss:0.7286422826394187\n",
            "train loss:1.0214708201535232\n",
            "train loss:1.0265138896637132\n",
            "train loss:0.8634139522194217\n",
            "train loss:0.9446262161794315\n",
            "train loss:0.7386042546794781\n",
            "=== epoch:17, train acc:0.998, test acc:0.991 ===\n",
            "train loss:0.9772890036867581\n",
            "train loss:0.8581461920939277\n",
            "train loss:0.9346291199044471\n",
            "train loss:0.9257607962963202\n",
            "train loss:0.6691820892161043\n",
            "train loss:1.0103660755763044\n",
            "train loss:1.0645308436104508\n",
            "train loss:0.8217988527589063\n",
            "train loss:0.9107040440686939\n",
            "train loss:0.9567224792965514\n",
            "train loss:1.0104994756267534\n",
            "train loss:0.7708084069199707\n",
            "train loss:1.0093631148343727\n",
            "train loss:0.8793488339368803\n",
            "train loss:0.876145717551172\n",
            "train loss:0.8668230755690906\n",
            "train loss:0.7983036185977366\n",
            "train loss:0.9243212261464138\n",
            "train loss:0.8862211997624805\n",
            "train loss:0.8403794871888846\n",
            "train loss:0.9692851512716585\n",
            "train loss:0.9076038162360577\n",
            "train loss:0.8358843193967651\n",
            "train loss:1.002728493624576\n",
            "train loss:0.8657816146980991\n",
            "train loss:0.7978660852675487\n",
            "train loss:0.8205999382383617\n",
            "train loss:0.9564840598736528\n",
            "train loss:0.854691143579533\n",
            "train loss:0.7835358288655258\n",
            "train loss:1.0540000002273067\n",
            "train loss:0.865859349221532\n",
            "train loss:0.8871617179904316\n",
            "train loss:0.8501606940807208\n",
            "train loss:0.7659911613408259\n",
            "train loss:0.9494474991869573\n",
            "train loss:0.8850465996773923\n",
            "train loss:0.8538815463709443\n",
            "train loss:0.7762900914464522\n",
            "train loss:0.8448022442352564\n",
            "train loss:0.850996656286748\n",
            "train loss:0.7975853447201056\n",
            "train loss:0.9347177145846876\n",
            "train loss:0.9685522238896315\n",
            "train loss:0.7268724410369117\n",
            "train loss:0.8816305335844952\n",
            "train loss:1.039053991909732\n",
            "train loss:0.8033223794687939\n",
            "train loss:0.9251736366868021\n",
            "train loss:0.8215171787954979\n",
            "train loss:0.8578211465648572\n",
            "train loss:0.9627408199512189\n",
            "train loss:0.7934483041182975\n",
            "train loss:0.9553124826096622\n",
            "train loss:0.9615557473560985\n",
            "train loss:0.854395823209806\n",
            "train loss:0.83042756748581\n",
            "train loss:1.0094245149673566\n",
            "train loss:0.818899283262796\n",
            "train loss:1.0443629921704725\n",
            "train loss:0.748982472626695\n",
            "train loss:0.79839675487857\n",
            "train loss:0.7873221265957803\n",
            "train loss:0.7600239212179689\n",
            "train loss:0.7692553646603393\n",
            "train loss:0.8325373082160458\n",
            "train loss:0.8489367265017574\n",
            "train loss:0.7341031858643631\n",
            "train loss:0.8014184322206372\n",
            "train loss:0.8317132964029068\n",
            "train loss:1.1408919843674206\n",
            "train loss:0.9528529323781624\n",
            "train loss:0.9111918640964076\n",
            "train loss:0.9105473004225833\n",
            "train loss:0.7334450520074098\n",
            "train loss:0.7700937091688225\n",
            "train loss:0.9123184460326182\n",
            "train loss:0.9501633325654274\n",
            "train loss:0.9172140000787011\n",
            "train loss:0.7394166916877155\n",
            "train loss:0.776537579532419\n",
            "train loss:0.8007418692752841\n",
            "train loss:0.664350608721108\n",
            "train loss:0.8562678801372817\n",
            "train loss:0.8962247734157914\n",
            "train loss:0.8060994899327354\n",
            "train loss:0.9221592358732277\n",
            "train loss:0.9427698865662208\n",
            "train loss:0.9232640693593643\n",
            "train loss:0.7408619860826271\n",
            "train loss:0.8985858784555191\n",
            "train loss:0.8735907130878856\n",
            "train loss:0.8931987064972471\n",
            "train loss:0.8697725523203775\n",
            "train loss:0.9512038707463353\n",
            "train loss:1.0060288478481096\n",
            "train loss:0.7741510322357228\n",
            "train loss:0.8175771779110096\n",
            "train loss:0.9452270744430784\n",
            "train loss:0.8144258858565065\n",
            "train loss:0.7216955971382909\n",
            "train loss:0.9215031242223032\n",
            "train loss:0.9584709393840316\n",
            "train loss:0.9221455795495614\n",
            "train loss:0.9037368682197586\n",
            "train loss:0.9319456405101022\n",
            "train loss:0.9028925448346534\n",
            "train loss:0.8661435338372222\n",
            "train loss:1.0238670791275388\n",
            "train loss:0.8282143361175387\n",
            "train loss:0.8355865550187346\n",
            "train loss:0.7640750007860707\n",
            "train loss:0.9072228479413836\n",
            "train loss:0.9527434952441896\n",
            "train loss:0.8046393431792407\n",
            "train loss:0.7446663531292284\n",
            "train loss:1.0547853527734707\n",
            "train loss:0.802173280020979\n",
            "train loss:0.8615774952802754\n",
            "train loss:0.8779592737866939\n",
            "train loss:0.7379495219801083\n",
            "train loss:0.8865924142893435\n",
            "train loss:0.671790815464242\n",
            "train loss:0.8598115372984511\n",
            "train loss:0.9148758171815213\n",
            "train loss:0.9805759782916301\n",
            "train loss:0.8612063971317978\n",
            "train loss:0.9163942471594888\n",
            "train loss:0.8591458708939954\n",
            "train loss:0.9124252902120991\n",
            "train loss:1.0568077281871002\n",
            "train loss:0.7378452341637411\n",
            "train loss:0.9524831713426113\n",
            "train loss:0.77081788685811\n",
            "train loss:0.7668694002744467\n",
            "train loss:0.9164444630173353\n",
            "train loss:0.7615485497086237\n",
            "train loss:0.8400464360080673\n",
            "train loss:0.8940691931637538\n",
            "train loss:1.1291323520532364\n",
            "train loss:0.8188928507224338\n",
            "train loss:0.6927106801131814\n",
            "train loss:0.695177555209947\n",
            "train loss:0.9185223052600412\n",
            "train loss:0.8845687180571908\n",
            "train loss:0.8143162632358245\n",
            "train loss:0.7252504583156332\n",
            "train loss:1.0110224156948953\n",
            "train loss:0.7736887705932812\n",
            "train loss:1.0173848883527823\n",
            "train loss:0.7825929395642883\n",
            "train loss:0.7469524048160544\n",
            "train loss:0.9660182882879949\n",
            "train loss:0.9283841925702028\n",
            "train loss:0.9094540890981843\n",
            "train loss:0.7082475052920267\n",
            "train loss:0.9478094866447451\n",
            "train loss:0.8732339604734192\n",
            "train loss:0.9743123154821565\n",
            "train loss:0.8986442863303067\n",
            "train loss:0.9780549929683597\n",
            "train loss:0.8340873916921083\n",
            "train loss:0.8584549860220487\n",
            "train loss:1.058150793481273\n",
            "train loss:0.7989225549482447\n",
            "train loss:0.8402843830493848\n",
            "train loss:0.8781317025807117\n",
            "train loss:0.8696807666798331\n",
            "train loss:0.863947793799911\n",
            "train loss:0.755290823624105\n",
            "train loss:0.9080065918793757\n",
            "train loss:0.8296295389403526\n",
            "train loss:0.8781426759217322\n",
            "train loss:0.9938786840974501\n",
            "train loss:0.7518114475206109\n",
            "train loss:0.7655547155804328\n",
            "train loss:0.7573975087593776\n",
            "train loss:0.6790560369084346\n",
            "train loss:0.950520179907311\n",
            "train loss:0.8664144556639002\n",
            "train loss:1.0451203151717212\n",
            "train loss:0.7992905450461503\n",
            "train loss:1.0858187579153913\n",
            "train loss:0.8201549705189278\n",
            "train loss:0.9079502037560689\n",
            "train loss:1.0958676164396466\n",
            "train loss:0.7388319244870217\n",
            "train loss:0.9180415146939012\n",
            "train loss:0.8235920808990521\n",
            "train loss:0.8892533363391693\n",
            "train loss:0.9222801182919924\n",
            "train loss:0.7677450260891203\n",
            "train loss:0.938942198335897\n",
            "train loss:0.898462921038529\n",
            "train loss:0.901938440019335\n",
            "train loss:0.7904050147855337\n",
            "train loss:0.9706356056227331\n",
            "train loss:0.9226237561824604\n",
            "train loss:0.814368840782747\n",
            "train loss:0.961554927416275\n",
            "train loss:0.7425498893079415\n",
            "train loss:0.8535133092179892\n",
            "train loss:0.9752149818851151\n",
            "train loss:0.7412410660111403\n",
            "train loss:0.7457395461856561\n",
            "train loss:0.8154789187636227\n",
            "train loss:0.88225562858748\n",
            "train loss:0.8342688715489353\n",
            "train loss:0.8840694380946065\n",
            "train loss:0.9853081411543052\n",
            "train loss:0.9187763048294891\n",
            "train loss:0.8656458923980577\n",
            "train loss:0.8828910979628993\n",
            "train loss:0.7666756126997349\n",
            "train loss:0.8014202000340582\n",
            "train loss:0.947622028120971\n",
            "train loss:1.064612956237044\n",
            "train loss:0.8778232447763026\n",
            "train loss:0.8341818524991836\n",
            "train loss:0.8793533979090999\n",
            "train loss:0.8275555591557571\n",
            "train loss:0.9612210734895053\n",
            "train loss:0.960941966652529\n",
            "train loss:0.8558376289460863\n",
            "train loss:0.8559186483750532\n",
            "train loss:0.8598641190835218\n",
            "train loss:0.8764637139326779\n",
            "train loss:0.7706950677663842\n",
            "train loss:0.8219155342515478\n",
            "train loss:0.8614964940974613\n",
            "train loss:1.0204437078945656\n",
            "train loss:0.8737567011210089\n",
            "train loss:0.8982558828042788\n",
            "train loss:0.9816359246825016\n",
            "train loss:0.8146176384109097\n",
            "train loss:0.8634508923851846\n",
            "train loss:0.8383135149003625\n",
            "train loss:0.9484067674465048\n",
            "train loss:0.7686126985270937\n",
            "train loss:0.780343270724659\n",
            "train loss:0.9006531103091873\n",
            "train loss:0.8061742516086301\n",
            "train loss:0.7552574949309799\n",
            "train loss:0.6667750726782596\n",
            "train loss:0.7404109387585598\n",
            "train loss:0.7862211796392945\n",
            "train loss:0.8640626253623627\n",
            "train loss:0.839541391437339\n",
            "train loss:0.9666609839774218\n",
            "train loss:0.8412790771015135\n",
            "train loss:0.961728274075046\n",
            "train loss:0.812070620040512\n",
            "train loss:0.8656923867427916\n",
            "train loss:0.8023796356681772\n",
            "train loss:0.9220740502897395\n",
            "train loss:0.9089201106274121\n",
            "train loss:0.8527069625378842\n",
            "train loss:0.9104315399997728\n",
            "train loss:0.9702808484354616\n",
            "train loss:0.9336409344501135\n",
            "train loss:0.8081742837616162\n",
            "train loss:0.8330845165200769\n",
            "train loss:0.9101731499951018\n",
            "train loss:0.9389285223724286\n",
            "train loss:1.106294081977111\n",
            "train loss:0.9965877466238701\n",
            "train loss:0.7389733130451475\n",
            "train loss:0.8086068551998676\n",
            "train loss:1.057361288501861\n",
            "train loss:0.8295927153476444\n",
            "train loss:0.8668254688400888\n",
            "train loss:0.8667169043871542\n",
            "train loss:1.0672916087108426\n",
            "train loss:0.7178823594351905\n",
            "train loss:0.8504018631602063\n",
            "train loss:0.907860778298417\n",
            "train loss:0.8462452989427193\n",
            "train loss:0.8649066615035597\n",
            "train loss:0.9495296481600859\n",
            "train loss:0.9775611769159173\n",
            "train loss:0.8779593280237719\n",
            "train loss:0.7534440351072288\n",
            "train loss:0.9675761530874162\n",
            "train loss:0.8994111407950585\n",
            "train loss:0.7644600721520217\n",
            "train loss:0.8613550738305022\n",
            "train loss:0.9797738113664543\n",
            "train loss:0.9760199901826794\n",
            "train loss:0.6790416500058791\n",
            "train loss:0.9643931216963023\n",
            "train loss:1.0327337792111222\n",
            "train loss:0.8116422540762224\n",
            "train loss:0.7478497183766813\n",
            "train loss:1.0071416010473764\n",
            "train loss:0.7378334381144098\n",
            "train loss:0.7839187064251337\n",
            "train loss:0.7962757980709011\n",
            "train loss:0.7749869347321927\n",
            "train loss:0.7232044489889866\n",
            "train loss:0.8979102542861722\n",
            "train loss:0.7875859224651282\n",
            "train loss:0.9548637048966593\n",
            "train loss:0.840717398033088\n",
            "train loss:0.9685030552426912\n",
            "train loss:0.9110618402896755\n",
            "train loss:0.9490951301652817\n",
            "train loss:0.9183959923434516\n",
            "train loss:0.8960057611278102\n",
            "train loss:0.8335557740186685\n",
            "train loss:0.9588634412032506\n",
            "train loss:0.8382708336370822\n",
            "train loss:0.7910909778821646\n",
            "train loss:0.8065567891376605\n",
            "train loss:0.9064026520275514\n",
            "train loss:0.9200418213703051\n",
            "train loss:0.8166739178585735\n",
            "train loss:0.9385302823483309\n",
            "train loss:0.8437805073282303\n",
            "train loss:0.9789443247572298\n",
            "train loss:0.8423180942534456\n",
            "train loss:0.8782590013470315\n",
            "train loss:0.8387720553385382\n",
            "train loss:0.874169839540934\n",
            "train loss:0.7894477453372076\n",
            "train loss:0.8396322297100433\n",
            "train loss:0.9509855045495564\n",
            "train loss:0.974369307095806\n",
            "train loss:0.9373388894841064\n",
            "train loss:0.8316415581088588\n",
            "train loss:0.8142712009098099\n",
            "train loss:0.8448000927470583\n",
            "train loss:0.8946836706277915\n",
            "train loss:0.8479269857066943\n",
            "train loss:0.9664049623909264\n",
            "train loss:0.9010932015654116\n",
            "train loss:0.7863924877341355\n",
            "train loss:1.0036502345194256\n",
            "train loss:0.9668597717642183\n",
            "train loss:0.7007644453022046\n",
            "train loss:0.9266471767444515\n",
            "train loss:0.9271480639080281\n",
            "train loss:0.9740820319499629\n",
            "train loss:0.9727760258379354\n",
            "train loss:0.9517695555946116\n",
            "train loss:0.8737400314645094\n",
            "train loss:0.8632370505575503\n",
            "train loss:0.9098724020556237\n",
            "train loss:0.7560996397531192\n",
            "train loss:0.7570648957282918\n",
            "train loss:0.7745848918496566\n",
            "train loss:0.9490537390345202\n",
            "train loss:0.8866146134664458\n",
            "train loss:1.004064278989441\n",
            "train loss:0.9005475220825696\n",
            "train loss:0.8021735250247114\n",
            "train loss:0.8757399754222936\n",
            "train loss:1.0508966914111015\n",
            "train loss:1.0204934943785706\n",
            "train loss:0.9220973521593446\n",
            "train loss:0.8027506206694103\n",
            "train loss:0.8459469240410071\n",
            "train loss:0.8566954452479786\n",
            "train loss:0.9001258761226404\n",
            "train loss:0.946513013057382\n",
            "train loss:0.845332792110004\n",
            "train loss:0.7820289756909884\n",
            "train loss:0.8469691484559121\n",
            "train loss:0.9862804231508481\n",
            "train loss:0.8125778159619709\n",
            "train loss:0.7918758713820102\n",
            "train loss:0.6375285530008894\n",
            "train loss:0.7929693525576947\n",
            "train loss:0.9169784765693941\n",
            "train loss:0.9009054786950599\n",
            "train loss:0.6882268377771824\n",
            "train loss:1.0124151536404222\n",
            "train loss:0.7810483667509005\n",
            "train loss:0.9129899748112057\n",
            "train loss:0.9646818834888465\n",
            "train loss:0.7166212491534393\n",
            "train loss:0.7801393296632413\n",
            "train loss:0.9468272592861148\n",
            "train loss:0.8355747191432176\n",
            "train loss:0.7206457764641813\n",
            "train loss:0.8493866074165853\n",
            "train loss:0.828531252680795\n",
            "train loss:0.8324634835348479\n",
            "train loss:0.8036685424891519\n",
            "train loss:0.8927394229626526\n",
            "train loss:0.895103327161671\n",
            "train loss:1.0009662791307277\n",
            "train loss:0.9778933041904078\n",
            "train loss:0.9263455712461215\n",
            "train loss:1.0251627378207528\n",
            "train loss:0.9925923707014398\n",
            "train loss:0.9930286753036408\n",
            "train loss:0.8410980854762495\n",
            "train loss:0.9037209670343589\n",
            "train loss:0.7627296937146629\n",
            "train loss:0.8560489244749888\n",
            "train loss:1.0584240823099118\n",
            "train loss:0.7763396030489007\n",
            "train loss:0.8108917887478312\n",
            "train loss:0.9220727809606288\n",
            "train loss:0.7917367740427982\n",
            "train loss:1.0108494095981617\n",
            "train loss:0.79716859267844\n",
            "train loss:0.7294707625233161\n",
            "train loss:0.9976514783571746\n",
            "train loss:0.8675582793853478\n",
            "train loss:0.662625027977752\n",
            "train loss:0.9575461112066135\n",
            "train loss:0.876747174271074\n",
            "train loss:0.8187668505132372\n",
            "train loss:0.872495654722632\n",
            "train loss:0.8375852219980392\n",
            "train loss:0.897625518572155\n",
            "train loss:0.7275183326555096\n",
            "train loss:0.8261115272486635\n",
            "train loss:0.874611419833284\n",
            "train loss:0.9115809385470228\n",
            "train loss:0.7820552460801133\n",
            "train loss:0.9429679670812752\n",
            "train loss:0.9329523204079675\n",
            "train loss:0.8928393214639435\n",
            "train loss:0.7993902002944689\n",
            "train loss:0.7849904889087905\n",
            "train loss:0.8823894749879918\n",
            "train loss:0.8201787020545056\n",
            "train loss:0.7810571121311564\n",
            "train loss:0.8590634770990593\n",
            "train loss:0.8862214120087702\n",
            "train loss:0.7939087764705665\n",
            "train loss:0.8069221045295842\n",
            "train loss:0.8170766055437746\n",
            "train loss:0.8674853551000458\n",
            "train loss:0.9931601691038675\n",
            "train loss:0.8063487690677733\n",
            "train loss:0.828552299289357\n",
            "train loss:0.8438891349639033\n",
            "train loss:0.826101652237309\n",
            "train loss:1.0070897593652859\n",
            "train loss:0.8341142570435598\n",
            "train loss:0.8402297553164653\n",
            "train loss:0.7395659517040991\n",
            "train loss:0.6733886677040701\n",
            "train loss:0.8191714734968442\n",
            "train loss:0.9686156471310143\n",
            "train loss:0.9610338920661705\n",
            "train loss:0.9032387824559136\n",
            "train loss:0.8969441682355407\n",
            "train loss:0.9166185045007137\n",
            "train loss:0.7962571953752249\n",
            "train loss:0.9106567973336134\n",
            "train loss:0.8761435544009666\n",
            "train loss:0.8046574850694002\n",
            "train loss:0.9242268052951792\n",
            "train loss:0.76989997851313\n",
            "train loss:1.0229080775095434\n",
            "train loss:0.9593999446311883\n",
            "train loss:0.8762003004115709\n",
            "train loss:0.8987303744472827\n",
            "train loss:0.9680923838504003\n",
            "train loss:0.8296107618416312\n",
            "train loss:0.8904645579240362\n",
            "train loss:0.9610060912152525\n",
            "train loss:0.8634516088924292\n",
            "train loss:0.7116307145163399\n",
            "train loss:0.8580704169621938\n",
            "train loss:0.9061769124951273\n",
            "train loss:0.7022844641566836\n",
            "train loss:0.9187015150820779\n",
            "train loss:0.9165093734211013\n",
            "train loss:0.8810270270909447\n",
            "train loss:0.805981250254781\n",
            "train loss:0.8420097943432168\n",
            "train loss:0.7230147675729437\n",
            "train loss:1.0204293626373673\n",
            "train loss:0.7182417057270378\n",
            "train loss:0.9764246801352384\n",
            "train loss:0.9117274013714529\n",
            "train loss:0.7443586039113786\n",
            "train loss:0.7705381345291856\n",
            "train loss:0.9830059683335428\n",
            "train loss:0.9445820619195635\n",
            "train loss:0.8891392487471624\n",
            "train loss:0.7958731841941598\n",
            "train loss:0.6786855638181784\n",
            "train loss:0.7182023651840957\n",
            "train loss:0.9582456026337777\n",
            "train loss:0.6899908839814874\n",
            "train loss:0.6893982944081233\n",
            "train loss:0.8107393448034524\n",
            "train loss:0.6562791852531678\n",
            "train loss:0.8675826860527001\n",
            "train loss:0.9700240621869584\n",
            "train loss:1.0916287218484404\n",
            "train loss:0.8924378321296592\n",
            "train loss:0.8840885247427548\n",
            "train loss:0.8806148476988848\n",
            "train loss:0.8559217541108736\n",
            "train loss:0.867972710112184\n",
            "train loss:1.080961683333209\n",
            "train loss:0.8537831798644997\n",
            "train loss:0.8565715619872136\n",
            "train loss:0.9364254209206091\n",
            "train loss:0.8196923323863841\n",
            "train loss:0.8225727730446155\n",
            "train loss:0.8325031026381037\n",
            "train loss:0.926565541468336\n",
            "train loss:0.7579329697161086\n",
            "train loss:0.9427406325975185\n",
            "train loss:0.9043182494362283\n",
            "train loss:0.8161747786587091\n",
            "train loss:0.7265140224366675\n",
            "train loss:0.9888869724171223\n",
            "train loss:0.7830305053461631\n",
            "train loss:0.8235385560710466\n",
            "train loss:0.8897097273896983\n",
            "train loss:0.8630437058973024\n",
            "train loss:0.7625364840481459\n",
            "train loss:0.8517767097232639\n",
            "train loss:1.1364696796852543\n",
            "train loss:0.8809836562996761\n",
            "train loss:0.7290053703763624\n",
            "train loss:0.9495816910361881\n",
            "train loss:0.9703381880426928\n",
            "train loss:0.798928438910528\n",
            "train loss:0.7891896100035375\n",
            "train loss:0.8800929014398237\n",
            "train loss:0.6575047579505386\n",
            "train loss:0.7148416601635419\n",
            "train loss:0.8733239750034483\n",
            "train loss:0.9386663744818411\n",
            "train loss:0.9600645458203618\n",
            "train loss:0.991593306201514\n",
            "train loss:0.9683483120591201\n",
            "train loss:0.9407137932014011\n",
            "train loss:0.7664070144259552\n",
            "train loss:0.8344781952424306\n",
            "train loss:0.806769588778545\n",
            "train loss:0.8452353770286297\n",
            "train loss:0.8893409824946333\n",
            "train loss:0.7903290473186911\n",
            "train loss:0.7859839041152434\n",
            "train loss:0.6800803140423295\n",
            "train loss:1.003635653090586\n",
            "train loss:0.8114610519977354\n",
            "train loss:0.8130574798759324\n",
            "train loss:0.8753400449477876\n",
            "train loss:0.9280042729266534\n",
            "train loss:0.9109956928260122\n",
            "train loss:0.8808092976337695\n",
            "train loss:0.8026907170998786\n",
            "train loss:0.9752223553608639\n",
            "train loss:0.7549724865521487\n",
            "train loss:0.9533236549884825\n",
            "train loss:0.8377662080505263\n",
            "train loss:0.7904961282216909\n",
            "train loss:0.8543559085993502\n",
            "train loss:1.0141355246417012\n",
            "train loss:0.7833466972963515\n",
            "train loss:0.9142891257240556\n",
            "train loss:0.8244726080133566\n",
            "train loss:0.9732366927748454\n",
            "train loss:0.9815264264095811\n",
            "train loss:0.922721144802469\n",
            "train loss:0.872883998045993\n",
            "train loss:0.8805411315546163\n",
            "train loss:0.8813175399735949\n",
            "train loss:1.0196351573787272\n",
            "train loss:0.8456087545236582\n",
            "train loss:0.8550029929034119\n",
            "train loss:0.8048002612578968\n",
            "train loss:0.8848047251224187\n",
            "train loss:0.89223436090495\n",
            "train loss:0.6992010043045147\n",
            "train loss:0.7722998471690515\n",
            "train loss:0.8877788644078342\n",
            "train loss:0.8564535614270017\n",
            "train loss:1.014790131935747\n",
            "train loss:0.8928522495463191\n",
            "train loss:0.7106517066370848\n",
            "train loss:0.8295296688082933\n",
            "train loss:0.708221451629949\n",
            "train loss:0.797000979311609\n",
            "train loss:0.7494827456855855\n",
            "train loss:0.9762156327109182\n",
            "train loss:0.9140648134056719\n",
            "train loss:1.0009697936096118\n",
            "train loss:0.8169925559262023\n",
            "train loss:0.7504746772864016\n",
            "train loss:0.8734363546846722\n",
            "train loss:0.977091669238819\n",
            "train loss:0.9140315430163488\n",
            "train loss:0.9654270834726981\n",
            "train loss:0.9078534969591634\n",
            "train loss:0.891690330358055\n",
            "train loss:0.8157334501755504\n",
            "train loss:0.7942056708505545\n",
            "=== epoch:18, train acc:0.998, test acc:0.99 ===\n",
            "train loss:0.8698608666690946\n",
            "train loss:0.5526111431202417\n",
            "train loss:0.8522432439725661\n",
            "train loss:0.8218029043776809\n",
            "train loss:0.8264851957227315\n",
            "train loss:1.0194705569136924\n",
            "train loss:0.7852824639084275\n",
            "train loss:0.8872970289218187\n",
            "train loss:0.7540305079909604\n",
            "train loss:0.851535975344578\n",
            "train loss:0.8852666730385694\n",
            "train loss:0.7706675286913431\n",
            "train loss:0.9115997070761478\n",
            "train loss:0.976576485943591\n",
            "train loss:0.8866369631567069\n",
            "train loss:0.7260163542718555\n",
            "train loss:0.69795143311184\n",
            "train loss:0.9016951622419775\n",
            "train loss:1.0057556393925389\n",
            "train loss:0.8079125074195366\n",
            "train loss:1.073631015298092\n",
            "train loss:0.8123262032019628\n",
            "train loss:0.704810522001911\n",
            "train loss:0.9059736447411953\n",
            "train loss:0.9953102779539711\n",
            "train loss:0.8430220864573047\n",
            "train loss:0.8635329759740152\n",
            "train loss:0.8417644208146003\n",
            "train loss:0.8855171871473424\n",
            "train loss:0.805075580807919\n",
            "train loss:0.9214002020227473\n",
            "train loss:0.8269706445464651\n",
            "train loss:0.93308366967522\n",
            "train loss:0.7035589836469149\n",
            "train loss:1.0302231973435105\n",
            "train loss:0.8842232384955365\n",
            "train loss:0.8206354275635188\n",
            "train loss:0.8653392411473981\n",
            "train loss:0.8165871609707344\n",
            "train loss:1.1024613779156969\n",
            "train loss:1.012505068908549\n",
            "train loss:0.8495175680293156\n",
            "train loss:0.8140443524841467\n",
            "train loss:0.8362844795304711\n",
            "train loss:0.7638826137456606\n",
            "train loss:0.8981698311553467\n",
            "train loss:1.1871098330092362\n",
            "train loss:0.8162492799005255\n",
            "train loss:0.8989850741464178\n",
            "train loss:0.978369090816512\n",
            "train loss:0.8658947971941982\n",
            "train loss:0.7738666283927398\n",
            "train loss:0.757942493311613\n",
            "train loss:0.8974598588827501\n",
            "train loss:0.8574566973711287\n",
            "train loss:0.7690793567670875\n",
            "train loss:0.8105961809144234\n",
            "train loss:0.8874045681174165\n",
            "train loss:0.8910631007957285\n",
            "train loss:0.7803894217401189\n",
            "train loss:0.7680684191907882\n",
            "train loss:0.7970535489886053\n",
            "train loss:0.9986222562161283\n",
            "train loss:0.9527401551592106\n",
            "train loss:0.9410494849214985\n",
            "train loss:1.012651314746559\n",
            "train loss:0.8532432711778116\n",
            "train loss:0.7316390546731312\n",
            "train loss:0.8102960212526473\n",
            "train loss:0.7935443447895258\n",
            "train loss:0.7651315273465359\n",
            "train loss:0.8745446985345514\n",
            "train loss:0.8573625678011199\n",
            "train loss:0.765338825597659\n",
            "train loss:0.9159014642126454\n",
            "train loss:0.9296033658587695\n",
            "train loss:0.9198559607596185\n",
            "train loss:0.9506346981069522\n",
            "train loss:1.044127384442382\n",
            "train loss:0.9434922004723097\n",
            "train loss:0.8518557652455567\n",
            "train loss:0.8657119639657317\n",
            "train loss:0.8281397134213454\n",
            "train loss:0.9072013335132371\n",
            "train loss:0.8906821553711307\n",
            "train loss:0.9153031802245043\n",
            "train loss:0.8169939536330003\n",
            "train loss:0.8093275901221376\n",
            "train loss:0.8300867472466561\n",
            "train loss:0.8588832036410556\n",
            "train loss:0.8847844524268836\n",
            "train loss:0.9421561236437427\n",
            "train loss:0.8898098112275477\n",
            "train loss:0.8854445816265499\n",
            "train loss:0.8541347917089495\n",
            "train loss:0.9516136750185353\n",
            "train loss:0.7531683042559122\n",
            "train loss:0.8376479485821848\n",
            "train loss:0.847862733984351\n",
            "train loss:0.9447491322488476\n",
            "train loss:0.8871994491140259\n",
            "train loss:0.8819472742911177\n",
            "train loss:0.6649122053380253\n",
            "train loss:0.8749343371201372\n",
            "train loss:0.8221617894149889\n",
            "train loss:0.8495975770817039\n",
            "train loss:0.7481535525087161\n",
            "train loss:0.7970494427315971\n",
            "train loss:0.8280616889363229\n",
            "train loss:0.912909184701939\n",
            "train loss:0.9284000658076565\n",
            "train loss:0.7230284150808308\n",
            "train loss:0.7913228469350593\n",
            "train loss:0.8786965794023076\n",
            "train loss:0.7645539962995129\n",
            "train loss:0.8416635735793284\n",
            "train loss:0.8755613182617674\n",
            "train loss:0.8799854909091117\n",
            "train loss:0.8426632164402083\n",
            "train loss:0.8872923430279314\n",
            "train loss:0.8627754700137743\n",
            "train loss:0.8513331446626631\n",
            "train loss:0.8119189947464956\n",
            "train loss:0.9520357163279232\n",
            "train loss:0.9778415003273802\n",
            "train loss:0.7961911297545121\n",
            "train loss:0.964778018647244\n",
            "train loss:0.7758291696025332\n",
            "train loss:0.9711234389854384\n",
            "train loss:0.7724383191600761\n",
            "train loss:1.0322936034018395\n",
            "train loss:0.8553257269673405\n",
            "train loss:0.9453213855023115\n",
            "train loss:0.8894800109955177\n",
            "train loss:0.8462999053342218\n",
            "train loss:0.9031896100639604\n",
            "train loss:0.8107425301869309\n",
            "train loss:0.9836737568771945\n",
            "train loss:0.9459592165442426\n",
            "train loss:0.8869854694522138\n",
            "train loss:0.7970998038449488\n",
            "train loss:0.8384029660182406\n",
            "train loss:0.8538234902878947\n",
            "train loss:0.9042537612444984\n",
            "train loss:0.8710114331981106\n",
            "train loss:0.8034093854339716\n",
            "train loss:0.9006582368371968\n",
            "train loss:0.9716855515673977\n",
            "train loss:0.6924238448082908\n",
            "train loss:0.7906007636618766\n",
            "train loss:0.922905182644884\n",
            "train loss:0.7547332130993754\n",
            "train loss:0.7008061909264204\n",
            "train loss:0.8925377758796038\n",
            "train loss:0.7696764478447092\n",
            "train loss:0.746374145766505\n",
            "train loss:0.8645463334459997\n",
            "train loss:0.90897088808289\n",
            "train loss:0.806975359525662\n",
            "train loss:0.8676834478558821\n",
            "train loss:0.7943713583011927\n",
            "train loss:0.8564915206998163\n",
            "train loss:0.8613318505837568\n",
            "train loss:0.9766296823054593\n",
            "train loss:0.9121210995302116\n",
            "train loss:0.8361639289782556\n",
            "train loss:0.7629653433321802\n",
            "train loss:0.7490640805691554\n",
            "train loss:0.8787797037060683\n",
            "train loss:0.7806812010159142\n",
            "train loss:0.8169325074042472\n",
            "train loss:0.8664998526602524\n",
            "train loss:0.8450044501937076\n",
            "train loss:0.8903261507894615\n",
            "train loss:0.8565281003735453\n",
            "train loss:0.8227083449915077\n",
            "train loss:0.8349321846722443\n",
            "train loss:0.8440213278912441\n",
            "train loss:0.948945436464015\n",
            "train loss:0.8944231327734841\n",
            "train loss:1.1056550806143257\n",
            "train loss:0.7842924004358934\n",
            "train loss:0.873238597348556\n",
            "train loss:0.7120735379111516\n",
            "train loss:0.9545300703611198\n",
            "train loss:0.847297092547414\n",
            "train loss:0.9498401002634143\n",
            "train loss:0.920338334185526\n",
            "train loss:1.0110267045261858\n",
            "train loss:1.0455453770890886\n",
            "train loss:0.662084249362183\n",
            "train loss:0.863980724778385\n",
            "train loss:0.9686372264047027\n",
            "train loss:0.9699719856075483\n",
            "train loss:1.030490875779672\n",
            "train loss:0.8804312787333377\n",
            "train loss:0.8230073652034643\n",
            "train loss:0.9059174921176745\n",
            "train loss:1.032220066260423\n",
            "train loss:0.9028273820536026\n",
            "train loss:0.8063210680257022\n",
            "train loss:0.7850661297379722\n",
            "train loss:0.896990741308128\n",
            "train loss:0.9017738183744932\n",
            "train loss:0.9755961709869512\n",
            "train loss:0.7870829561597111\n",
            "train loss:0.818605821480731\n",
            "train loss:0.832557598324046\n",
            "train loss:0.8923081145742178\n",
            "train loss:0.7968995677327005\n",
            "train loss:0.8719655305141218\n",
            "train loss:1.0020953287567238\n",
            "train loss:0.9168214857409387\n",
            "train loss:0.9278195826208454\n",
            "train loss:0.887800513842772\n",
            "train loss:0.9386692980957058\n",
            "train loss:0.8496908480344386\n",
            "train loss:0.8113498906737923\n",
            "train loss:0.8369715773169301\n",
            "train loss:0.8520069997607229\n",
            "train loss:1.0734548515648146\n",
            "train loss:0.7160613006107767\n",
            "train loss:0.8352613948671458\n",
            "train loss:0.8078360224036583\n",
            "train loss:0.8529900057942864\n",
            "train loss:0.8195671343205245\n",
            "train loss:0.9028282234565878\n",
            "train loss:0.8542029643169343\n",
            "train loss:0.8066054567583183\n",
            "train loss:0.8808129042509587\n",
            "train loss:0.8373058740609927\n",
            "train loss:0.7851977738193312\n",
            "train loss:0.9388909294504163\n",
            "train loss:1.086247616432957\n",
            "train loss:0.8896248125447377\n",
            "train loss:0.9363533458607993\n",
            "train loss:0.9479719106751279\n",
            "train loss:1.0333467585730247\n",
            "train loss:0.8414904241049161\n",
            "train loss:0.9849724148329845\n",
            "train loss:0.8938335854358362\n",
            "train loss:0.844090537412238\n",
            "train loss:0.9086422985819455\n",
            "train loss:0.7469249687784489\n",
            "train loss:0.7313328204735651\n",
            "train loss:0.7924941962256092\n",
            "train loss:0.8738211632832765\n",
            "train loss:0.9807537546438485\n",
            "train loss:0.9340593931998771\n",
            "train loss:0.9587051714323281\n",
            "train loss:0.676867446747512\n",
            "train loss:0.8709371919271248\n",
            "train loss:0.8831121726871821\n",
            "train loss:0.8817112433266229\n",
            "train loss:0.8823799788301079\n",
            "train loss:0.9172493450407786\n",
            "train loss:0.7710517500249086\n",
            "train loss:0.8595868786018663\n",
            "train loss:0.8947939095427994\n",
            "train loss:0.8423784765416218\n",
            "train loss:0.8905424252875417\n",
            "train loss:0.7826346559318905\n",
            "train loss:0.8054996817531682\n",
            "train loss:0.8229708350751909\n",
            "train loss:0.8366097160339939\n",
            "train loss:0.9412470314371337\n",
            "train loss:0.9906673891582336\n",
            "train loss:0.9098392003334854\n",
            "train loss:0.8461679460649617\n",
            "train loss:0.8297659537678977\n",
            "train loss:0.7836536561852313\n",
            "train loss:0.8842943864548924\n",
            "train loss:0.9368729797838453\n",
            "train loss:1.0060753366360353\n",
            "train loss:0.9899567616158785\n",
            "train loss:0.9210807209183453\n",
            "train loss:0.7686479184465282\n",
            "train loss:0.9541987347538672\n",
            "train loss:0.9022636498535133\n",
            "train loss:0.7889009858423514\n",
            "train loss:0.7958499169084622\n",
            "train loss:0.8607446092606905\n",
            "train loss:0.9547699453724409\n",
            "train loss:0.9017472169048323\n",
            "train loss:0.8910307352750219\n",
            "train loss:0.9716194227561965\n",
            "train loss:0.8561260556570052\n",
            "train loss:0.8308576315275492\n",
            "train loss:0.8850221258633372\n",
            "train loss:0.833251619864794\n",
            "train loss:0.9281229536578894\n",
            "train loss:0.8508766499818746\n",
            "train loss:0.783343389368131\n",
            "train loss:0.9364437661598256\n",
            "train loss:0.9089354767144698\n",
            "train loss:0.8256534887846538\n",
            "train loss:0.5914833946310074\n",
            "train loss:0.9461596577075869\n",
            "train loss:0.9667997544526905\n",
            "train loss:0.9055993996801703\n",
            "train loss:0.8657411954835679\n",
            "train loss:0.8721963035163924\n",
            "train loss:0.9349374157004636\n",
            "train loss:0.9008379582677244\n",
            "train loss:0.7089696512852754\n",
            "train loss:0.8773675145345979\n",
            "train loss:0.7565385147425868\n",
            "train loss:0.819437472537547\n",
            "train loss:0.896887601416326\n",
            "train loss:0.7801982753481321\n",
            "train loss:0.8187736522845966\n",
            "train loss:0.9693065515331156\n",
            "train loss:1.031265383869748\n",
            "train loss:0.9139800768562115\n",
            "train loss:0.7258571149613162\n",
            "train loss:0.7963428282190184\n",
            "train loss:0.8400003450224489\n",
            "train loss:0.8451269354360116\n",
            "train loss:0.7784054424735607\n",
            "train loss:0.7435889209318498\n",
            "train loss:0.8395023329604075\n",
            "train loss:0.8958782464923\n",
            "train loss:0.9382901988733903\n",
            "train loss:0.9067189647108915\n",
            "train loss:0.8091456511226065\n",
            "train loss:0.9935967609703417\n",
            "train loss:0.795751353890172\n",
            "train loss:0.8758126799113819\n",
            "train loss:0.9951574097527369\n",
            "train loss:0.6666585626954761\n",
            "train loss:0.9603148569233647\n",
            "train loss:0.8905182313761513\n",
            "train loss:0.8297792121193496\n",
            "train loss:1.0141334662305848\n",
            "train loss:0.9101143026970623\n",
            "train loss:0.7834726045902376\n",
            "train loss:0.7324100008592346\n",
            "train loss:0.7658430302407675\n",
            "train loss:0.9477634496426051\n",
            "train loss:0.9611715796685116\n",
            "train loss:0.9554958191041636\n",
            "train loss:0.8875486575600003\n",
            "train loss:0.8789854028112468\n",
            "train loss:0.7987498929137301\n",
            "train loss:0.8818363522002588\n",
            "train loss:0.7328714703442681\n",
            "train loss:0.7648138636691938\n",
            "train loss:0.9365598478728863\n",
            "train loss:0.8458537740053125\n",
            "train loss:0.9618160153306102\n",
            "train loss:0.8773868256380826\n",
            "train loss:0.7891882391636369\n",
            "train loss:0.9910779569189774\n",
            "train loss:0.7266207698848167\n",
            "train loss:0.7534227377260251\n",
            "train loss:0.7429978220590115\n",
            "train loss:0.7942453371741638\n",
            "train loss:0.8380442901468962\n",
            "train loss:0.8022940780230056\n",
            "train loss:0.9055851602737565\n",
            "train loss:0.8842115101790256\n",
            "train loss:0.9377439737238685\n",
            "train loss:0.9522312805614915\n",
            "train loss:0.944937330878127\n",
            "train loss:0.8635419645600142\n",
            "train loss:0.847153418824838\n",
            "train loss:0.8189935084334127\n",
            "train loss:0.8437707003371642\n",
            "train loss:0.8293317695507931\n",
            "train loss:0.7325757429411703\n",
            "train loss:0.7174191065163771\n",
            "train loss:0.7919126661970417\n",
            "train loss:0.847459896592109\n",
            "train loss:0.7567000245035002\n",
            "train loss:0.7531960558642843\n",
            "train loss:0.6680971788678508\n",
            "train loss:1.0382932712364807\n",
            "train loss:0.8107478981814248\n",
            "train loss:0.7615298646801247\n",
            "train loss:0.9906797055282296\n",
            "train loss:0.9530462045728596\n",
            "train loss:0.7758771873425492\n",
            "train loss:0.7510574823777977\n",
            "train loss:0.8538029611522853\n",
            "train loss:0.8472103360318906\n",
            "train loss:0.90136324623698\n",
            "train loss:0.8501127109557812\n",
            "train loss:1.0793257829604448\n",
            "train loss:0.9782076789931006\n",
            "train loss:0.9148520408541558\n",
            "train loss:0.875166988433108\n",
            "train loss:0.8227685506841607\n",
            "train loss:0.6531874869345308\n",
            "train loss:0.7556100978872583\n",
            "train loss:0.7754329267277498\n",
            "train loss:0.9809873962396264\n",
            "train loss:0.8690850042992107\n",
            "train loss:0.9410111151738793\n",
            "train loss:0.7332516752782356\n",
            "train loss:1.0003195045648205\n",
            "train loss:0.7680413502116495\n",
            "train loss:0.8978934500357105\n",
            "train loss:0.8392860293021489\n",
            "train loss:0.7687890436553143\n",
            "train loss:0.7499697131420885\n",
            "train loss:0.8307267131939118\n",
            "train loss:0.7799330935018485\n",
            "train loss:0.8751735818004859\n",
            "train loss:0.8363117461554274\n",
            "train loss:0.7928563550814026\n",
            "train loss:0.8621275241310936\n",
            "train loss:0.9905865150439719\n",
            "train loss:0.7949057273774572\n",
            "train loss:0.9430325303691369\n",
            "train loss:0.8246487973589247\n",
            "train loss:0.8949594373357455\n",
            "train loss:0.7369775621746775\n",
            "train loss:0.8754230793818653\n",
            "train loss:0.8924700569079903\n",
            "train loss:0.9413584019064986\n",
            "train loss:0.7323405367541556\n",
            "train loss:0.8114799173172217\n",
            "train loss:1.1370031981004738\n",
            "train loss:0.7030896283117659\n",
            "train loss:0.7948190133100824\n",
            "train loss:0.8060179664181486\n",
            "train loss:0.8652480010083337\n",
            "train loss:0.8177466856028365\n",
            "train loss:0.8785224063812882\n",
            "train loss:0.837864836473269\n",
            "train loss:0.8303908363238203\n",
            "train loss:1.0070074791715666\n",
            "train loss:0.9829598255165883\n",
            "train loss:0.8786434785663828\n",
            "train loss:0.9584771149537152\n",
            "train loss:0.7158744804703092\n",
            "train loss:0.9814373842483184\n",
            "train loss:0.8489435109984871\n",
            "train loss:0.7268851187175639\n",
            "train loss:0.9116951175352007\n",
            "train loss:0.8714697837212497\n",
            "train loss:0.7896942926147886\n",
            "train loss:0.7404394078039036\n",
            "train loss:0.8069406201136079\n",
            "train loss:1.018798142396833\n",
            "train loss:0.8329884622543047\n",
            "train loss:0.8120231305071869\n",
            "train loss:0.7550071435746344\n",
            "train loss:0.9621966819659908\n",
            "train loss:0.8650829222472085\n",
            "train loss:0.7014089881902469\n",
            "train loss:0.9129964466782764\n",
            "train loss:0.6618204039330141\n",
            "train loss:0.9000438179136325\n",
            "train loss:0.8526358058768786\n",
            "train loss:0.9139754599673903\n",
            "train loss:0.8967987975733164\n",
            "train loss:0.8712706896604284\n",
            "train loss:0.8660272709715291\n",
            "train loss:0.9409273813936955\n",
            "train loss:1.035461626250413\n",
            "train loss:0.7653120301681468\n",
            "train loss:0.7485700505050191\n",
            "train loss:0.8588801109383106\n",
            "train loss:0.8559531264109578\n",
            "train loss:0.7030059929875746\n",
            "train loss:0.8674967963826233\n",
            "train loss:0.9088064403342081\n",
            "train loss:0.8828661441845922\n",
            "train loss:0.9013142897178522\n",
            "train loss:0.9180109809840951\n",
            "train loss:0.7819607663146603\n",
            "train loss:0.7570372690489101\n",
            "train loss:0.7685774603871697\n",
            "train loss:0.7927650086077597\n",
            "train loss:1.023414162035535\n",
            "train loss:0.9052711388772393\n",
            "train loss:0.7441981038573505\n",
            "train loss:0.7518829291434769\n",
            "train loss:0.7384282654664354\n",
            "train loss:0.8225793723849687\n",
            "train loss:0.9174807796994748\n",
            "train loss:0.9065303528265352\n",
            "train loss:0.8042986290929944\n",
            "train loss:0.8644511745605286\n",
            "train loss:0.7532135473403939\n",
            "train loss:0.7456253206853393\n",
            "train loss:0.8280686066601979\n",
            "train loss:0.9832144544530931\n",
            "train loss:0.9228838328918088\n",
            "train loss:0.9185376494569611\n",
            "train loss:0.7603635563646877\n",
            "train loss:0.8106188636517844\n",
            "train loss:0.9022353968544264\n",
            "train loss:0.8426271589842077\n",
            "train loss:0.9665538333919851\n",
            "train loss:0.8583620947331123\n",
            "train loss:0.6189023563804283\n",
            "train loss:0.9786257310650238\n",
            "train loss:0.7484536133975778\n",
            "train loss:0.8451228518818742\n",
            "train loss:0.8983250143957244\n",
            "train loss:0.7595867087138447\n",
            "train loss:0.7438894860941748\n",
            "train loss:1.0052856528655338\n",
            "train loss:0.8036479205355388\n",
            "train loss:0.870119300339071\n",
            "train loss:0.9233142103723053\n",
            "train loss:0.8282173833374898\n",
            "train loss:0.9088359754308029\n",
            "train loss:0.9623170392064722\n",
            "train loss:0.809528892712143\n",
            "train loss:0.9055387823298784\n",
            "train loss:0.8615729659645524\n",
            "train loss:0.9369527656701342\n",
            "train loss:0.7799576145262168\n",
            "train loss:0.8921383044813388\n",
            "train loss:0.8976057435639292\n",
            "train loss:1.0246316979156966\n",
            "train loss:0.9780954542263103\n",
            "train loss:0.9434378495453091\n",
            "train loss:0.9435713159951953\n",
            "train loss:0.7985716196656307\n",
            "train loss:0.9263025928184393\n",
            "train loss:0.8277582483353885\n",
            "train loss:1.0028796961270954\n",
            "train loss:0.8656756055798985\n",
            "train loss:0.8990920018462379\n",
            "train loss:0.9103681125239131\n",
            "train loss:0.8713325971180598\n",
            "train loss:0.8997550516793802\n",
            "train loss:0.7797621507725022\n",
            "train loss:0.8693580055239145\n",
            "train loss:0.9787957107099408\n",
            "train loss:0.880043654308743\n",
            "train loss:0.8338966396671543\n",
            "train loss:0.8512745058948415\n",
            "train loss:0.7762725528316552\n",
            "train loss:0.8218744776231846\n",
            "train loss:0.7995212012584438\n",
            "train loss:0.9503723737422122\n",
            "train loss:0.9406532804610259\n",
            "train loss:0.8901692876105856\n",
            "train loss:0.7397758463464497\n",
            "train loss:0.8779984542878462\n",
            "train loss:0.8512194395762497\n",
            "train loss:0.7974845349238774\n",
            "train loss:0.9082940165932173\n",
            "train loss:0.9208804275836379\n",
            "train loss:0.8963960705976167\n",
            "train loss:1.0275546878197965\n",
            "train loss:1.031169013279648\n",
            "train loss:0.8502143968752479\n",
            "train loss:1.0201940298907823\n",
            "train loss:0.9301491614748527\n",
            "train loss:0.9538080998867704\n",
            "train loss:1.0359596693893918\n",
            "train loss:0.9955005068226542\n",
            "train loss:0.8333221576779494\n",
            "train loss:0.940349498980099\n",
            "train loss:0.8304984156149099\n",
            "train loss:1.0921925194485842\n",
            "train loss:0.8672372014876744\n",
            "train loss:0.8859380487612336\n",
            "train loss:0.824163473735272\n",
            "train loss:0.9447426562413095\n",
            "train loss:0.7630036258100507\n",
            "train loss:0.9621598084302336\n",
            "train loss:0.7934284406530707\n",
            "train loss:0.9159400925146602\n",
            "train loss:0.6900353049163186\n",
            "train loss:0.8427856670529371\n",
            "train loss:0.9959613979953592\n",
            "train loss:0.8178175362099483\n",
            "train loss:0.8138995942063728\n",
            "train loss:0.8331821127443685\n",
            "train loss:0.8313763720492341\n",
            "train loss:0.8852355344071325\n",
            "train loss:0.83023984806534\n",
            "train loss:0.7295374674907404\n",
            "train loss:0.8938499932400467\n",
            "train loss:0.8492056323072245\n",
            "train loss:0.8834958847810254\n",
            "train loss:0.8482311743885929\n",
            "train loss:0.7742643404923172\n",
            "train loss:1.0565410384644738\n",
            "train loss:0.8260797217945829\n",
            "train loss:0.9422334387573982\n",
            "train loss:0.7839005987938146\n",
            "train loss:0.7693364561374568\n",
            "train loss:0.7925470859852041\n",
            "train loss:0.741630326467774\n",
            "train loss:0.9750581962965272\n",
            "train loss:0.7990851534483912\n",
            "train loss:0.938352522723618\n",
            "train loss:0.6904376554663746\n",
            "train loss:0.7242267876731746\n",
            "train loss:0.904040910917352\n",
            "train loss:0.9377370308635846\n",
            "train loss:0.8749111183308304\n",
            "=== epoch:19, train acc:0.999, test acc:0.99 ===\n",
            "train loss:0.6958830154706663\n",
            "train loss:0.8042418189754386\n",
            "train loss:0.9611049555868421\n",
            "train loss:0.7472452383666162\n",
            "train loss:0.9777000395503169\n",
            "train loss:0.7062003450004157\n",
            "train loss:0.9654139174525536\n",
            "train loss:0.8858279284841761\n",
            "train loss:0.8575683855645307\n",
            "train loss:0.7973017170264876\n",
            "train loss:0.8482060879804598\n",
            "train loss:0.7216127989952805\n",
            "train loss:0.9782439249669117\n",
            "train loss:0.7872381325753467\n",
            "train loss:1.0238533301032824\n",
            "train loss:0.8678384556781024\n",
            "train loss:0.7719218352556706\n",
            "train loss:0.7862462086304046\n",
            "train loss:0.8597178645236526\n",
            "train loss:0.924373129285975\n",
            "train loss:0.7376182434195294\n",
            "train loss:0.8342687549320429\n",
            "train loss:0.8408077568070861\n",
            "train loss:0.853072374962588\n",
            "train loss:0.8791991659632346\n",
            "train loss:0.9356510800058437\n",
            "train loss:0.9656218705553988\n",
            "train loss:0.9247610717578835\n",
            "train loss:0.7966432747720876\n",
            "train loss:0.9066917052737449\n",
            "train loss:0.9342051016407753\n",
            "train loss:0.7858023493368695\n",
            "train loss:0.8165159003309896\n",
            "train loss:0.9190940634018036\n",
            "train loss:0.9703732931065083\n",
            "train loss:0.8257427642426923\n",
            "train loss:0.7791980119559478\n",
            "train loss:0.8818359203531138\n",
            "train loss:0.8693131072089638\n",
            "train loss:0.7570367914081519\n",
            "train loss:0.9455677173483928\n",
            "train loss:0.8760887940571219\n",
            "train loss:0.8726397403387616\n",
            "train loss:0.8873856562413939\n",
            "train loss:0.888210904277529\n",
            "train loss:0.887313969868073\n",
            "train loss:0.7685005110780158\n",
            "train loss:0.9653402837658278\n",
            "train loss:0.9318941353263077\n",
            "train loss:0.8627060033869519\n",
            "train loss:0.9532570549203107\n",
            "train loss:0.9068740792555263\n",
            "train loss:0.6808655311223901\n",
            "train loss:0.7933972583198405\n",
            "train loss:0.7807102097982647\n",
            "train loss:0.8763862579563424\n",
            "train loss:0.8739560965478766\n",
            "train loss:0.9744286251568299\n",
            "train loss:0.8680532427822242\n",
            "train loss:1.0587901783983416\n",
            "train loss:0.9539693598120266\n",
            "train loss:0.9459325803268993\n",
            "train loss:0.8225028560850681\n",
            "train loss:0.7335244083666411\n",
            "train loss:1.020395061842037\n",
            "train loss:0.9419967986735732\n",
            "train loss:0.927813242355779\n",
            "train loss:0.8524010348116211\n",
            "train loss:0.8222892764819264\n",
            "train loss:0.8971654634651296\n",
            "train loss:0.9185650286745638\n",
            "train loss:0.7664310233205208\n",
            "train loss:0.8938633321100875\n",
            "train loss:0.8436614690350757\n",
            "train loss:0.8866750075448218\n",
            "train loss:0.7277968841260415\n",
            "train loss:0.77806010532925\n",
            "train loss:0.9079440480942167\n",
            "train loss:0.7692407293641638\n",
            "train loss:0.7387465785845364\n",
            "train loss:0.6987790583007589\n",
            "train loss:0.7992682637249807\n",
            "train loss:0.9005021408901588\n",
            "train loss:0.811208680430694\n",
            "train loss:0.9253290696269724\n",
            "train loss:0.8831321092820988\n",
            "train loss:0.8999948810024192\n",
            "train loss:0.9330150039801868\n",
            "train loss:0.8383163447809694\n",
            "train loss:0.6635595762169099\n",
            "train loss:0.7895857439232782\n",
            "train loss:0.864379830714698\n",
            "train loss:0.7683646010265267\n",
            "train loss:0.9129710689805965\n",
            "train loss:0.7837632217186329\n",
            "train loss:1.0538146154084693\n",
            "train loss:1.1160008185944914\n",
            "train loss:0.8622486563815214\n",
            "train loss:0.8623332958085334\n",
            "train loss:0.8339702935024347\n",
            "train loss:0.7755256195999278\n",
            "train loss:0.8165792838540881\n",
            "train loss:0.8707058241935571\n",
            "train loss:0.8263323481788697\n",
            "train loss:0.8795676382271987\n",
            "train loss:0.7857410719962747\n",
            "train loss:0.9205626159593717\n",
            "train loss:0.8037466704137018\n",
            "train loss:0.8819004866615029\n",
            "train loss:0.87980987235508\n",
            "train loss:0.8693874592815692\n",
            "train loss:0.9409583888629058\n",
            "train loss:0.878169195878342\n",
            "train loss:0.9691465454676259\n",
            "train loss:0.7762287291197018\n",
            "train loss:1.0012448742297715\n",
            "train loss:1.0804136084330758\n",
            "train loss:1.0055289368092886\n",
            "train loss:0.8819284361643551\n",
            "train loss:0.8287440199178475\n",
            "train loss:0.9174195884379981\n",
            "train loss:0.8770314722979303\n",
            "train loss:0.9586319589317331\n",
            "train loss:0.9021273492570313\n",
            "train loss:0.8897702698943415\n",
            "train loss:0.8199375923055081\n",
            "train loss:0.9471631881101922\n",
            "train loss:0.7858834998613082\n",
            "train loss:0.9750796134583175\n",
            "train loss:1.0629995054146373\n",
            "train loss:0.7716434732445958\n",
            "train loss:0.7744318927390141\n",
            "train loss:0.7362120261103844\n",
            "train loss:0.8146336019891142\n",
            "train loss:0.8933988478795492\n",
            "train loss:0.7606438776875991\n",
            "train loss:0.8524770408338724\n",
            "train loss:0.9772760298391355\n",
            "train loss:0.9631065076104267\n",
            "train loss:0.9388236191686647\n",
            "train loss:0.9091021904511658\n",
            "train loss:0.9917984039712608\n",
            "train loss:0.9117264971362457\n",
            "train loss:1.0263610400071732\n",
            "train loss:0.9104428926259251\n",
            "train loss:1.0024032972123578\n",
            "train loss:1.0732723311667391\n",
            "train loss:0.7925481419596205\n",
            "train loss:0.7383810679876724\n",
            "train loss:1.0065484410623895\n",
            "train loss:1.0309931219099804\n",
            "train loss:0.7419596824849879\n",
            "train loss:0.888214278504606\n",
            "train loss:0.8682268209787147\n",
            "train loss:0.6793722704923358\n",
            "train loss:0.9550274798941181\n",
            "train loss:0.9583711383471433\n",
            "train loss:0.8498093957427051\n",
            "train loss:1.0925035551827573\n",
            "train loss:0.822741684173689\n",
            "train loss:1.029085860355696\n",
            "train loss:0.7634424487305423\n",
            "train loss:0.9929124751460998\n",
            "train loss:0.8647164873145659\n",
            "train loss:0.8188540377279001\n",
            "train loss:0.9827427876990167\n",
            "train loss:0.9400577731381642\n",
            "train loss:0.8434230098525392\n",
            "train loss:0.7480705762270421\n",
            "train loss:0.8366582559347493\n",
            "train loss:0.7374525896062772\n",
            "train loss:0.9427688647241759\n",
            "train loss:0.8036488441755526\n",
            "train loss:1.079951783479216\n",
            "train loss:0.8247809114816899\n",
            "train loss:0.8987283874669592\n",
            "train loss:0.8239066499280825\n",
            "train loss:0.9396593482982617\n",
            "train loss:0.935973129781545\n",
            "train loss:1.0132373740278742\n",
            "train loss:0.9270218097337006\n",
            "train loss:0.800844664134171\n",
            "train loss:0.8909576945355917\n",
            "train loss:0.7962371602957519\n",
            "train loss:0.7452568060225001\n",
            "train loss:0.8957774437964732\n",
            "train loss:0.8272791528125558\n",
            "train loss:0.7947208864009333\n",
            "train loss:0.9482163684441334\n",
            "train loss:0.9113563365857987\n",
            "train loss:0.8209678127568832\n",
            "train loss:0.7754333210496861\n",
            "train loss:0.8021559747756681\n",
            "train loss:0.955414001928269\n",
            "train loss:0.8356410608253284\n",
            "train loss:1.0277720251347418\n",
            "train loss:0.703373823723047\n",
            "train loss:0.9715022414974502\n",
            "train loss:0.7113015401922624\n",
            "train loss:0.8660165126810901\n",
            "train loss:0.8036504841459507\n",
            "train loss:0.9996386622957926\n",
            "train loss:0.9263181139412832\n",
            "train loss:0.838080947008947\n",
            "train loss:0.8829253434635445\n",
            "train loss:0.7118197764615661\n",
            "train loss:0.8647877571816732\n",
            "train loss:0.7930097886931402\n",
            "train loss:0.8846111577634901\n",
            "train loss:0.7901530574483573\n",
            "train loss:1.0433954116357718\n",
            "train loss:0.8263656080372573\n",
            "train loss:0.7719719245222073\n",
            "train loss:0.636794905382596\n",
            "train loss:0.8821676983803283\n",
            "train loss:0.9023320269805211\n",
            "train loss:0.7583782069965531\n",
            "train loss:0.9970909730324929\n",
            "train loss:0.8900871390021956\n",
            "train loss:0.86878645468333\n",
            "train loss:0.6882831634239948\n",
            "train loss:0.7687904995633371\n",
            "train loss:0.9112304943964088\n",
            "train loss:0.992874843870037\n",
            "train loss:0.8584626945221071\n",
            "train loss:0.8472181111722462\n",
            "train loss:0.9393755369304447\n",
            "train loss:0.8739959602939232\n",
            "train loss:0.9093268164844962\n",
            "train loss:0.8631970295125957\n",
            "train loss:0.7925370114669248\n",
            "train loss:0.7521658619064698\n",
            "train loss:0.7968256582171952\n",
            "train loss:0.7653271101477784\n",
            "train loss:0.8162703682609747\n",
            "train loss:0.8608220353794707\n",
            "train loss:0.7529569234491242\n",
            "train loss:0.9222455689916775\n",
            "train loss:0.8357675670192302\n",
            "train loss:0.811292357624813\n",
            "train loss:0.8782411507015341\n",
            "train loss:0.741592667971254\n",
            "train loss:0.7805879167248129\n",
            "train loss:0.7649162569753613\n",
            "train loss:0.7655637302532982\n",
            "train loss:0.7844794425050092\n",
            "train loss:0.8801523685252107\n",
            "train loss:0.7990347303319889\n",
            "train loss:0.8999004058117457\n",
            "train loss:0.7175366575073674\n",
            "train loss:0.8162391372106231\n",
            "train loss:0.8703206925134908\n",
            "train loss:0.8188316637777808\n",
            "train loss:0.8627826178316256\n",
            "train loss:0.8168567297476619\n",
            "train loss:0.8126553599825098\n",
            "train loss:0.737122389549737\n",
            "train loss:0.7646351814084261\n",
            "train loss:0.9466507844474336\n",
            "train loss:0.8147515230183826\n",
            "train loss:0.9851377469643093\n",
            "train loss:0.7928561801243387\n",
            "train loss:0.9393302162883899\n",
            "train loss:0.8391487877116879\n",
            "train loss:0.7667510651963068\n",
            "train loss:0.9166266531091033\n",
            "train loss:0.8797182879082489\n",
            "train loss:0.7313872153866952\n",
            "train loss:0.7621652217962545\n",
            "train loss:0.8455202331183485\n",
            "train loss:1.02604955661335\n",
            "train loss:0.9414846873395445\n",
            "train loss:0.7933588722782717\n",
            "train loss:0.6139344115275186\n",
            "train loss:0.8723824591717763\n",
            "train loss:0.9223720577509681\n",
            "train loss:0.6509516941384266\n",
            "train loss:0.9551097952122221\n",
            "train loss:0.9389834386649748\n",
            "train loss:0.887099228756039\n",
            "train loss:1.0769354994194729\n",
            "train loss:0.8544267934784652\n",
            "train loss:0.8278243125216843\n",
            "train loss:0.782563749307591\n",
            "train loss:0.7760673337708676\n",
            "train loss:0.8883648382326692\n",
            "train loss:0.872555115990974\n",
            "train loss:0.7657251260170196\n",
            "train loss:0.7655563662382289\n",
            "train loss:0.8574480030462215\n",
            "train loss:0.6674101684862158\n",
            "train loss:0.7908513946204617\n",
            "train loss:0.7888643378484221\n",
            "train loss:0.6463076091678824\n",
            "train loss:0.644500487214197\n",
            "train loss:0.8564985280954877\n",
            "train loss:0.9802590572496578\n",
            "train loss:1.0896795118063414\n",
            "train loss:0.9651786841577805\n",
            "train loss:0.7840193468912314\n",
            "train loss:0.7382520367317863\n",
            "train loss:1.0366165048187814\n",
            "train loss:0.8264700342840193\n",
            "train loss:0.7699094173720674\n",
            "train loss:0.8471363657376572\n",
            "train loss:0.8707060726718521\n",
            "train loss:0.7871945290469116\n",
            "train loss:0.7157129527022722\n",
            "train loss:0.8030816266560349\n",
            "train loss:0.8643193798969852\n",
            "train loss:0.8507439412852679\n",
            "train loss:0.7172763621476264\n",
            "train loss:0.8968411326419812\n",
            "train loss:0.7857457988916959\n",
            "train loss:0.6840590763595048\n",
            "train loss:0.7518239279635058\n",
            "train loss:0.8483272897342935\n",
            "train loss:0.7943607422087956\n",
            "train loss:0.9688181132269849\n",
            "train loss:0.8433697933392433\n",
            "train loss:0.815111592808051\n",
            "train loss:0.9574730679880689\n",
            "train loss:0.8673088188609634\n",
            "train loss:0.9897615133007622\n",
            "train loss:0.8002712051855696\n",
            "train loss:0.9557455373802913\n",
            "train loss:0.8754106680824444\n",
            "train loss:0.8242089284726525\n",
            "train loss:0.7047284339910462\n",
            "train loss:0.702528726602505\n",
            "train loss:0.8513290864090867\n",
            "train loss:0.8722627769019293\n",
            "train loss:0.9000691358747064\n",
            "train loss:0.821944703378948\n",
            "train loss:0.9556209525106408\n",
            "train loss:0.8565500706953421\n",
            "train loss:0.7589353184830152\n",
            "train loss:0.8295964012578289\n",
            "train loss:0.9062503258442571\n",
            "train loss:0.929668596065888\n",
            "train loss:0.7712210329338945\n",
            "train loss:1.013773523658438\n",
            "train loss:0.925980591745248\n",
            "train loss:0.816661740175599\n",
            "train loss:0.9327352700749875\n",
            "train loss:0.8573714154979677\n",
            "train loss:0.8271476315613014\n",
            "train loss:0.9435521949261725\n",
            "train loss:0.828661721254883\n",
            "train loss:0.855490985730855\n",
            "train loss:0.7962121181008951\n",
            "train loss:0.9867037393546625\n",
            "train loss:0.7880288237527722\n",
            "train loss:0.917107063065217\n",
            "train loss:0.9811172956177562\n",
            "train loss:1.0002597424429511\n",
            "train loss:1.0842900424996507\n",
            "train loss:0.9175412600154279\n",
            "train loss:0.9641852649757692\n",
            "train loss:0.8017791526187067\n",
            "train loss:0.7971799535729628\n",
            "train loss:0.8575454541424992\n",
            "train loss:0.8432431806195957\n",
            "train loss:0.8523483772090604\n",
            "train loss:0.8119500177442491\n",
            "train loss:0.8686172642101626\n",
            "train loss:0.775228321636954\n",
            "train loss:0.867083949080493\n",
            "train loss:0.9243511444820257\n",
            "train loss:1.0358208533613502\n",
            "train loss:0.9317046995005472\n",
            "train loss:0.9270375273791182\n",
            "train loss:0.8908992190879459\n",
            "train loss:0.9000221499714957\n",
            "train loss:0.8818231045083804\n",
            "train loss:0.8161377947838634\n",
            "train loss:0.8094231590955566\n",
            "train loss:0.7503221224051575\n",
            "train loss:0.7281926487708881\n",
            "train loss:0.9931117592600693\n",
            "train loss:0.9355613356152213\n",
            "train loss:0.7264282300023365\n",
            "train loss:1.025409259300202\n",
            "train loss:0.8526601104447036\n",
            "train loss:0.7343866071415681\n",
            "train loss:0.7886039247059636\n",
            "train loss:0.7211143531659916\n",
            "train loss:0.8227062649444659\n",
            "train loss:0.8973094091437953\n",
            "train loss:0.8503871605943699\n",
            "train loss:0.8630462562721983\n",
            "train loss:0.8817045485537843\n",
            "train loss:0.8465959747856171\n",
            "train loss:0.8770371913857936\n",
            "train loss:0.9565963534157254\n",
            "train loss:0.9321467867789194\n",
            "train loss:0.9017491907554432\n",
            "train loss:0.8002860699907651\n",
            "train loss:0.8077335011338849\n",
            "train loss:0.810935714394276\n",
            "train loss:0.8306916959260013\n",
            "train loss:0.8035214235638111\n",
            "train loss:0.883542353645098\n",
            "train loss:0.727130280905915\n",
            "train loss:0.7393937218502606\n",
            "train loss:0.9640565917828837\n",
            "train loss:0.7567323567744233\n",
            "train loss:0.8569879754511905\n",
            "train loss:1.0332503522595986\n",
            "train loss:0.7245379079877636\n",
            "train loss:0.9086939487275086\n",
            "train loss:0.8936630820654554\n",
            "train loss:0.8493737734916107\n",
            "train loss:0.720006030230236\n",
            "train loss:0.7971079194611499\n",
            "train loss:0.7964771616831093\n",
            "train loss:0.8763148122631695\n",
            "train loss:0.9382649054676822\n",
            "train loss:0.9284737779938216\n",
            "train loss:0.8478939097651493\n",
            "train loss:0.8069342732429331\n",
            "train loss:0.816474078277033\n",
            "train loss:0.8856215623525157\n",
            "train loss:0.9150783420977046\n",
            "train loss:0.9207875134308621\n",
            "train loss:0.8372639675303952\n",
            "train loss:0.9183716106448714\n",
            "train loss:0.8330451783688291\n",
            "train loss:0.9765875868193261\n",
            "train loss:0.8865948443611475\n",
            "train loss:0.8827491716367561\n",
            "train loss:0.8904950852407532\n",
            "train loss:0.9482726875556231\n",
            "train loss:0.7783096661963252\n",
            "train loss:0.9149264603888496\n",
            "train loss:0.9909729769932347\n",
            "train loss:0.8041814971365124\n",
            "train loss:0.9514299603610631\n",
            "train loss:1.0011742316773558\n",
            "train loss:0.9181358156322122\n",
            "train loss:0.8905830693840922\n",
            "train loss:0.9321417246138626\n",
            "train loss:0.944544044987377\n",
            "train loss:0.8861825473208172\n",
            "train loss:0.9013683798418736\n",
            "train loss:0.8848658355798419\n",
            "train loss:0.951093400803012\n",
            "train loss:0.8538801255842919\n",
            "train loss:0.691287719310538\n",
            "train loss:0.8220680203794635\n",
            "train loss:0.799716487225717\n",
            "train loss:0.8206546999620389\n",
            "train loss:0.7330376083158215\n",
            "train loss:0.7647749219928305\n",
            "train loss:0.8000879404978442\n",
            "train loss:0.8917926617715837\n",
            "train loss:0.9489846765694547\n",
            "train loss:0.9159422786918588\n",
            "train loss:0.7542201459978309\n",
            "train loss:0.7812027688820256\n",
            "train loss:0.8684301312150668\n",
            "train loss:0.8078267874463029\n",
            "train loss:0.9405748356649255\n",
            "train loss:0.853564269462813\n",
            "train loss:0.7610614222424238\n",
            "train loss:1.1245227886047948\n",
            "train loss:0.8349142378034263\n",
            "train loss:0.8190855773480714\n",
            "train loss:1.1583911298103906\n",
            "train loss:1.046933924690528\n",
            "train loss:0.7825273941207231\n",
            "train loss:0.9417837729548323\n",
            "train loss:1.0027227492922954\n",
            "train loss:0.8083892347568914\n",
            "train loss:0.7460930671656985\n",
            "train loss:0.9731571763257676\n",
            "train loss:0.9187698645476695\n",
            "train loss:1.0356071094413968\n",
            "train loss:1.0622744426851485\n",
            "train loss:1.01155033783471\n",
            "train loss:0.9409654365627731\n",
            "train loss:0.9243904658094366\n",
            "train loss:0.9750493931711982\n",
            "train loss:0.8216537017777356\n",
            "train loss:0.7520410079967218\n",
            "train loss:0.9392967137433638\n",
            "train loss:0.9594594599968567\n",
            "train loss:0.9386432260994154\n",
            "train loss:0.8579662690940976\n",
            "train loss:0.9186887330127367\n",
            "train loss:0.7646678930871827\n",
            "train loss:0.8729746113442455\n",
            "train loss:0.9616164803388326\n",
            "train loss:0.6844080600595391\n",
            "train loss:0.9487762124344177\n",
            "train loss:1.0767518687062483\n",
            "train loss:0.8560160836012823\n",
            "train loss:0.8325607581658891\n",
            "train loss:0.8639835775231555\n",
            "train loss:0.9599873394484847\n",
            "train loss:0.8820723180843508\n",
            "train loss:0.7364790903185258\n",
            "train loss:0.7212114801338139\n",
            "train loss:0.7829670629457725\n",
            "train loss:0.8066910507292203\n",
            "train loss:0.9310755659882397\n",
            "train loss:0.8338340995543381\n",
            "train loss:0.8192330637456209\n",
            "train loss:0.9183940163571914\n",
            "train loss:0.8536094503628938\n",
            "train loss:1.0187966124240764\n",
            "train loss:0.6662758254631541\n",
            "train loss:0.9447240374480482\n",
            "train loss:0.9310449303376501\n",
            "train loss:0.90383843793519\n",
            "train loss:0.8785728795692436\n",
            "train loss:0.7230675614173804\n",
            "train loss:0.7192772934346682\n",
            "train loss:0.6700965754464238\n",
            "train loss:0.9880287296682382\n",
            "train loss:0.6869460907047952\n",
            "train loss:0.8100059934212064\n",
            "train loss:1.006800949482216\n",
            "train loss:0.9426509810588809\n",
            "train loss:0.8915394876848508\n",
            "train loss:0.9561182874913229\n",
            "train loss:0.7975473191843361\n",
            "train loss:0.7608240498809329\n",
            "train loss:0.9502960769936628\n",
            "train loss:0.8030406675833051\n",
            "train loss:0.8154532924063994\n",
            "train loss:0.8049863579595951\n",
            "train loss:1.0022081080393999\n",
            "train loss:0.7599702531139623\n",
            "train loss:0.9222384445964262\n",
            "train loss:0.8654169398633652\n",
            "train loss:0.8204706944418549\n",
            "train loss:0.7559174493338919\n",
            "train loss:0.8118639099757168\n",
            "train loss:0.7658800735057071\n",
            "train loss:0.7287466684662932\n",
            "train loss:0.8893609580410851\n",
            "train loss:0.9007667968860972\n",
            "train loss:0.7497206683033029\n",
            "train loss:0.8612953464415033\n",
            "train loss:0.877452394581254\n",
            "train loss:0.8173699365707483\n",
            "train loss:0.8362618155690466\n",
            "train loss:1.0056388830528407\n",
            "train loss:0.8583590516590605\n",
            "train loss:0.7303045588899962\n",
            "train loss:0.8004266014982907\n",
            "train loss:0.9761979030949339\n",
            "train loss:1.0395181275570138\n",
            "train loss:0.8282934887954534\n",
            "train loss:0.9683599691963961\n",
            "train loss:0.9411082354963431\n",
            "train loss:0.788984660596391\n",
            "train loss:0.9197563249556319\n",
            "train loss:0.9072388496465547\n",
            "train loss:0.8578377792505049\n",
            "train loss:0.8445531842261235\n",
            "train loss:0.8091832810504487\n",
            "train loss:0.77162978106356\n",
            "train loss:0.8113271878011195\n",
            "train loss:0.8378696243560441\n",
            "train loss:0.9630576660247226\n",
            "train loss:0.8478372173407077\n",
            "train loss:0.7969660968877702\n",
            "train loss:0.9543818822074389\n",
            "train loss:0.7780276326810258\n",
            "train loss:0.8294664355074789\n",
            "train loss:0.9033514841515352\n",
            "train loss:0.8852956144325129\n",
            "train loss:0.8781705627934369\n",
            "train loss:0.7721764179950639\n",
            "train loss:0.9001403588116679\n",
            "train loss:0.8348341216834256\n",
            "train loss:0.9704394191542812\n",
            "train loss:0.7712785059425018\n",
            "train loss:0.7574369876096594\n",
            "train loss:0.8760362090998873\n",
            "train loss:0.7870112236867252\n",
            "train loss:0.8522070751296316\n",
            "train loss:0.878147909521948\n",
            "train loss:0.8986082633571831\n",
            "train loss:0.8729468096839047\n",
            "train loss:0.7822029627647592\n",
            "train loss:1.0666612835780214\n",
            "train loss:0.811474233373878\n",
            "train loss:0.8764905583924749\n",
            "train loss:0.9429400582081849\n",
            "train loss:0.8027121418221351\n",
            "train loss:0.7115312731974561\n",
            "train loss:0.8231463694828043\n",
            "train loss:0.972531322804075\n",
            "train loss:0.8654915672956068\n",
            "train loss:0.7417172344115827\n",
            "train loss:0.8650525834723998\n",
            "train loss:0.9010640480088778\n",
            "=== epoch:20, train acc:0.997, test acc:0.995 ===\n",
            "train loss:0.8643382545449444\n",
            "train loss:0.8685291757524263\n",
            "train loss:0.9162856882606226\n",
            "train loss:1.1446460898978597\n",
            "train loss:0.8299556920982795\n",
            "train loss:0.7466276529434466\n",
            "train loss:0.7277821179500841\n",
            "train loss:0.9024495791437901\n",
            "train loss:0.8719638373302598\n",
            "train loss:0.7813432259888619\n",
            "train loss:0.7358893126093207\n",
            "train loss:0.8579525820812155\n",
            "train loss:0.8616984119680517\n",
            "train loss:0.750332488927884\n",
            "train loss:0.8022372240016034\n",
            "train loss:0.9647074571793398\n",
            "train loss:0.8273710071944725\n",
            "train loss:0.7880829165151546\n",
            "train loss:0.8887476259116925\n",
            "train loss:0.6251149570853266\n",
            "train loss:0.7933711474818544\n",
            "train loss:0.8355985209831794\n",
            "train loss:0.8470797183682055\n",
            "train loss:0.7982431104998142\n",
            "train loss:0.7370485960310043\n",
            "train loss:0.7611026778668556\n",
            "train loss:0.9405527996812749\n",
            "train loss:0.8951648897435425\n",
            "train loss:0.7406449612847266\n",
            "train loss:0.8070360733672274\n",
            "train loss:0.7840029454246158\n",
            "train loss:0.7814322179741805\n",
            "train loss:0.8664215259454838\n",
            "train loss:0.8395054888136673\n",
            "train loss:0.9405123119970338\n",
            "train loss:0.880412086876103\n",
            "train loss:1.003194956555832\n",
            "train loss:0.7756419759176981\n",
            "train loss:0.9452469935628087\n",
            "train loss:0.8890774967299712\n",
            "train loss:0.6701007092218262\n",
            "train loss:0.8033287282416481\n",
            "train loss:0.7415860698972268\n",
            "train loss:0.8623794991120454\n",
            "train loss:0.9376862004244179\n",
            "train loss:0.7619892727759473\n",
            "train loss:0.8081331726720852\n",
            "train loss:0.9963457032545321\n",
            "train loss:0.9111583743165498\n",
            "train loss:0.9175637067079258\n",
            "train loss:0.7712560924427565\n",
            "train loss:0.8985892153468232\n",
            "train loss:1.0295186073028475\n",
            "train loss:0.7411309929072963\n",
            "train loss:0.9094301925754178\n",
            "train loss:0.8056967751389321\n",
            "train loss:0.7250188213782087\n",
            "train loss:0.7887267328865468\n",
            "train loss:0.9263835200874238\n",
            "train loss:0.7413560533006389\n",
            "train loss:1.026268966532406\n",
            "train loss:0.8931305704486043\n",
            "train loss:0.7795472534483084\n",
            "train loss:0.9932657989019649\n",
            "train loss:0.9371304939256363\n",
            "train loss:0.9297126861333171\n",
            "train loss:0.7120064041354752\n",
            "train loss:0.8222835022022374\n",
            "train loss:0.8527392170939477\n",
            "train loss:0.9207863676900381\n",
            "train loss:1.1114569469290447\n",
            "train loss:0.7249723532022646\n",
            "train loss:0.8749139787529052\n",
            "train loss:0.7915861393549285\n",
            "train loss:0.8614568400090553\n",
            "train loss:0.8120331578349197\n",
            "train loss:0.9581565910006951\n",
            "train loss:0.863782189712975\n",
            "train loss:0.9286229206436966\n",
            "train loss:0.8210065664927959\n",
            "train loss:0.9338204209316949\n",
            "train loss:0.9223107490771675\n",
            "train loss:0.7710311528978175\n",
            "train loss:1.029761403426439\n",
            "train loss:0.864941013935641\n",
            "train loss:0.6725006178053422\n",
            "train loss:0.8840731104492661\n",
            "train loss:0.8741178507891276\n",
            "train loss:0.8830036206359686\n",
            "train loss:0.9285338749761148\n",
            "train loss:0.8327842929531754\n",
            "train loss:0.8820325351997114\n",
            "train loss:0.7840158043676617\n",
            "train loss:0.7818661864650953\n",
            "train loss:0.7557767883823626\n",
            "train loss:0.8617470768961357\n",
            "train loss:0.8087277188815453\n",
            "train loss:0.8703055002425777\n",
            "train loss:0.8979544340892153\n",
            "train loss:0.9884808111331224\n",
            "train loss:0.9750486794568478\n",
            "train loss:1.009428358354854\n",
            "train loss:0.9026899312293167\n",
            "train loss:0.9275148262883871\n",
            "train loss:0.9191614652980702\n",
            "train loss:0.8198729299277063\n",
            "train loss:0.9967640596679503\n",
            "train loss:0.8682651007544051\n",
            "train loss:0.7679102410646144\n",
            "train loss:0.9508037836653419\n",
            "train loss:0.8483592569142373\n",
            "train loss:0.8123910309364685\n",
            "train loss:1.0089432101795799\n",
            "train loss:0.8566346613800333\n",
            "train loss:0.9081121241110247\n",
            "train loss:0.9191350184468365\n",
            "train loss:0.8940936024508184\n",
            "train loss:0.788743219747737\n",
            "train loss:0.9069719125230209\n",
            "train loss:0.7437177424804357\n",
            "train loss:0.951387550604015\n",
            "train loss:0.8203338031457741\n",
            "train loss:0.9201302242092162\n",
            "train loss:0.7962544369400598\n",
            "train loss:0.8280789335712168\n",
            "train loss:0.7684721422318751\n",
            "train loss:0.9727392027889387\n",
            "train loss:0.9065132293254908\n",
            "train loss:0.7912730441249072\n",
            "train loss:0.9004228514634229\n",
            "train loss:0.8910700425949307\n",
            "train loss:0.894950307061534\n",
            "train loss:0.9497965744781881\n",
            "train loss:0.8708697693407345\n",
            "train loss:1.064160704325915\n",
            "train loss:0.9052760253856418\n",
            "train loss:0.8940341820814858\n",
            "train loss:0.9834903747902695\n",
            "train loss:0.8384048467558898\n",
            "train loss:0.8720249220692763\n",
            "train loss:0.9019116401743219\n",
            "train loss:0.9409176716013916\n",
            "train loss:0.7895183676260245\n",
            "train loss:0.8691921584953335\n",
            "train loss:0.9072238590493805\n",
            "train loss:0.7504803997142376\n",
            "train loss:0.7881155385274843\n",
            "train loss:0.786562077790175\n",
            "train loss:0.8702250874652679\n",
            "train loss:0.9688528165867684\n",
            "train loss:0.8317693135482955\n",
            "train loss:0.721818422651128\n",
            "train loss:0.8738988804321174\n",
            "train loss:0.8425381169211353\n",
            "train loss:0.909670556316438\n",
            "train loss:0.9403973922725731\n",
            "train loss:0.6355784448130255\n",
            "train loss:0.7990983387815557\n",
            "train loss:0.9597714635253797\n",
            "train loss:0.8297218394901432\n",
            "train loss:0.9059418461217773\n",
            "train loss:0.8242180105947482\n",
            "train loss:0.9568036234272493\n",
            "train loss:0.9109257109308837\n",
            "train loss:0.8716588219717366\n",
            "train loss:0.8478273349847956\n",
            "train loss:0.8329041278133907\n",
            "train loss:1.0072658114905813\n",
            "train loss:0.7233946735981195\n",
            "train loss:0.8227962525717806\n",
            "train loss:0.7974336103995436\n",
            "train loss:0.7005990622659288\n",
            "train loss:1.0130802420978497\n",
            "train loss:1.0198275555396785\n",
            "train loss:0.7151325664706378\n",
            "train loss:0.9049401621226844\n",
            "train loss:0.7972775671216165\n",
            "train loss:0.9255025684398592\n",
            "train loss:0.8077245592331507\n",
            "train loss:0.8185148084381387\n",
            "train loss:1.0516063272661669\n",
            "train loss:0.8614484198674962\n",
            "train loss:0.9561947527114821\n",
            "train loss:0.8847862214767775\n",
            "train loss:0.9286070209666815\n",
            "train loss:0.8767092339493506\n",
            "train loss:0.7232866383455843\n",
            "train loss:0.9479585506294268\n",
            "train loss:0.9019621505711015\n",
            "train loss:0.8824142858620189\n",
            "train loss:0.9338062732327294\n",
            "train loss:0.7697837008190074\n",
            "train loss:0.8316731538853339\n",
            "train loss:0.8414599352165137\n",
            "train loss:0.9309746943511584\n",
            "train loss:0.7751864781066671\n",
            "train loss:0.8455068231446066\n",
            "train loss:1.0611079504300087\n",
            "train loss:1.0992944819098143\n",
            "train loss:0.8184782515495104\n",
            "train loss:0.7354722782697176\n",
            "train loss:0.9436657911281184\n",
            "train loss:0.8240213014535853\n",
            "train loss:0.8987715345022802\n",
            "train loss:0.9678182558187632\n",
            "train loss:0.72736335150742\n",
            "train loss:0.7250104797981602\n",
            "train loss:0.8794969964332344\n",
            "train loss:1.050965497850075\n",
            "train loss:0.9243100242854138\n",
            "train loss:0.8656003773510929\n",
            "train loss:0.8259262599022079\n",
            "train loss:0.8212976282311989\n",
            "train loss:0.7799587631483036\n",
            "train loss:0.9515527528292564\n",
            "train loss:1.0134074404403295\n",
            "train loss:0.8960224349843097\n",
            "train loss:0.7498235009668255\n",
            "train loss:0.9928600894210987\n",
            "train loss:0.8642902496415015\n",
            "train loss:0.9557279177445323\n",
            "train loss:0.8956252414520405\n",
            "train loss:0.8817911022639261\n",
            "train loss:0.7712401982481532\n",
            "train loss:0.6946391880034041\n",
            "train loss:0.7895349098524527\n",
            "train loss:0.7415326249935222\n",
            "train loss:0.6819016202495023\n",
            "train loss:1.00558874737158\n",
            "train loss:0.9704805581310721\n",
            "train loss:0.8486811018007533\n",
            "train loss:0.8591895423572202\n",
            "train loss:0.6244977504544663\n",
            "train loss:0.7100252483874033\n",
            "train loss:0.8335592705295558\n",
            "train loss:0.8248444664712424\n",
            "train loss:0.8561367695495348\n",
            "train loss:0.8763252209459638\n",
            "train loss:0.9834512850590776\n",
            "train loss:0.8480982755870384\n",
            "train loss:0.8212330763560352\n",
            "train loss:0.8845581677692748\n",
            "train loss:0.8266897812558269\n",
            "train loss:0.8504078272359747\n",
            "train loss:0.9877837819981\n",
            "train loss:0.743141399952478\n",
            "train loss:0.5526784780979132\n",
            "train loss:0.8671532374667786\n",
            "train loss:0.7844600703499124\n",
            "train loss:0.7188330045618776\n",
            "train loss:0.7790010713402795\n",
            "train loss:0.6238743694858557\n",
            "train loss:0.9195752926530445\n",
            "train loss:0.798716874090467\n",
            "train loss:0.9159993304808254\n",
            "train loss:0.9204128570931879\n",
            "train loss:0.8707258270143471\n",
            "train loss:0.8655228325906574\n",
            "train loss:0.8928617689064443\n",
            "train loss:0.6639586551675503\n",
            "train loss:0.7948665714621889\n",
            "train loss:0.8725994890466673\n",
            "train loss:0.6688502293827723\n",
            "train loss:0.8305434450094771\n",
            "train loss:0.9757263430142521\n",
            "train loss:0.8263946618367143\n",
            "train loss:0.7909708475789575\n",
            "train loss:0.786850948200284\n",
            "train loss:1.0276305609663046\n",
            "train loss:0.8445666519213375\n",
            "train loss:0.789578754833715\n",
            "train loss:0.9608649100414821\n",
            "train loss:0.8955700701821399\n",
            "train loss:0.7743726621315586\n",
            "train loss:0.9844574950270559\n",
            "train loss:0.8330680202327533\n",
            "train loss:0.8587980542620286\n",
            "train loss:0.7072523136541363\n",
            "train loss:0.86442675736699\n",
            "train loss:0.9343166774577516\n",
            "train loss:0.7069745125383845\n",
            "train loss:0.9516208925708816\n",
            "train loss:0.8773768750178392\n",
            "train loss:0.8101169112927442\n",
            "train loss:0.9108781281771102\n",
            "train loss:0.8989948854752913\n",
            "train loss:0.7851698280769479\n",
            "train loss:1.0196293823578977\n",
            "train loss:0.9123263983570544\n",
            "train loss:1.0228393020804125\n",
            "train loss:0.945976002514757\n",
            "train loss:0.8502390310719404\n",
            "train loss:0.8206554971762986\n",
            "train loss:0.8477844562446504\n",
            "train loss:0.9392408525260875\n",
            "train loss:0.8576719874376344\n",
            "train loss:0.8771690980315934\n",
            "train loss:0.8303726175988815\n",
            "train loss:0.7782096355996743\n",
            "train loss:0.8928544640072991\n",
            "train loss:0.8862267356722627\n",
            "train loss:0.8930795293484592\n",
            "train loss:0.9269234776908206\n",
            "train loss:0.7304356389078579\n",
            "train loss:0.8017902787513873\n",
            "train loss:1.000593896508866\n",
            "train loss:0.9777867965769542\n",
            "train loss:0.6604954058840637\n",
            "train loss:0.8232482878727935\n",
            "train loss:0.8562658775290991\n",
            "train loss:0.8893267077210315\n",
            "train loss:0.8551933170513\n",
            "train loss:0.9749713262001998\n",
            "train loss:0.9235184414226529\n",
            "train loss:0.7935929524582289\n",
            "train loss:0.8870754666307554\n",
            "train loss:0.8763782263770196\n",
            "train loss:0.9317679961544519\n",
            "train loss:0.8903528317787335\n",
            "train loss:0.9024459171913243\n",
            "train loss:0.8358454561603413\n",
            "train loss:0.8994954979180529\n",
            "train loss:0.8301702271019997\n",
            "train loss:0.8267961806428277\n",
            "train loss:0.703499482674232\n",
            "train loss:0.9349600592497396\n",
            "train loss:0.7260559706483256\n",
            "train loss:0.793492551417994\n",
            "train loss:0.9320081209326844\n",
            "train loss:0.6782794716652714\n",
            "train loss:0.8487566915679136\n",
            "train loss:0.7528785263407524\n",
            "train loss:0.9473367802380627\n",
            "train loss:0.7361584980844975\n",
            "train loss:0.8810946316753703\n",
            "train loss:0.9026280007222519\n",
            "train loss:0.8263048122902243\n",
            "train loss:0.7030345958031751\n",
            "train loss:0.8892999397031935\n",
            "train loss:0.7254744024495707\n",
            "train loss:1.0882412949693674\n",
            "train loss:0.8062405102569821\n",
            "train loss:0.9930809539288427\n",
            "train loss:0.8099708638014504\n",
            "train loss:0.8757170755082435\n",
            "train loss:0.8312010587243486\n",
            "train loss:0.8481368218841442\n",
            "train loss:0.8174868975576769\n",
            "train loss:0.8285869331479181\n",
            "train loss:0.8622118403599882\n",
            "train loss:0.829317170708991\n",
            "train loss:0.7610066764058427\n",
            "train loss:1.0599864228010398\n",
            "train loss:1.1021185861533456\n",
            "train loss:0.9807019479222815\n",
            "train loss:0.7961608085105842\n",
            "train loss:0.8361934259056308\n",
            "train loss:1.0318356377626634\n",
            "train loss:0.8412496832809793\n",
            "train loss:0.9424576816930619\n",
            "train loss:0.9281678418834077\n",
            "train loss:0.941204016414224\n",
            "train loss:0.6839316015682663\n",
            "train loss:0.8886750693521759\n",
            "train loss:0.8550751598371776\n",
            "train loss:0.7283428240821105\n",
            "train loss:0.946391151876457\n",
            "train loss:0.7653692231920727\n",
            "train loss:0.808262905367108\n",
            "train loss:0.8099709004977997\n",
            "train loss:0.703142763682329\n",
            "train loss:0.7619777059464923\n",
            "train loss:0.9019275246193521\n",
            "train loss:0.8410610812071347\n",
            "train loss:0.6726481922046764\n",
            "train loss:0.8118033887947297\n",
            "train loss:0.9020689911010069\n",
            "train loss:0.9778269080917065\n",
            "train loss:0.9014460170834816\n",
            "train loss:0.9142852480872683\n",
            "train loss:0.7866044995273398\n",
            "train loss:0.8265762614507214\n",
            "train loss:0.7812303525892115\n",
            "train loss:0.7910781956506641\n",
            "train loss:0.8846973762143997\n",
            "train loss:0.7182584785667622\n",
            "train loss:0.7646788404007131\n",
            "train loss:0.8039238329206886\n",
            "train loss:1.1113696863238431\n",
            "train loss:1.0160144717267585\n",
            "train loss:0.932265446445712\n",
            "train loss:0.846949383141448\n",
            "train loss:0.7800315331618718\n",
            "train loss:0.8326777519352998\n",
            "train loss:0.8864226358432902\n",
            "train loss:0.7920055606057561\n",
            "train loss:0.8843409036024491\n",
            "train loss:0.7269972673171233\n",
            "train loss:0.9218912375968484\n",
            "train loss:0.9834995105949276\n",
            "train loss:0.8718259520539489\n",
            "train loss:0.7849039089365923\n",
            "train loss:0.8697400763438116\n",
            "train loss:0.912657392674992\n",
            "train loss:0.885520388950912\n",
            "train loss:0.8562722496859072\n",
            "train loss:0.8728600681679265\n",
            "train loss:0.9921027694084232\n",
            "train loss:0.6554857579977571\n",
            "train loss:0.825882280523039\n",
            "train loss:0.7924551121388221\n",
            "train loss:0.7312729149517517\n",
            "train loss:0.8878655142146527\n",
            "train loss:0.8071284432983171\n",
            "train loss:0.7733197044542053\n",
            "train loss:0.8291892512421019\n",
            "train loss:0.919222856138095\n",
            "train loss:0.8938271168121384\n",
            "train loss:0.7558797133430588\n",
            "train loss:0.7369252570998913\n",
            "train loss:0.7401400899911031\n",
            "train loss:0.8715740244422446\n",
            "train loss:0.8343813021811581\n",
            "train loss:0.8698706418625873\n",
            "train loss:0.839379240186486\n",
            "train loss:0.7874917867330271\n",
            "train loss:0.7539984882585489\n",
            "train loss:0.9551149392065506\n",
            "train loss:0.8670480653693741\n",
            "train loss:0.87540130646837\n",
            "train loss:0.8016727891899572\n",
            "train loss:0.9858780921920307\n",
            "train loss:0.9263932788594069\n",
            "train loss:1.0389212467732039\n",
            "train loss:0.6473106238934399\n",
            "train loss:0.767629951058309\n",
            "train loss:0.8101444727334995\n",
            "train loss:0.7822972898159192\n",
            "train loss:0.9380419887781108\n",
            "train loss:0.916534634630506\n",
            "train loss:0.8451591598002417\n",
            "train loss:0.8888970885377827\n",
            "train loss:0.9003282409060966\n",
            "train loss:0.8730999445327255\n",
            "train loss:0.9109530447259813\n",
            "train loss:0.859757524702646\n",
            "train loss:0.9246495887708269\n",
            "train loss:0.7872382344119768\n",
            "train loss:0.7475711777575136\n",
            "train loss:0.809552279019895\n",
            "train loss:0.9726276411400483\n",
            "train loss:0.8393451203026713\n",
            "train loss:0.9711987049775029\n",
            "train loss:0.8681438337936318\n",
            "train loss:0.9250964929172657\n",
            "train loss:0.8174953994776647\n",
            "train loss:0.7816097903684931\n",
            "train loss:0.8895650013091747\n",
            "train loss:0.9131569906495558\n",
            "train loss:0.8959119401533566\n",
            "train loss:0.6675763999296632\n",
            "train loss:0.9188676838554435\n",
            "train loss:0.8106483320207717\n",
            "train loss:0.8150794733373351\n",
            "train loss:0.8909737700655554\n",
            "train loss:0.8890611140786576\n",
            "train loss:0.9102953329936726\n",
            "train loss:0.896449983866329\n",
            "train loss:0.7291896258552797\n",
            "train loss:0.8477367213537811\n",
            "train loss:0.7702494582652375\n",
            "train loss:0.9551060145280139\n",
            "train loss:0.9209938919077899\n",
            "train loss:0.8590128030496692\n",
            "train loss:0.8348989853069427\n",
            "train loss:0.8528040799369866\n",
            "train loss:0.8526444190074358\n",
            "train loss:0.8196389531343984\n",
            "train loss:0.892418233605108\n",
            "train loss:0.7046681646068131\n",
            "train loss:0.9202899197685329\n",
            "train loss:0.7575589866421875\n",
            "train loss:0.8631301747174264\n",
            "train loss:0.8747515066976197\n",
            "train loss:0.9200979611159501\n",
            "train loss:0.817934404794681\n",
            "train loss:0.7423116719226229\n",
            "train loss:0.8961790406951222\n",
            "train loss:0.8763913272955648\n",
            "train loss:0.8588586669794348\n",
            "train loss:0.8406266642316145\n",
            "train loss:0.8718900808985829\n",
            "train loss:0.9730297248452696\n",
            "train loss:0.7910914912817506\n",
            "train loss:0.779394921269843\n",
            "train loss:0.8811332936518191\n",
            "train loss:0.8069020813142739\n",
            "train loss:0.8298643973915445\n",
            "train loss:0.7943318885683355\n",
            "train loss:0.9124639621790506\n",
            "train loss:0.9705962975999708\n",
            "train loss:0.824151191437404\n",
            "train loss:0.8734301549623029\n",
            "train loss:0.9149235806951157\n",
            "train loss:0.9453050393830057\n",
            "train loss:0.8312438458650699\n",
            "train loss:0.8516942529059492\n",
            "train loss:0.8434692517854123\n",
            "train loss:0.8187731936198113\n",
            "train loss:0.840331457127051\n",
            "train loss:0.8686296861740214\n",
            "train loss:0.7741057542739764\n",
            "train loss:0.8135420539781283\n",
            "train loss:0.9126856193803466\n",
            "train loss:0.7723652670553938\n",
            "train loss:0.8551130066904353\n",
            "train loss:1.0462795631270774\n",
            "train loss:0.9377281003519119\n",
            "train loss:0.8769219535680974\n",
            "train loss:0.9174113420394785\n",
            "train loss:0.8159140758077383\n",
            "train loss:0.9283091189333429\n",
            "train loss:0.7497208187020974\n",
            "train loss:0.8508407874253856\n",
            "train loss:0.7996080815127502\n",
            "train loss:0.9183745016995561\n",
            "train loss:0.879198811203006\n",
            "train loss:0.9237042037102557\n",
            "train loss:0.8177754305610483\n",
            "train loss:0.9562980295971719\n",
            "train loss:0.8673660177126301\n",
            "train loss:0.7601485615803714\n",
            "train loss:0.74922197789778\n",
            "train loss:0.9015177349341614\n",
            "train loss:0.8417979189135479\n",
            "train loss:0.7572662242476311\n",
            "train loss:0.8974468369749142\n",
            "train loss:0.8326494599098626\n",
            "train loss:1.0230661173553883\n",
            "train loss:0.8100636932649644\n",
            "train loss:0.7658864632270557\n",
            "train loss:1.0495799583801055\n",
            "train loss:0.7056521835455551\n",
            "train loss:0.7234390858903852\n",
            "train loss:0.9089748679263814\n",
            "train loss:1.0328248255883894\n",
            "train loss:0.803264835719198\n",
            "train loss:0.9033533030534816\n",
            "train loss:0.8383780570545106\n",
            "train loss:0.7193710540985501\n",
            "train loss:0.8422393492268054\n",
            "train loss:0.8091098272213909\n",
            "train loss:0.7906505460346405\n",
            "train loss:0.8904961045749793\n",
            "train loss:0.753018146924757\n",
            "train loss:0.9470508805282267\n",
            "train loss:0.9674516308207322\n",
            "train loss:0.9998339691262556\n",
            "train loss:0.9369196863967759\n",
            "train loss:0.8979488515038376\n",
            "train loss:0.988695252414871\n",
            "train loss:0.8968100844304503\n",
            "train loss:0.7452230309510187\n",
            "train loss:0.9277573124686432\n",
            "train loss:0.8883203378907606\n",
            "train loss:0.7721700941707664\n",
            "train loss:0.877287285090722\n",
            "train loss:0.8361115434411037\n",
            "train loss:0.7964744020373091\n",
            "train loss:0.9755310141609695\n",
            "train loss:0.8583364311475602\n",
            "train loss:0.8274775426287825\n",
            "train loss:0.8441148157273635\n",
            "train loss:0.7718379279681212\n",
            "train loss:0.9003637572918721\n",
            "train loss:1.0037813495966177\n",
            "train loss:0.8363521139060669\n",
            "train loss:0.9699561554075012\n",
            "train loss:0.8265540961796062\n",
            "train loss:0.860616019513325\n",
            "train loss:0.9830125683070006\n",
            "train loss:0.809630274991479\n",
            "train loss:0.8071386166992677\n",
            "train loss:0.7951565690813492\n",
            "train loss:0.8281704994524316\n",
            "train loss:0.8610179759962358\n",
            "train loss:0.8953609031296851\n",
            "train loss:0.9472518036231965\n",
            "train loss:0.9043847761168425\n",
            "train loss:0.8710140178101944\n",
            "train loss:0.8218429896969284\n",
            "train loss:0.7727494051411142\n",
            "train loss:0.8180305614872124\n",
            "train loss:0.8758999987256874\n",
            "train loss:0.8378523065244607\n",
            "train loss:0.8545568915675765\n",
            "train loss:1.0038075660299675\n",
            "train loss:0.8924141312304852\n",
            "train loss:0.8414771258307823\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.9933\n",
            "Saved Network Parameters!\n"
          ]
        }
      ],
      "source": [
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from common.trainer import Trainer\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "network = DeepConvNet()  \n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=20, mini_batch_size=100,\n",
        "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "trainer.train()\n",
        "\n",
        "# パラメータの保存\n",
        "network.save_params(\"../ch08/deep_convnet_params.pkl\")\n",
        "print(\"Saved Network Parameters!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG6Pa438f_27"
      },
      "source": [
        "# ch08/misclassified_mnist.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "DcwrYsEJf_27",
        "outputId": "f1711ee8-f796-4a7c-b06f-fd7e67203c3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "calculating test accuracy ... \n",
            "test accuracy:0.9933\n",
            "======= misclassified result =======\n",
            "{view index: (label, inference), ...}\n",
            "{1: (5, 8), 2: (2, 7), 3: (9, 4), 4: (2, 7), 5: (3, 5), 6: (8, 2), 7: (6, 4), 8: (2, 7), 9: (3, 5), 10: (6, 0), 11: (9, 4), 12: (7, 1), 13: (5, 7), 14: (2, 3), 15: (5, 2), 16: (2, 7), 17: (8, 2), 18: (9, 4), 19: (5, 3), 20: (5, 8)}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 20 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "network = DeepConvNet()\n",
        "network.load_params(\"../ch08/deep_convnet_params.pkl\")\n",
        "\n",
        "print(\"calculating test accuracy ... \")\n",
        "#sampled = 1000\n",
        "#x_test = x_test[:sampled]\n",
        "#t_test = t_test[:sampled]\n",
        "\n",
        "classified_ids = []\n",
        "\n",
        "acc = 0.0\n",
        "batch_size = 100\n",
        "\n",
        "for i in range(int(x_test.shape[0] / batch_size)):\n",
        "    tx = x_test[i*batch_size:(i+1)*batch_size]\n",
        "    tt = t_test[i*batch_size:(i+1)*batch_size]\n",
        "    y = network.predict(tx, train_flg=False)\n",
        "    y = np.argmax(y, axis=1)\n",
        "    classified_ids.append(y)\n",
        "    acc += np.sum(y == tt)\n",
        "    \n",
        "acc = acc / x_test.shape[0]\n",
        "print(\"test accuracy:\" + str(acc))\n",
        "\n",
        "classified_ids = np.array(classified_ids)\n",
        "classified_ids = classified_ids.flatten()\n",
        " \n",
        "max_view = 20\n",
        "current_view = 1\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.2, wspace=0.2)\n",
        "\n",
        "mis_pairs = {}\n",
        "for i, val in enumerate(classified_ids == t_test):\n",
        "    if not val:\n",
        "        ax = fig.add_subplot(4, 5, current_view, xticks=[], yticks=[])\n",
        "        ax.imshow(x_test[i].reshape(28, 28), cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "        mis_pairs[current_view] = (t_test[i], classified_ids[i])\n",
        "            \n",
        "        current_view += 1\n",
        "        if current_view > max_view:\n",
        "            break\n",
        "\n",
        "print(\"======= misclassified result =======\")\n",
        "print(\"{view index: (label, inference), ...}\")\n",
        "print(mis_pairs)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.1.2 さらに認識精度を高めるには\n",
        "- アンサンブル学習  \n",
        "- 学習係数の減衰（learning rate decay）  \n",
        "- **Data Augmentation**（データ拡張）"
      ],
      "metadata": {
        "id": "8u5V3j6at4Di"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.1.3 層を深くすることのモチベーション\n",
        "- パラメータの数を少なくできること  \n",
        "受容野を広くカバーできる  \n",
        "- 活性化関数の数が増え、非線形性がさらに強まって表現力が向上する\n",
        "- 学習の効率性  \n"
      ],
      "metadata": {
        "id": "K7aD1LYyuZNA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.2 ディープラーニングの小歴史\n",
        "## 8.2.1 ImageNet\n",
        "## 8.2.2 VGG\n",
        "- VGG16やVGG19  \n",
        "\n",
        "## 8.2.3 GoogLeNet\n",
        "- インセプション構造（横方向に幅がある）  \n",
        "\n",
        "## 8.2.4 ResNet\n",
        "- スキップ構造  \n"
      ],
      "metadata": {
        "id": "2l5n6ZO7vlvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.3 ディープラーニングの高速化\n",
        "## 8.3.1 取り組むべき問題\n",
        "- 畳み込み層の演算の高速化が課題  \n",
        "→ 積和演算\n",
        "\n",
        "## 8.3.2 GPUによる高速化\n",
        "\n",
        "## 8.3.3 分散学習\n",
        "\n",
        "## 8.3.4 演算精度のビット削減\n"
      ],
      "metadata": {
        "id": "i87USc61xbbM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r-T_MXQf_27"
      },
      "source": [
        "# ch08/half_float_network.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjUskl_cf_28",
        "outputId": "cc42082e-2d2d-423e-dd6f-abf237eace84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "caluculate accuracy (float64) ... \n",
            "0.9933\n",
            "caluculate accuracy (float16) ... \n",
            "0.9933\n"
          ]
        }
      ],
      "source": [
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "network = DeepConvNet()\n",
        "network.load_params(\"../ch08/deep_convnet_params.pkl\")\n",
        "\n",
        "sampled = 10000 # 高速化のため\n",
        "x_test = x_test[:sampled]\n",
        "t_test = t_test[:sampled]\n",
        "\n",
        "print(\"caluculate accuracy (float64) ... \")\n",
        "print(network.accuracy(x_test, t_test))\n",
        "\n",
        "# float16に型変換\n",
        "x_test = x_test.astype(np.float16)\n",
        "for param in network.params.values():\n",
        "    param[...] = param.astype(np.float16)\n",
        "\n",
        "print(\"caluculate accuracy (float16) ... \")\n",
        "print(network.accuracy(x_test, t_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.4 ディープラーニングの実用例\n",
        "## 8.4.1 物体検出\n",
        "- 物体の位置とクラス分類  \n",
        "- R-CNN, Faster R-CNN  \n",
        "\n",
        "## 8.4.2 セグメンテーション\n",
        "- FCN(Fully Convolutional Network)  \n",
        "\n",
        "## 8.4.3 画像キャプション生成\n",
        "- NIC(Newral Image Caption)  \n",
        "CNNとRNNから構成  "
      ],
      "metadata": {
        "id": "d9xazZPtzSmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.5 ディープラーニングの未来\n",
        "## 8.5.1 画像スタイル変換\n",
        "## 8.5.2 画像生成\n",
        "- DCGAN\n",
        "\n",
        "## 8.5.3 自動運転\n",
        "- SegNet\n",
        "\n",
        "## 8.5.4 Deep Q-Network （強化学習）\n"
      ],
      "metadata": {
        "id": "qrT3iooS5C4j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.6 まとめ"
      ],
      "metadata": {
        "id": "vQhdD4LN6TBn"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}